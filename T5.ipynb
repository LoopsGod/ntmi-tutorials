{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022/T5_student",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPRrNbmJTbHdQrtj48HR7QZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide\n",
        "\n",
        "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
        "* Note that, as always, the notebook contains a condensed version of the theory We recommend you read the theory part before the LC session.\n"
      ],
      "metadata": {
        "id": "jzvWfS-ELNzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ILOs\n",
        "\n",
        "After completing this lab you should be able to\n",
        "\n",
        "* develop NGram LMs (classic and neural) in Python (and PyTorch)\n",
        "* estimate parameters of LMs via MLE\n",
        "* evaluate LMs intrinsically in terms of perplexity\n",
        "* evaluate LMs statistically in terms of properties of generated text"
      ],
      "metadata": {
        "id": "KqR7WUDXLeME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Notes\n",
        "\n",
        "* In this notebook you are expected to use $\\LaTeX$. \n",
        "* Use python3.\n",
        "* Use Torch\n",
        "* To have GPU support run this notebook on Google Colab (you will find more instructions later).\n",
        "\n",
        "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n",
        "\n",
        "If you need a short introduction to PyTorch [check this tutorial](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n"
      ],
      "metadata": {
        "id": "YBR2bPwLL9gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "* Data\n",
        "* Word segmentation and tokenisation\n",
        "* Language models\n",
        "* NGram language models\n",
        "    * Unigram LM\n",
        "    * Higher order LMs\n",
        "    * Implementation\n",
        "    * Experiment\n",
        "* Evaluation\n",
        "* Neural NGram LM"
      ],
      "metadata": {
        "id": "CqDZh0QJJsOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Graded Exercises\n",
        "\n",
        "**Important.** The grader may re-run your notebook to investigate its correctness, but you do upload your notebook with the cells already run and make sure that all your answers are visible without the need to re-run the notebook. \n",
        "\n",
        "The weight of the exercise is indicated below.\n",
        "\n",
        "* [Statistics from classic LMs](#classic) (30%)\n",
        "* [Perplexity](#perplexity) (20%)\n",
        "* [Neural NGram LMs](#neural) (50%)\n"
      ],
      "metadata": {
        "id": "Hg54PiqdLoN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up"
      ],
      "metadata": {
        "id": "fVnfg0kMLrsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "LzjCfsJDNL1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install sentencepiece\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "QV4oRuW-XYED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "In this tutorial we will develop models of text generation. So our data for this tutorial will be collections of sentences, or *corpora*. We will use corpora available in NLTK."
      ],
      "metadata": {
        "id": "B8qybX6RNDKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "KfbZPjfaZDdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_nltk_corpus(corpus, max_length=30):\n",
        "    \"\"\"\n",
        "    Shuffle and split a corpus.\n",
        "    corpus: list of sentences, each sentence is a list of tokens, each token is a string.\n",
        "    max_length: discard sentences longer than this\n",
        "\n",
        "    Return: training sentences, dev sentences, test sentences\n",
        "        in each corpus a sentence is now a string where tokens are space separated\n",
        "    \"\"\"\n",
        "    sentences = corpus.sents()\n",
        "    # do not change the seed in here    \n",
        "    order = np.random.RandomState(42).permutation(np.arange(len(sentences)))    \n",
        "    shuffled = [' '.join(sentences[i]) for i in order if len(sentences[i]) <= max_length]    \n",
        "    return shuffled[2000:], shuffled[1000:2000], shuffled[:1000]"
      ],
      "metadata": {
        "id": "BNqS36sZNExH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "training, dev, test = split_nltk_corpus(brown)"
      ],
      "metadata": {
        "id": "7-pkD_jNOHBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of sentences: training={len(training)} dev={len(dev)} test={len(test)}\")"
      ],
      "metadata": {
        "id": "74md717DKLc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"# A few training sentences\\n\\n\")\n",
        "for x in training[:10]:\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "Fk5nks5bKG0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word segmentation and tokenisation\n",
        "\n",
        "Our models of text generation are probability distributions over finite-length sequences of discrete symbols. These discrete symbols are what we call *tokens*. The **vocabulary** of the model is therefore the finite set of known tokens which it can use to make sequences. \n",
        "\n",
        "We are interested in a special type of sequence, namely, sentences. Sentences are typically made of linguistic units that we call words. The linguistic notion of *word* is much too complex for our models. In practice, we use a data-driven and computationally convenient notion instead.\n",
        "\n",
        "In this tutorial we will work with tokens that are subword units obtained via a compression algorithm known as *byte pair encoding* (BPE). You can optionally check the [original paper](http://www.aclweb.org/anthology/P16-1162) for more detail. \n",
        "\n",
        "We will use a package called `sentencepiece` that implements an efficient BPE tokeniser (and word segmenter) for us. This tokeniser is language independent, it learns a vocabulary of subword units from a corpus of sentences (without the need for any tokenisation). Because this is based on a compression algorithm, we can choose the level of compression, that is, we can choose the number of tokens that we want to have in the vocabulary, the BPE algorithm will find what collection of tokens best describes the corpus at the given budget. \n",
        "\n",
        "After trained, we can use the BPE model to tokenise and detokenise sentences for us deterministically. "
      ],
      "metadata": {
        "id": "x6hEpj0DOzxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import io"
      ],
      "metadata": {
        "id": "3q9Vx37hT_1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helper function trains a BPE model for us:"
      ],
      "metadata": {
        "id": "2pEGOnBb5xRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_vocabulary(corpus, vocab_size):\n",
        "    \"\"\"\n",
        "    Return a BPE model as implemented by the sentencepiece package.\n",
        "\n",
        "    corpus: an iterable of sentences, each sentence is a python string\n",
        "    \"\"\"\n",
        "    proto = io.BytesIO()\n",
        "\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        sentence_iterator=iter(corpus), \n",
        "        model_writer=proto, \n",
        "        vocab_size=vocab_size,\n",
        "        pad_id=0,\n",
        "        bos_id=1,\n",
        "        eos_id=2,\n",
        "        unk_id=3,\n",
        "    )\n",
        "\n",
        "    return spm.SentencePieceProcessor(model_proto=proto.getvalue())"
      ],
      "metadata": {
        "id": "9cMWg7AnUE1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = fit_vocabulary(training, vocab_size=1000)"
      ],
      "metadata": {
        "id": "--9I-m4sXO6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how we control the vocabulary size:"
      ],
      "metadata": {
        "id": "fE0kDLoU7sw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size()"
      ],
      "metadata": {
        "id": "fwDYZnQb7uzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see how we can use this object to tokenize and detokenize text:"
      ],
      "metadata": {
        "id": "aQwWTQhQ7v7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_str = training[0]\n",
        "print(example_str)"
      ],
      "metadata": {
        "id": "uTGnu6Ce6X_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `encode` can be used to tokenize a string into a list of tokens (each a string). To be able to read the output, you need to use the argument `out_type=str`, otherwise the tokenizer will convert the tokens to numerical codes."
      ],
      "metadata": {
        "id": "uLsaq2ht6bMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(example_str, out_type=str)"
      ],
      "metadata": {
        "id": "82Ma5982633i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `decode` method can map the tokens back to original form:"
      ],
      "metadata": {
        "id": "8gMpdG7N6wAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(example_str, out_type=str))"
      ],
      "metadata": {
        "id": "VrR9lypI6wJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without `out_type=str` we get a sequence of codes:"
      ],
      "metadata": {
        "id": "5mmZQwUk6pYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(example_str)"
      ],
      "metadata": {
        "id": "PlDeuNoIV43l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, of course, `decode` can map it back to text:"
      ],
      "metadata": {
        "id": "coFQMcyY6t-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(example_str))"
      ],
      "metadata": {
        "id": "Po5bVZndWLZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two main advantages of this strategy for tokenization:\n",
        "\n",
        "1. we control the vocabulary size (which helps us control memory usage)\n",
        "2. oftentimes unseen words are made of a combination of existing subword units, so we can deal with more words than before"
      ],
      "metadata": {
        "id": "SOgw51RB7Lup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"This is a tutorial within Natuurlijke Talmodellen en Interfaces at the UvA.\", out_type=str)"
      ],
      "metadata": {
        "id": "LitMBNpm7awk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also preprocess whole batches of sentences:"
      ],
      "metadata": {
        "id": "BBZfphivEmDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode([\"This is a sentence.\",  \"And this is another\"], out_type=str)"
      ],
      "metadata": {
        "id": "vw2g0klsEo5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode([\"This is a sentence.\",  \"And this is another\"], out_type=str))"
      ],
      "metadata": {
        "id": "NCVRuWpfEvKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ungraded exercise.** Play a bit with the tokenizer object."
      ],
      "metadata": {
        "id": "GmFEQrPlMIFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Models\n",
        "    \n",
        "A language model (LM) is a **probability distribution over text**, where text is a finite sequences of words. \n",
        "    \n",
        "LMs can be used to generate text as well as to quantify a degree of naturalness (or rather, a degree of resemblance to training data) which is useful for example to compare and rank alternative pieces of text in terms of fluency.\n",
        "    \n",
        "To design an LM, we need to talk about units of text (e.g., documents, paragraphs, sentences) as outcomes of a random experiment, and for that we need random variables.\n",
        "    \n",
        "We will generally refer to the unit of text as a *sentence*, but that's just for convenience, you could design an LM over documents and very little, if anything, would change.  \n",
        "\n",
        "A **random sentence** is a finite sequence of symbols from the vocabulary of a given language. As a running example, we will refer to this language as English. The vocabulary of English will be denoted by $\\mathcal W$, a finite collection of unique symbols, each of which we refer to as a *word* (but in practice, these symbols are any unit we care to model). We will denote a random sentence by $X$, or more explicitly, by the random sequence $X = \\langle W_1, \\ldots, W_L \\rangle$. Here $L$ indicates the sequence length. Each word in the sequence is a random variable that takes on values in $\\mathcal W$. We will adopt an important convention, every sentence is a finite sequence that ends with a special symbol, the end-of-sequence (EOS) symbol. \n",
        "\n",
        "Formally, random sentences take on values in the set $\\mathcal W^*$ of all strings made of symbols in $\\mathcal W$, which is a set that does include an infinte number of valid English sentences (possibly not all English sentences, as our vocabulary may not be complete enough, but hopefully this space is still large enough for the LM to be useful) as well as an infinite number of sequences that are not valid English sentences.       \n",
        "    \n",
        "Part of the deal with a language model is to define and estimate a probability distribution that expresses a preference for sentences that are more likely to be accepted as English sentences. In practice an LM will prefer sentences that reproduce statistics of the observations used to estimate its parameters, whether these sentences will resemble English sentences or not will depend on how expressive the LM is, that is, whether or not the LM can capture patterns as complex as those arising from well-formed English (or whatever variant/register of English was observed during training).\n",
        "    \n",
        "**Notation guide** Some textbooks or papers use $W_1^L$ instead of $W_{1:L}$ for ordered sequences, both are clear enough, but we will use the notation adopted by the textbook, that is, $W_{1:L}$. The textbook uses $W_1\\cdots W_L$ (without commas) as another notation for ordered sequences, but we prefer to explicitly mark the sequence with angle brackets to avoid ambiguities, i.e., we prefer $\\langle W_1, \\ldots, W_L \\rangle$. For assignments, we will use the lowercase version of the letter that names the random variable: $w_{1:l} = \\langle w_1, \\ldots, w_l \\rangle$. \n"
      ],
      "metadata": {
        "id": "KVKUr3UcybC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following notational shortcuts are rather convenient:\n",
        "\n",
        "* we will often use $W_{1:L}$ for a random sentence instead of the longer form $\\langle W_1, \\ldots, W_L \\rangle$, and similarly for outcomes (i.e., $w_{1:l}$ instead of $\\langle w_1, \\ldots, w_l\\rangle$), but in the long form we shall never drop the angle brackets, as otherwise it's hard to tell that we mean an ordered sequence\n",
        "* we will use $W_{<i}$ (or $w_{<i}$ for an outcome) to denote the sequence of tokens that precedes the $i$th token, this sequence is empty $W_{<i} \\triangleq \\langle \\rangle$ for $i \\le 1$, for $i>1$ the sequence is defined as $W_{<i} \\triangleq \\langle W_1, \\ldots, W_{i-1}\\rangle$\n",
        "* sometimes it will be useful to find a more compact notation for $W_{<i}$, in those cases we refer to it as a *random history* and denote it by $H$.\n",
        "\n"
      ],
      "metadata": {
        "id": "nuyxN3qpz-nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LMs can be described by a **generative story**, that is, a stochastic procedure that explains how an outcome $w_{1:l}$ is drawn from the model distribution. Though we may find inspiration in how we believe the data were generated, the generative story is not a faithful representation of any linguistic process, it is all but an abstraction that codes our own assumptions about the problem. \n",
        "\n",
        "The most general form this generative story can take, that is, the form with the least amount of assumptions, looks as follows:\n",
        "\n",
        "1. For each position $i$ of a sequence, condition on the history $h_i$ and draw the $i$th word $w_i$ with probability $P(W=w_i|H=h_i)$. \n",
        "2. Append $w_i$ to the end of the history: $h_i \\circ \\langle w_i \\rangle$\n",
        "2. Stop generating if $w_i$ is the EOS token, else repeat from (1).\n",
        "\n",
        "We say this procedure is very general because it is essentially just chain rule spelled out in English words, though here the order of enumeration is determined by the left-to-right order of tokens in an English sentence.\n",
        "\n",
        "\n",
        "Here is an example for a sequence of length $l=3$:\n",
        "\n",
        "$P_X(\\langle w_1, w_2, w_3 \\rangle) = P_{W|H}(w_1|\\langle \\rangle) P_{W|H}(w_2|\\langle w_1 \\rangle) P_{W|H}(w_3 |\\langle w_1, w_2 \\rangle)$\n",
        "\n",
        "For our example sentence *He went to the store* this means:\n",
        "\n",
        "\\begin{align}\n",
        "P_X(\\langle \\text{He, went, to, the, store, EOS} \\rangle) &= P_{X|H}(\\text{He}|\\langle \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{went}|\\langle \\text{He} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{to}|\\langle \\text{He}, \\text{went} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{the}|\\langle \\text{He},  \\text{went}, \\text{to} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{store}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{EOS}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the}, \\text{store} \\rangle) \n",
        "\\end{align}\n",
        "\n",
        "* where with some abuse of notation we use the words themselves as outcomes instead of their corresponding indices. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P4xrSiKo0bcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution**  Write down the general rule for the probability $P_X$ of a sentence $w_{1:l}$. Don't forget to indicate the precise random variable associated with every distribution (that is, for example, $P_X(w_{1:l})$ and $P(X=w_{1:l})$ are correct while $P(w_{1:l})$ is incomplete). "
      ],
      "metadata": {
        "id": "QseSMY5K1QP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "\n",
        "$P_X(w_{1:l}) = \\prod_{i=1}^{l}P_{W|H}(w_i|w_{<i})$\n",
        "\n",
        "where $l$ is the length of the sentence.\n",
        "    \n",
        "</details>\n",
        "    \n",
        "---"
      ],
      "metadata": {
        "id": "UMkvbZrS1Sd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LM above is just an abstraction, not a concrete implementation, think of it as a general template for building models. \n",
        "\n",
        "A concrete model design needs to specify the conditional probability distributions in the model, this is known as the **parameterisation** of the model, and an algorithm for parameter estimation. "
      ],
      "metadata": {
        "id": "EysRkGqW1vDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NGram Language Models\n",
        "\n",
        "One way to achieve a tractable parameterisation of a language model is to make a conditional independence assumption, which simplifies the factors in the chain rule."
      ],
      "metadata": {
        "id": "197Ez3tYFj9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram LM\n",
        "\n",
        "From week 1 of the course, we actually already know the *unigram LM*, which is the simplest language model: the idea is to forget the history completely, therefore making the strong assumption that words are drawn from the same distribution independently of thei preceding context (i.e., $W_i \\perp W_{<i}$):\n",
        "\n",
        "\\begin{equation}\n",
        "P_X(w_{1:l}) \\overset{\\text{ind.}}{\\triangleq}  \\prod_{i=1}^l P_W(w_i)\n",
        "\\end{equation}\n",
        "\n",
        "For the parameterisation, we let $W$ follow a Categorical distribution with parameter $\\pi_{1:V} \\in \\Delta_{V-1}$. \n",
        "\n",
        "Then the probability mass function of the unigram LM assigns probability \n",
        "\n",
        "\\begin{align}\n",
        "f_X(w_{1:l}; \\theta) &\\triangleq \\prod_{i=1}^l \\text{Cat}(w_i|\\pi_{1:V}) \\\\\n",
        "&=\\prod_{i=1}^l \\pi_{w_i}\n",
        "\\end{align}\n",
        "\n",
        "where $\\theta = \\{\\pi_{1}, \\ldots, \\pi_V\\}$ are the trainable parameters of the language model, and the Categorical pmf is $\\text{Cat}(c|\\pi_{1:V}) = \\pi_c$.\n",
        "\n",
        "From now on we will often have multiple pmfs in the same expression, that's because our models will have multiple components. To remind you of which pmf we are talking about, we will subscript the letter $f$ with the random variable with which the pmf is associated. The stuff that appears after the semi-colon is the parameter upon which the pmf depends parametrically."
      ],
      "metadata": {
        "id": "_Qv0RV8310dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Example***\n",
        "\n",
        "Consider the sentence *He went to the store*, its probability under the unigram LM is\n",
        "\n",
        "$P_X(\\langle \\text{He, went, to, the, store, EOS} \\rangle) = P_W(\\text{He}) \\times P_W(\\text{went}) \\times P_W(\\text{to}) \\times P_W(\\text{the}) \\times P_W(\\text{store}) \\times P_W(\\text{EOS})$\n",
        "\n",
        "which, as a function of the parameters of the model, evaluates to\n",
        "\n",
        "$P_S(\\langle \\text{He, went, to, the, store, EOS} \\rangle) = \\pi_{\\text{He}} \\times \\pi_{\\text{went}} \\times \\pi_{\\text{to}} \\times \\pi_{\\text{the}} \\times \\pi_{\\text{store}} \\times \\pi_{\\text{EOS}}$\n",
        "\n",
        "where again we use the words instead of their indices.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OmlYCvuN3K8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a concrete instance of a language model:\n",
        "* a set of independence assumptions (the unigram assumption)\n",
        "* a parameterisation (we will use a single Categorical distribution)\n",
        "\n",
        "If we are given the parameters $\\pi_{1:V}$, we will be able to sample $X$ from this model, and we will be able to assess the probability the model assigns to any given sentence in its sample space. \n",
        "\n",
        "So, we just need to find a way to choose $\\pi_{1:V}$. We could pick any vector in the probability simplex. Clearly, some probability vectors are better than others though, so we better find a procedure that enjoys some theoretical support.\n",
        "\n",
        "We can turn to basic statistics for help, in particular, we can turn to maximum likelihood estimation."
      ],
      "metadata": {
        "id": "4P923XeH3Ylk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter estimation\n",
        "\n",
        "Suppose we are given a corpus $\\mathcal D$ containing $N$ sentences\n",
        "\n",
        "* each sentence is of the form $w_{1:l_n}^{(n)}$ for $n=1, \\ldots, N$\n",
        "* where $l_n$ is the length of the $n$th sentence\n",
        "\n",
        "The MLE solution for the unigram LM is based on gathering counts and computing the relative frequency of word types:\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi_w = \\frac{\\mathrm{count}_W(w)}{\\sum_{o \\in \\mathcal W}\\mathrm{count}_W(o)} \n",
        "\\end{equation}\n",
        "\n",
        "And we may, for example, employ a smoothing technique such as Laplace smoothing.\n",
        "\n"
      ],
      "metadata": {
        "id": "QpQ3fkjy3hOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Higher order LMs\n",
        "\n",
        "    \n",
        "A unigram LM makes some rather unreasonable assumptions. Clearly, words in a sentence do depend on one another."
      ],
      "metadata": {
        "id": "c3rmYz_Z4CPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** Can the unigram LM assign different probabilities to `what a nice day` and `day what nice a`?"
      ],
      "metadata": {
        "id": "w_LT8blp4HN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "    No really. The sentence contains the exact same tokens and they occur the exact same number of times.\n",
        "    \n",
        "    \n",
        "</details>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TJcOn4Uv4K4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to capture dependencies in English sentences, we better go back to the general chain rule\n",
        "    \n",
        "\\begin{equation}\n",
        "P_X(w_{1:l}) = \\prod_{i=1}^{l}P_{W|H}(w_i|w_{<i})\n",
        "\\end{equation}\n",
        "\n",
        "and ask ourselves, why did we make such a strong independence assumption in the unigram LM and can we avoid it?\n",
        "\n",
        "The deal is that with our current way of parameterising a Categorical distribution, we need to store  a $V$-dimensional parameter vector for every unique history in the training data. This type of parameterisation, where we have one parameter (a probability value) per outcome per context, is known as **tabular** (you can imagine storing the parameters in a table, each row is a context, each column is an outcome).\n",
        "The more words we allow in the context, the more parameters we will have to estimate, and this tabular representation grows very quickly. Not only this costs a lot of parameters, most histories will appear very few times, a very long history will probably only appear once. Thus we will not be able to gather enough data to estimate the parameters of the LM reliably. \n",
        "\n",
        "We have at least two ways around this problem: we can revisit the model from the point of view of the (conditional) independencies we make, or we can change the way we parameterise Categorical distributions to make them more parameter efficient. The combination of the two strategies lies at the core of much of the progress in modern NLP, so we will look into both.\n",
        "\n",
        "The core of the problem is the data sparsity due to long histories, then let's shorten the histry. But unlike the unigra LM, we won't discard the history entirely, we will just forget some of it instead. \n",
        "\n",
        "The $n$-gram LM assumes that $W_i$ is independent of all but the $n-1$ immediately preceding words. It uses Categorical distributions to model the distribution of $W | H=x_{i-n+1:i-1}$ in context. \n",
        "\n",
        "\n",
        "In its standard formulation, the $n$-gram LM uses tabular cpds, that is, it stores one Categorical distribution per unique history: $X | H=h \\sim \\text{Cat}(\\pi_{1:V}^{(h)})$ with $\\pi_{1:V}^{(h)} \\in \\Delta_{V-1}$ for each history $h \\in \\mathcal W^{n-1}$ (a sequence of $n-1$ tokens).\n",
        "\n",
        "The joint probability the $n$-gram LM assigns to a sentence $x_{1:m}$ is\n",
        "\n",
        "\\begin{align}\n",
        "P_X(w_{1:l}) &\\overset{cond.ind.}{\\triangleq} \\prod_{i=1}^l P_{W|H}(w_i|\\underbrace{w_{i-n+1:i-1}}_{h_i})\n",
        "\\end{align}\n",
        "\n",
        "and therefore the pmf of the $n$-gram LM is:\n",
        "\\begin{align}\n",
        "    f_X(w_{1:l}; \\theta)&= \\prod_{i=1}^l \\text{Cat}(w_i|\\pi_{1:V}^{(h_i)}) \\\\\n",
        "    &=\\prod_{i=1}^l \\pi^{(h_i)}_{w_i}\n",
        "\\end{align}\n",
        "where $\\theta = \\{\\pi_{1:V}^{(h)} \\text{ for every }h \\in \\mathcal W^{n-1} \\}$ are the parameters of the model.\n",
        "\n",
        "\n",
        "The $n$-gram LM is what we call a **Markov model** of order $o=n-1$, the order indicates the length of the shortened history we condition on when drawing $W_i$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9V3ZQQer4N-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution**  Write down the probability of the sentence \n",
        "\n",
        "    He went to the store\n",
        "    \n",
        "under a bigram language model. "
      ],
      "metadata": {
        "id": "h_wjJRDR5rCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "\n",
        "$P_X(\\langle \\text{He, went, to, the, store, EOS} \\rangle) = P_{W|H}(\\text{He}|\\langle \\rangle) \\times P_{W|H}(\\text{went}|\\langle \\text{He} \\rangle) \\times P_{W|H}(\\text{to}| \\langle \\text{went} \\rangle) \\times P_{W|H}(\\text{the}| \\langle \\text{to} \\rangle) \\times P_{W|H}(\\text{store}|\\langle \\text{the} \\rangle) \\times P_{W|H}(\\text{EOS}|\\langle \\text{store} \\rangle)$\n",
        "\n",
        "Tip: recall that *the* is a word while $\\langle \\text{the} \\rangle$ is a sequence, and recall that though sentences in corpora don't usually come decorated with an EOS, our LMs treat them as if they did.\n",
        "    \n",
        "Trick: it's quite handy to imagine that we had $n-1$ occurences of a begin-of-sentence (BOS) symbol prior to the first token in the sentence, this gives us a fixed-size history across time steps: the first step would look like $P_{W|H}(\\text{He}|\\langle \\text{BOS} \\rangle)$.\n",
        "\n",
        "</details>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6v-yE6Ej5vZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter estimation\n",
        "\n",
        "The Laplace-smoothed MLE solution for tabular cpds is\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi_w^{(h)} = \\frac{\\mathrm{count}_{HW}(h \\circ \\langle w \\rangle) + \\alpha}{\\mathrm{count}_H(h) + \\alpha V}\n",
        "\\end{equation}\n",
        "\n",
        "where  $h \\circ \\langle w \\rangle$ is the concatenation of history and word. \n",
        "\n",
        "Often this is not enough for smoothing higher order models, the reason being that not only certain words will be unseen, but very often histories will be unseen. For example, we have seen \"a nice dog\" as a history, but have not seen \"a strange dog\". There are many techniques to overcome missing histories, the simplest of which is the *backoff* technique, whereby upon failing to encounter \"a strange dog\", we try \"UNK strange dog\"; failing to encounter it again, we try \"UNK UNK dog\", and if we also fail to encounter that, we use \"UNK UNK UNK\", at which point at the very list, the corresponding cpd will be uniform over the vocabulary. You will see this technique being used in the implementation below.\n",
        "    \n",
        "**Notation guideline** When we use a superscript like $\\theta_{1:V}^{(h)}$ we mean the probability vector selected by the history, or the probability vector that is specific to the cpd that conditions on the given history $h$. For example, in a bigram LM, we have one $V$-dimensional parameter vector for each one of the $V$ cpds in the model. That is, there is one cpd per word in our vocabulary, because each time we condition on a history $\\langle w \\rangle$, we get a different cpd, and each of these cpds is a discrete distribution over the entire vocabulary. \n",
        "    "
      ],
      "metadata": {
        "id": "yDoz6Zvr6I9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "Here we provide a complete implementation of an NGram LM, our implementation is kept simple for didactic purposes and is, therefore, not super efficient. You will only be able to play with models of small order (we recommend not going beyond a trigram LM).\n",
        "\n",
        "**Ungraded exercise.** Study the implementation of the NGramLM. "
      ],
      "metadata": {
        "id": "Jooli5Gg6iKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "from itertools import tee\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NGramLM:\n",
        "    \"\"\"\n",
        "    This is an n-gram LM with generalised Laplace smoothing.    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, ngram_size: int, alpha=0.0, backoff=False, BOS=\"<s>\", EOS=\"</s>\", UNK=\"<unk>\", seed=None):  \n",
        "        \"\"\"\n",
        "        ngram_size: \n",
        "        alpha: Laplace smoothing coefficient\n",
        "        backoff: if True, for every known h, we also store counts for shortened versions of h\n",
        "         where, from left-to-right, we 'forget' words in h (by seeting them to UNK)\n",
        "         \n",
        "         Example: in a trigram LM, if ['a', 'cute', 'dog'] is the history and 'barks' is the next word\n",
        "            we gather counts for:\n",
        "            * ['cute', 'dog'] , 'barks'\n",
        "            * ['UNK', 'dog'] , 'barks'\n",
        "            * ['UNK', 'UNK'] , 'barks'\n",
        "\n",
        "        BOS: symbol to be used internally as BOS\n",
        "        EOS: symbol to be used as EOS (the EOS symbol is within the vocab, and it is used to stop generation)\n",
        "        UNK: symbol to be used as UNK (used for Laplace smoothing and backoff and whenever new words come in a test time)\n",
        "        seed: seed for numpy random state (used in the sampling algorithm)\n",
        "        \"\"\"\n",
        "        if ngram_size < 1:\n",
        "            raise ValueError(f\"ngram_size must be at least 1, got {ngram_size}\")\n",
        "        if ngram_size > 3:\n",
        "           raise ValueError(\"This implementation is not efficient enough, you will probably run out of memory if you try going beyond trigrams\")\n",
        "        self._order = ngram_size - 1\n",
        "        self._BOS = BOS\n",
        "        self._EOS = EOS\n",
        "        self._UNK = UNK\n",
        "        self._alpha = alpha   \n",
        "        self._backoff = backoff\n",
        "        \n",
        "        # Used to store cpds\n",
        "        self._cpds = None\n",
        "        # Used to store the vocabulary\n",
        "        self._word2int = {\n",
        "            EOS: 0,\n",
        "            UNK: 1,\n",
        "        }\n",
        "        self._EOS_ID = 0\n",
        "        self._UNK_ID = 1\n",
        "        # Used to store the vocabulary\n",
        "        self._words = [EOS, UNK]\n",
        "        self._rng = np.random if seed is None else np.random.RandomState(seed)\n",
        "        \n",
        "    @property\n",
        "    def order(self):\n",
        "        \"\"\"1 minus the ngram_size\"\"\"\n",
        "        return self._order\n",
        "    \n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        \"\"\"Number of known words\"\"\"\n",
        "        return len(self._words)\n",
        "\n",
        "    def num_parameters(self):\n",
        "        \"\"\"Count the number of parameters in the model\"\"\"\n",
        "        if self._cpds is None:\n",
        "            return 0\n",
        "        return len(self._cpds) * self.vocab_size\n",
        "\n",
        "    def _get_cpd(self, history):\n",
        "        \"\"\"\n",
        "        This function returns the categorical parameter associated with a certain history.\n",
        "\n",
        "        :param history: a sequence of words (a tuple)\n",
        "        \"\"\"\n",
        "        if len(history) != self._order:\n",
        "            raise ValueError(f\"A history should have length {self._order}\")\n",
        "        # lookup the cpd for this history\n",
        "        cpd = self._cpds.get(history, None)        \n",
        "        if cpd is None:  # if the history is unknown \n",
        "            if self._backoff: # we either try to backoff to a shorter history\n",
        "                # we backoff one word at a time from left-to-right\n",
        "                unk_history = list(history)\n",
        "                for i in range(len(history)):\n",
        "                    unk_history[i] = self._UNK # forget another word \n",
        "                    cpd = self._cpds.get(history, None) # try finding a cpd for the remaining history\n",
        "                    if cpd is not None: # if we find, return it\n",
        "                        return cpd\n",
        "            # or return a uniform distribution\n",
        "            return np.ones(self.vocab_size) / self.vocab_size\n",
        "        else:\n",
        "            # if we have a cpd, we return it\n",
        "            return cpd\n",
        "                \n",
        "    def _get_parameter(self, history, word):\n",
        "        \"\"\"\n",
        "        This function returns the categorical parameter associated with a certain word given a certain history.\n",
        "\n",
        "        :param history: a sequence of words (a tuple)\n",
        "        :param word: a word (a str)\n",
        "        :return: a float representing P(word|history)\n",
        "            this function takes care of backoff if needed\n",
        "            and it takes care of unknown words\n",
        "        \"\"\"\n",
        "        cpd = self._get_cpd(history)        \n",
        "        return cpd[self._word2int.get(word, self._UNK_ID)]            \n",
        "        \n",
        "    def fit(self, data_stream):\n",
        "        \"\"\"\n",
        "        Fit the parameters of the NGramLM via MLE, possibly using Laplace smoothing and backoff.\n",
        "        data_stram: an iterable of sentences, each sentence is a list of tokens, each token is a string\n",
        "        \"\"\"\n",
        "        # we will iterate through data_stream twice, once for making a vocabulary\n",
        "        # once for gathering counts        \n",
        "        stream1, stream2 = tee(data_stream, 2)\n",
        "\n",
        "        # Make the vocabulary of known words\n",
        "        for sentence in stream1:\n",
        "            if type(sentence) not in [list, tuple]:\n",
        "                raise ValueError(\"Did you forget to tokenize your sentences\")\n",
        "            for word in sentence:\n",
        "                wid = self._word2int.get(word, None)\n",
        "                if wid is None: # if we haven't seen this word\n",
        "                    # it must be new, so we get a new id for it\n",
        "                    wid = len(self._words)\n",
        "                    # and store it\n",
        "                    self._word2int[word] = wid\n",
        "                    self._words.append(word)\n",
        "\n",
        "        V = len(self._words)\n",
        "        \n",
        "        # Make the dictionary of counts        \n",
        "        #  for each known history h we will store a V-dimensional count vector\n",
        "        #  (we could use a sparse data structure, but this implementation is meant\n",
        "        #   to be didactic, rather than efficient)\n",
        "        joint_counts = defaultdict(lambda: np.zeros(V))\n",
        "        for sentence in stream2:            \n",
        "            sentence = [self._BOS] * self._order  + sentence # pad with BOS tokens\n",
        "            if len(sentence) == 0 or sentence[-1] != self._EOS: # pad with EOS token\n",
        "                sentence.append(self._EOS)\n",
        "            l = len(sentence)\n",
        "            for i in range(self.order, l):  # for each content word (all after BOS)\n",
        "                # we have to make it a tuple otherwise we cannot use it as a key in a python dict\n",
        "                history = tuple(sentence[i - self._order: i])\n",
        "                # get the id of the next word\n",
        "                word = sentence[i]\n",
        "                word_id = self._word2int[word]\n",
        "                # count the joint occurrence\n",
        "                joint_counts[history][word_id] += 1\n",
        "                # reserve some events for the future                \n",
        "                if self._backoff:\n",
        "                    unk_history = list(history)\n",
        "                    # erase the history from left to right creating unk histories\n",
        "                    # this is a way to further smooth the model\n",
        "                    for j in range(self._order):\n",
        "                        unk_history[j] = self._UNK\n",
        "                        joint_counts[tuple(unk_history)][word_id] += 1\n",
        "        \n",
        "        # Make probabilities\n",
        "        cpds = dict()\n",
        "        for h, counts in joint_counts.items():\n",
        "            counts += self._alpha  # Laplace smoothing\n",
        "            cpds[h] = counts / np.sum(counts)\n",
        "        \n",
        "        self._cpds = cpds\n",
        "        \n",
        "    def log_prob(self, sentence):\n",
        "        \"\"\"\n",
        "        Compute the log probability of a sentence under this model. \n",
        "                \n",
        "        input: \n",
        "            sentence: a list of tokens\n",
        "        output:\n",
        "            log probability\n",
        "        \"\"\"\n",
        "        if type(sentence) not in [list, tuple]:\n",
        "                raise ValueError(\"Did you forget to tokenize your sentences\")\n",
        "        if self._cpds is None:\n",
        "            raise ValueError(\"Did you fit the model?\")\n",
        "        log_prob = 0.        \n",
        "        sentence = [self._BOS] * self._order + sentence  # pad sentence with BOS\n",
        "        if len(sentence) == 0 or sentence[-1] != self._EOS:\n",
        "            sentence.append(self._EOS) # pad sentence with EOS\n",
        "        for i in range(self._order, len(sentence)):  # there are two more words <start> and <end> of sentence\n",
        "            history = tuple(sentence[i - self._order: i])\n",
        "            word = sentence[i]        \n",
        "            ngram_probability = self._get_parameter(history, word)\n",
        "            # accumulate the log prob\n",
        "            log_prob += np.log(ngram_probability)    \n",
        "        return log_prob \n",
        "\n",
        "    def _sample(self, rng, trim_EOS=False, max_length=1000):\n",
        "        \"\"\"\n",
        "        Draw and return a single sample.\n",
        "\n",
        "        rng: a np random state\n",
        "        trim_EOS: whether we should trim the EOS symbol from the output\n",
        "        max_length: used to interrupt the sampler in case EOS is not drawn\n",
        "        \"\"\"\n",
        "        sentence = [self._BOS] * self._order\n",
        "        V = self.vocab_size\n",
        "        #uniform = np.ones(V) / V\n",
        "        eos_id = self._word2int[self._EOS]\n",
        "        for i in range(max_length):\n",
        "            h = tuple(sentence[len(sentence)-self._order:])\n",
        "            #cpd = self._cpds.get(h, uniform)\n",
        "            cpd = self._get_cpd(h)\n",
        "            k = rng.choice(V, p=cpd)\n",
        "            sentence.append(self._words[k])\n",
        "            if k == eos_id:\n",
        "                break\n",
        "        if trim_EOS:\n",
        "            return sentence[self._order:-1] \n",
        "        else:               \n",
        "            return sentence[self._order:]\n",
        "\n",
        "    def sample(self, num_samples=None, trim_EOS=False, max_length=1000, seed=None):\n",
        "        \"\"\"\n",
        "        num_samples: if None, returns a single sample\n",
        "            if more than 0, returns a list containing num_samples samples within\n",
        "            each sample is a list of tokens\n",
        "        trim_EOS: whether we should trim the EOS symbol from the output\n",
        "        max_length: used to interrupt the sampler in case EOS is not drawn\n",
        "        seed: use it to control the np random state\n",
        "        \"\"\"\n",
        "        if self._cpds is None:\n",
        "            raise ValueError(\"Did you fit the model?\")\n",
        "        rng = self._rng if seed is None else np.random.RandomState(seed)\n",
        "        if num_samples is None:\n",
        "            return self._sample(rng, trim_EOS=trim_EOS, max_length=max_length)\n",
        "        elif num_samples > 0:\n",
        "            return [self._sample(rng, trim_EOS=trim_EOS, max_length=max_length) for _ in range(num_samples)]\n",
        "        else:\n",
        "            raise ValueError(f\"num_samples should be a positive integer, got {num_samples}\")"
      ],
      "metadata": {
        "id": "w9oQOcRx6isu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a toy demonstration:"
      ],
      "metadata": {
        "id": "MfnjbYOJ9tKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_lm = NGramLM(1, seed=42)  "
      ],
      "metadata": {
        "id": "7FzZNYe5WF7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit the model by giving it a collection of tokenized sentences for MLE:"
      ],
      "metadata": {
        "id": "Kc0-e4DU9vcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "\n",
        "toy_tokenized_corpus = [\n",
        "    \"a a a\".split(),\n",
        "    \"b b b b b b\".split(),\n",
        "    \"c\".split()\n",
        "]\n",
        "for w, n in Counter(chain.from_iterable(toy_tokenized_corpus)).most_common():\n",
        "    print(f\"word={w} occurrences={n}\")\n",
        "print(f\"Number of EOS: {len(toy_tokenized_corpus)}\")\n",
        "print(f\"Average sentence length: {np.mean([len(x) for x in toy_tokenized_corpus])}\") "
      ],
      "metadata": {
        "id": "VXoqYM3Y-MHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_lm.fit(toy_tokenized_corpus)"
      ],
      "metadata": {
        "id": "RyEkk9yE5aSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the model to draw samples as well as to assess the probability of an outcome:"
      ],
      "metadata": {
        "id": "K2oUKPJ-9zPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = unigram_lm.sample(100, trim_EOS=True, seed=42)\n",
        "for x in xs[:10]:\n",
        "    print(f\"{unigram_lm.log_prob(x):.2f} {' '.join(x)}\")"
      ],
      "metadata": {
        "id": "7Lg_2LHY9HFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that language models aim at reproducing statistics of the observed data, or at least, the statistics that are within their capacity (given the conditional independence assumptions in place).\n",
        "\n",
        "A unigram LM can match observed word frequencies:"
      ],
      "metadata": {
        "id": "d-MdTKTF97g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter(chain.from_iterable(xs))\n",
        "total = sum(counter.values())\n",
        "for w, n in counter.most_common():\n",
        "    print(w, n/total)"
      ],
      "metadata": {
        "id": "Zr393WQv6oXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should also be able to learn the average sequence length (though the length distribution as a whole is limited to a Geometric law, which oftentimes is not the observed pattern)."
      ],
      "metadata": {
        "id": "BEEMoCz8AHV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(f\"Average sentence length from samples: {np.mean([len(x) for x in xs])}\")\n",
        "_ = plt.hist([len(x) for x in xs])\n",
        "_ = plt.xlabel(\"Length of sample\")\n",
        "_ = plt.ylabel(\"Frequency\")"
      ],
      "metadata": {
        "id": "VUmm_yVy8f_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can increase the NGram size and use some smoothing tricks:"
      ],
      "metadata": {
        "id": "syeKJLDmAU0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_lm = NGramLM(3, alpha=1e-3, backoff=True, seed=42)  \n",
        "trigram_lm.fit(toy_tokenized_corpus)\n",
        "xs3 = trigram_lm.sample(100, trim_EOS=True, seed=42)\n",
        "for x in xs3[:10]:\n",
        "    print(f\"{trigram_lm.log_prob(x):.2f} {' '.join(x)}\")"
      ],
      "metadata": {
        "id": "QvliXNEIDwUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter3 = Counter(chain.from_iterable(xs3))\n",
        "total3 = sum(counter3.values())\n",
        "for w, n in counter3.most_common():\n",
        "    print(w, n/total3)"
      ],
      "metadata": {
        "id": "5A9XKry4AkgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average sentence length from samples: {np.mean([len(x) for x in xs3])}\")\n",
        "_ = plt.hist([len(x) for x in xs3])\n",
        "_ = plt.xlabel(\"Length of sample\")\n",
        "_ = plt.ylabel(\"Frequency\")"
      ],
      "metadata": {
        "id": "REp4e3QhD_hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment "
      ],
      "metadata": {
        "id": "M4RP4rmBA0H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_unigram_lm = NGramLM(1, alpha=1e-3, seed=42)  \n",
        "brown_unigram_lm.fit(\n",
        "    tokenizer.encode(training, out_type=str) # here we tokenize with sentencepiece in order to keep the vocabulary size manageable\n",
        ")\n",
        "brown_unigram_lm.num_parameters()"
      ],
      "metadata": {
        "id": "m7iHXfwNDiu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will find made-up words, because the unigram LM might sample subword units that should not be next to one another, and then the tokenizer might end up merging them."
      ],
      "metadata": {
        "id": "nIYaDWgPFRFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in brown_unigram_lm.sample(10, seed=42):\n",
        "    print(f\"{brown_unigram_lm.log_prob(x):.2f} {tokenizer.decode(x)}\")"
      ],
      "metadata": {
        "id": "7UH7NvHpD10z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bigram LM should do better, but at the cost of a big increase in model size:"
      ],
      "metadata": {
        "id": "pMRBm2YTFdMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_bigram_lm = NGramLM(2, alpha=1e-3, backoff=True, seed=42)  \n",
        "brown_bigram_lm.fit(\n",
        "    tokenizer.encode(training, out_type=str)\n",
        ")\n",
        "brown_bigram_lm.num_parameters()"
      ],
      "metadata": {
        "id": "S-UGzy9sP4t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in brown_bigram_lm.sample(10, seed=42):\n",
        "    print(f\"{brown_bigram_lm.log_prob(x):.2f} {tokenizer.decode(x)}\")"
      ],
      "metadata": {
        "id": "CYGYNCTxBPES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A trigam LM should do even better, but the model is getting much too large. For a small vocabulary size (eg, 1000) you will be able to use this class, for larger vocabulary sizes our implementation is not sufficiently efficient and you will run out of memory."
      ],
      "metadata": {
        "id": "HEla4T-IMWhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_trigram_lm = NGramLM(3, alpha=1e-3, backoff=True, seed=42)  \n",
        "brown_trigram_lm.fit(\n",
        "    tokenizer.encode(training, out_type=str)\n",
        ")\n",
        "brown_trigram_lm.num_parameters()"
      ],
      "metadata": {
        "id": "HrzlWylKMjZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in brown_trigram_lm.sample(10, seed=42):\n",
        "    print(f\"{brown_trigram_lm.log_prob(x):.2f} {tokenizer.decode(x)}\")"
      ],
      "metadata": {
        "id": "qYTzxCDhMpfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"classic\"> **Graded exercises - Statistics from classic NGram LMs**\n",
        "\n",
        "For each model (unigram, bigram, and trigram LM):\n",
        "\n",
        "* Sample from the LM and compare the length distribution (for BPE-tokenized sentences) of the LM and the dev set. Use as many samples as you have sentences in the dev set. While sampling use the seed 42.\n",
        "* Using the sample samples: first BPE-detokenize the samples, then tokenize them at the white space (with python `.split()`), check how often the LM generates tokens that did not exist in the original NLTK training corpus, report the ratio of out-of-vocabulary (OOV) tokens. Compare both LMs in terms of this ratio.\n",
        "* Manually inspect a random subset of 30 of these new tokens and report how many are words that do not exist in English.\n",
        "* Comment on what you observe and relate your observations to technical aspects of the model.\n",
        "\n",
        "For hyperparameters use `alpha=1e-3, backoff=True` (these are not necessarily optimum, but we will not invest time searching for better ones)."
      ],
      "metadata": {
        "id": "3m67hhfLFopG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTRIBUTE YOUR SOLUTION"
      ],
      "metadata": {
        "id": "PVuyVk5CEJuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "There are a few ways to evaluate the performance of an LM.\n",
        "\n",
        "Sometimes you can plug it into an application (e.g., an auto-complete system or a system that ranks sentence for fluency), in those cases we can test whether that downstream application improves as we modify the language model. This is called *extrinsic* evaluation.\n",
        "\n",
        "To evaluate an LM independently from an application we need to evaluate its statistical properties in an attempt to determine how well the model fits the data, namely, how well it reproduces statistics of observed data. This is called *intrinsinc* evaluation. In this course, we are going to focus on intrinsic evaluation of the LM.\n",
        "\n",
        "We generally have access to 3 datasets: \n",
        "\n",
        "* Training is used for estimating $\\theta$.\n",
        "* Develpment is used to make choices during the design phase (choose hyperparameters such as smoothing technique, order of model, etc).\n",
        "* Test is used for measuring the accuracy of the final model.\n",
        "   \n",
        "One indication of the model's fitness to the data is the value of the model likelihood given novel sentences (e.g., sentence held-out from training). \n",
        "We assume this dataset $\\mathcal T$ of novel sentences consits of $K$ independent sentences each denoted $w_{1:l_k}^{(k)}$, then the model likelihood given $\\mathcal T$ is the probability mass that the model assigns to $\\mathcal T$: \n",
        "\n",
        "$\\prod_{k=1}^K f_X(w_{1:l_k}^{(k)}; \\theta)$\n",
        "\n",
        "or in form of the log-probability:\n",
        "\n",
        "$\\sum_{k=1}^K \\log f_{X}(w_{1:l_k}^{(k)}; \\theta)$\n",
        "\n",
        "Then define the log-likelihood as follows:\n",
        "\n",
        "$\\mathcal L_{\\mathcal T}(\\theta) = \\sum_{k=1}^K \\log f_X(w_{1:l_k}^{(k)}; \\theta)$\n",
        "\n",
        "\n",
        "Then the model that assings the higher $\\mathcal L$ given the test set is the one that's better predictive of future data, presumably that's the case because it found a better fit of the training data (a better compromise between memorisation and generalisation). \n",
        "\n",
        "In other words, given two probabilistic models, the one that assigns a higher probability to the test data is taken as intrinsically better. One detail we need to abstract away from is differences in factorisation of the models which may cause their likelihoods not to be comparable, but for that we will define *perplexity* below. \n",
        "\n",
        "The log likelihood is used because the probability of a particular sentence according to the LM can be a very small number, and the product of these small numbers can become even smaller, and it will cause numerical\n",
        "precision problems. \n",
        "\n",
        "\n",
        "**Perplexity** of a language model on a test set is the inverse probability of the test set, normalized\n",
        "by the number of tokens. Perplexity is a notion of average branching factor, thus an LM with low perplexity can be thought of as a *less confused* LM. That is, each time it introduces a word given some history it picks from a reduced subset of the entire vocabulary (in other words, it is more certain of how to continue from the history). \n",
        "\n",
        "If a dataset contains $t$ tokens where $t = \\sum_{k=1}^K l_k$, then the perplexity of the model given the test set is\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{PP}_{\\mathcal T}(\\theta) = \\exp\\left( -\\frac{1}{t} \\mathcal L_{\\mathcal T}(\\theta) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "The lower the perplexity, the better the model is. Comparisons in terms of perplexity are only fair if the models have the same vocabulary."
      ],
      "metadata": {
        "id": "nYcaZEYgJzJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"perplexity\"> **Graded exercise - Perplexity**\n",
        "\n",
        "* Implement the perplexity per token function, make sure it passess the two test cases\n",
        "* Use it to compare the unigram LM, the bigram LM and the trigram LM for the Brown corpus. Report a table with the number of parameters and the perplexity in the dev and in the test set. For hyperparameters use `alpha=1e-3, backoff=True` (these are not necessarily optimum, but we will not invest time searching for better ones)."
      ],
      "metadata": {
        "id": "35PavY4BIiZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity_per_token(model: NGramLM, dataset):\n",
        "    \"\"\"\n",
        "    model: a trained NGramLM\n",
        "    dataset: a tokenized corpus of sentences\n",
        "\n",
        "    Return the perplexity of the model given the dataset. This should be a single real number.\n",
        "    \"\"\"\n",
        "    raise ValueError(\"Implement me\")    "
      ],
      "metadata": {
        "id": "Tf65BLbXMjXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented a few test cases for you, so you can make sure your implementation of perplexity is correct."
      ],
      "metadata": {
        "id": "JrCJo2XPIfVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def toy_test_perplexity_per_token():\n",
        "    print(\"Testing with toy corpus\")\n",
        "    toy_tokenized_corpus = [\n",
        "        \"a a a\".split(),\n",
        "        \"b b b b b b\".split(),\n",
        "        \"c\".split()\n",
        "    ]\n",
        "    unigram_lm = NGramLM(1, seed=42)\n",
        "    unigram_lm.fit(toy_tokenized_corpus)\n",
        "    assert np.isclose(perplexity_per_token(unigram_lm, toy_tokenized_corpus), 4.954, 1e-3)\n",
        "\n",
        "    trigram_lm = NGramLM(3, alpha=1e-3, backoff=True, seed=42)  \n",
        "    trigram_lm.fit(toy_tokenized_corpus)\n",
        "    assert np.isclose(perplexity_per_token(trigram_lm, toy_tokenized_corpus), 2.055, 1e-3)\n",
        "\n",
        "toy_test_perplexity_per_token()"
      ],
      "metadata": {
        "id": "OCQEoGgGHuMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank \n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "def ptb_test_perplexity_per_token():\n",
        "    print(\"Testing with PTB without BPE tokenisation\")\n",
        "    ptb_training, ptb_dev, ptb_test = split_nltk_corpus(treebank)\n",
        "    rows = []\n",
        "    for ngram_size in [1, 2, 3]:\n",
        "        lm = NGramLM(ngram_size, alpha=1e-3, backoff=True, seed=42)\n",
        "        lm.fit((x.split() for x in ptb_training))\n",
        "        rows.append(['ptb', ngram_size, lm.vocab_size, lm.num_parameters(), perplexity_per_token(lm, [x.split() for x in ptb_dev]), perplexity_per_token(lm, [x.split() for x in ptb_test])])\n",
        "\n",
        "    print(\"Testing with PTB with BPE tokenisation\")\n",
        "    ptb_tokenizer = fit_vocabulary(ptb_training, vocab_size=1000)    \n",
        "    for ngram_size in [1, 2, 3]:        \n",
        "        lm = NGramLM(ngram_size, alpha=1e-3, backoff=True, seed=42)                                 \n",
        "        lm.fit(ptb_tokenizer.encode(ptb_training, out_type=str))\n",
        "        rows.append(['ptb', ngram_size, lm.vocab_size, lm.num_parameters(), perplexity_per_token(lm, ptb_tokenizer.encode(ptb_dev, out_type=str)), perplexity_per_token(lm, ptb_tokenizer.encode(ptb_test, out_type=str))])\n",
        "\n",
        "    print(tabulate(rows, headers=['corpus', 'ngram size', 'vocab size', 'num parameters', 'perplexity dev', 'perplexity test']))\n",
        "\n",
        "    print(\"\\n\\nRemarks:\")    \n",
        "    print(\"* with BPE tokenisation the model is not as often surprised with unknown symbols, this should improve perplexity a lot\")\n",
        "    print(\"* but note that the comparison of models with different vocabularies is not strictly fair\")\n",
        "    print(\"* note how the number of parameters grows fast as a function of vocab size and ngram size\")\n",
        "    print(\"* alpha and backoff can affect the perplexity, one would have to adjust those numbers, but we won't do that in this tutorial\")\n",
        "\n",
        "    assert np.isclose(rows[0][-2], 3953, 1), \"Did you change anything? Did you implement perplexity correctly?\"\n",
        "    assert np.isclose(rows[1][-2], 1889, 1), \"Did you change anything? Did you implement perplexity correctly?\"\n",
        "    assert np.isclose(rows[2][-2], 3215, 1), \"Did you change anything? Did you implement perplexity correctly?\"\n",
        "    assert np.isclose(rows[3][-2], 328, 1), \"Did you change anything? Did you implement perplexity correctly?\"\n",
        "    assert np.isclose(rows[4][-2], 143, 1), \"Did you change anything? Did you implement perplexity correctly?\"\n",
        "    assert np.isclose(rows[5][-2], 265, 1), \"Did you change anything? Did you implement perplexity correctly?\"\n",
        "\n",
        "ptb_test_perplexity_per_token()    "
      ],
      "metadata": {
        "id": "lob3OebVN1pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REPORT YOUR OWN EXPERIMENT USING THE BROWN CORPUS"
      ],
      "metadata": {
        "id": "_OdH8hD7XVdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural NGram LM\n",
        "\n",
        "Just like the classic $n$-gram LM, a neural $n$-gram LM makes a conditional independence assumption to simplify the factors in the chain rule. Rather than storying the probabilities values of observed $(h, w)$ pairs, the neural model stores the parameters necessary to predict those probabilities by transformation of the concatenation of the one-hot encodings of the words in the history. The pmf of the neural $n$-gram LM is defined as follows:\n",
        "\n",
        "\\begin{align}\n",
        "f_X(w_{1:l};\\theta) &= \\prod_{i=1}^l \\mathrm{Cat}(x_i|\\mathbf g(w_{i-n+1:i-1}; \\theta)) \n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf g$ is a neural network with parameters $\\theta$, it maps a tuple of words $x_{i-n+1:i-1}$, from a vocabulary of $V$ known words, to a $V$-dimensional probability vector.\n",
        "\n",
        "This neural network is typically implemented as follows:\n",
        "\n",
        "* embed each word in the history into a $D$-dimensional space: $\\mathbf e_j = \\mathrm{embed}_D(w_j; \\theta_{\\text{in}})$\n",
        "* concatenate the word embeddings for the words in the history: $\\mathbf u_i = \\mathrm{concat}(\\mathbf e_{i-n+1}, \\ldots, \\mathbf e_{i-1})$\n",
        "* use a single-layer feed-forward NN to transform the history encoding $\\mathbf u_i$ into a vector of $V$ logits: $\\mathbf s = \\mathrm{ffnn}_V(\\mathbf u_i; \\theta_{\\text{out}})$\n",
        "* use softmax to obtain probabilities for the possible words at the $i$th position: $\\mathbf g(w_{i-n+1:i-1}; \\theta)$\n",
        "* the parameters are $\\theta = \\theta_{\\text{in}} \\cup \\theta_{\\text{out}}$ where \n",
        "    * $\\theta_{\\text{in}}$ is an embedding matrix $\\mathbf E \\in \\mathbb R^{V\\times D}$\n",
        "    * $\\theta_{\\text{out}}$ are the parameters of the hidden layer $\\mathbf W^{[1]} \\in \\mathbb R^{H\\times (n-1)D}$ and $\\mathbf b^{[1]} \\in \\mathbb R^H$, and the parameters of the output layer $\\mathbf W^{[2]} \\in \\mathbb R^{V\\times H}$ and $\\mathbf b^{[2]} \\in \\mathbb R^V$\n",
        "\n",
        "\n",
        "Next we implement this in PyTorch using `torch.nn.Embedding` for the embedding layer and `torch.nn.Linear` for the affine transformations inside the FFNN. As non-linearity for hidden layers we will use ReLU."
      ],
      "metadata": {
        "id": "JlqKGKLn1FSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_all(seed=42):    \n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)    \n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "seed_all()"
      ],
      "metadata": {
        "id": "z_z8N_ZpTNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf3iDZ-nhL9E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as td\n",
        "import torch.optim as opt\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ungraded exercise** Study the NeuralNGram LM class below and complete the code for the forward pass and for the loss function. We have a test case that you can use to check your implementation. Once you are satisfied, compare your solution to ours, if they differ understand what you did wrong and use ours for the rest of the notebook."
      ],
      "metadata": {
        "id": "Lx0KNq86diKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNGramLM(nn.Module):\n",
        "\n",
        "    def __init__(self, ngram_size: int, vocab_size, embedding_dim: int, hidden_size: int, pad_id=0, bos_id=1, eos_id=2):\n",
        "        \"\"\"\n",
        "        ngram_size: longest ngram\n",
        "        vocab_size: number of known words\n",
        "        embedding_dim: dimensionality of word embeddings\n",
        "        hidden_size: dimensionality of hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert ngram_size > 1, \"This class expects at least ngram_size 2\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.ngram_size = ngram_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.pad = pad_id\n",
        "        self.bos = bos_id\n",
        "        self.eos = eos_id\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
        "        self.logits_predictor = nn.Sequential(\n",
        "            nn.Linear((ngram_size - 1) * embedding_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, vocab_size),\n",
        "        )\n",
        "\n",
        "    def num_parameters(self):\n",
        "        return sum(np.prod(theta.shape) for theta in self.parameters())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameterise the conditional distributions over X[i] given history x[:i] for i=1...I.\n",
        "\n",
        "        This procedure takes care that the ith output distribution conditions only on the n-1 observations before x[i].\n",
        "        It also takes care of padding to the left with BOS symbols.\n",
        "\n",
        "        x: [batch_size, max_length]\n",
        "\n",
        "        Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
        "        \"\"\"\n",
        "        # The inputs to the FFNN are the ngram_size-1 previous words:\n",
        "\n",
        "        # Create a sequence of BOS symbols to be prepended to x.\n",
        "        # [batch_size, ngram_size - 1]\n",
        "        bos = torch.full((x.shape[0], self.ngram_size - 1), self.bos, device=x.device)\n",
        "        # [batch_size, max_length + ngram_size - 1]\n",
        "        _x = torch.cat([bos, x], 1)\n",
        "\n",
        "        # For each output step, we will have ngram_size - 1 inputs, so we collect those from x\n",
        "        # [batch_size, max_length, ngram_size - 1]\n",
        "        inputs = torch.cat([_x.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(x.shape[0], 1, -1) for i in range(x.shape[1])], 1)\n",
        "        \n",
        "        raise NotImplementedError(\"Use the layers in this model to predict a batch `s` of logits with shape [batch_size, max_length, vocab_size] for a batch of `inputs` with shape [batch_size, max_length, ngram_size - 1]\")\n",
        "        s = None  # use your own implementation (or copy ours from the answer model below)\n",
        "\n",
        "        # For numerical stability, we prefer to parameterise the Categorical using logits, rather than probs.\n",
        "        # It would be equivalent (up to numerical precision) to use: Categorical(probs=F.softmax(s, -1))\n",
        "        return td.Categorical(logits=s)\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        \"\"\"\n",
        "        Computes the log probability of each sentence in a batch.\n",
        "\n",
        "        x: [batch_size, max_length]\n",
        "        \"\"\"\n",
        "        # [batch_size, max_length]\n",
        "        logp = self(x).log_prob(x)\n",
        "        # [batch_size]\n",
        "        logp = torch.where(x != self.pad, logp, torch.zeros_like(logp)).sum(-1)\n",
        "        return logp    \n",
        "\n",
        "    def sample(self, batch_size=1, max_length=50):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():  # sampling discrete outcomes is not differentiable\n",
        "            # Reserve memory for the samples (it's not important what symbol to use, I use BOS for clarity)\n",
        "            x = torch.full((batch_size, max_length), self.bos, device=self.embed.weight.device) \n",
        "            # Keeps track of which samples are complete (i.e., already include EOS)\n",
        "            complete = torch.zeros(batch_size, device=self.embed.weight.device)\n",
        "\n",
        "            for i in range(max_length):\n",
        "                # We condition on x[:,:i] (the prefixes) and parameterise Categoricals per step\n",
        "                #  then sample tokens. This will sample all tokens (including tokens in the prefix), \n",
        "                #  but we are only interested in the 'current' one, which we use to udpate our\n",
        "                #  actual sample x\n",
        "                # [batch_size, length]\n",
        "                x_i = self(x[:,:i+1]).sample()\n",
        "                # Here we update the current token to something freshly sampled\n",
        "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
        "                x[:, i] = x_i[:, i] * (1 - complete)\n",
        "                # Here we update the state of the sentence (i.e., complete or not).\n",
        "                complete = (complete.bool() + (x_i[:, i] == self.eos)).float()\n",
        "            \n",
        "            return x\n",
        "\n",
        "    def loss(self, x):   \n",
        "        \"\"\"\n",
        "        Compute a scalar loss from a batch of sentences.\n",
        "        The loss is the negative log likelihood of the model estimated on a single batch:\n",
        "            - 1/batch_size * \\sum_{s} log P(x[s]|theta)\n",
        "\n",
        "        x: [batch_size, max_length] \n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Implement me!\")"
      ],
      "metadata": {
        "id": "vPa1nCVBhP4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "toy_lm = NeuralNGramLM(ngram_size=3, vocab_size=tokenizer.vocab_size(), embedding_dim=12, hidden_size=7)\n",
        "toy_lm.num_parameters()"
      ],
      "metadata": {
        "id": "syu-Z_-PkNwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I use 1 as EOS and 0 as BOS/PAD\n",
        "obs = torch.tensor(\n",
        "    [[5, 7, 6, 2, toy_lm.eos, toy_lm.pad],\n",
        "    [4, 5, 7, 4, 6, toy_lm.eos]]\n",
        ")"
      ],
      "metadata": {
        "id": "FRSWqNdmnfs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the forward pass is correct"
      ],
      "metadata": {
        "id": "EamB0ca0mC_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert type(toy_lm(obs)) is td.Categorical, \"Did you change the return type?\"\n",
        "assert torch.allclose(torch.sum(toy_lm(obs).probs, -1), torch.ones_like(obs).float(), 1e-3), \"Your probabilities do not sum to 1\"\n",
        "assert toy_lm(obs).probs.shape == obs.shape + (tokenizer.vocab_size(),), \"The shape should be [2, 6, vocab_size]\""
      ],
      "metadata": {
        "id": "9RQIYa-plZ7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can estimate the loss using the 2 observations above:"
      ],
      "metadata": {
        "id": "K_Dnm6SW9VM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_lm.loss(obs)"
      ],
      "metadata": {
        "id": "dcJ_Dw5ykWmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert type(toy_lm.loss(obs)) is torch.Tensor, \"Your loss should be a torch tensor\"\n",
        "assert toy_lm.loss(obs).requires_grad, \"Your loss should be differentiable\"\n",
        "assert toy_lm.loss(obs).shape == tuple(), \"Your loss should be a scalar tensor\"\n",
        "assert np.isclose(toy_lm.loss(obs).item(), 37, 1), \"Without training, with seed 42, and the obs we gave you, your loss should be close to 37. If this is not correct, check with your TA.\""
      ],
      "metadata": {
        "id": "CYWYSFLOYIq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution for forward method </b> </summary>\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "    \"\"\"\n",
        "    Parameterise the conditional distributions over X[i] given history x[:i] for i=1...I.\n",
        "\n",
        "    This procedure takes care that the ith output distribution conditions only on the n-1 observations before x[i].\n",
        "    It also takes care of padding to the left with BOS symbols.\n",
        "\n",
        "    x: [batch_size, max_length]\n",
        "\n",
        "    Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
        "    \"\"\"\n",
        "    # The inputs to the FFNN are the ngram_size-1 previous words:\n",
        "\n",
        "    # Create a sequence of BOS symbols to be prepended to x.\n",
        "    # [batch_size, ngram_size - 1]\n",
        "    bos = torch.full((x.shape[0], self.ngram_size - 1), self.bos, device=x.device)\n",
        "    # [batch_size, max_length + ngram_size - 1]\n",
        "    _x = torch.cat([bos, x], 1)\n",
        "\n",
        "    # For each output step, we will have ngram_size - 1 inputs, so we collect those from x\n",
        "    # [batch_size, max_length, ngram_size - 1]\n",
        "    inputs = torch.cat([_x.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(x.shape[0], 1, -1) for i in range(x.shape[1])], 1)\n",
        "\n",
        "    # Embed the input histories\n",
        "    # [batch_size, max_length, ngram_size - 1, D]\n",
        "    e = self.embed(inputs)\n",
        "    # [batch_size, max_length, (ngram_size - 1) * D]\n",
        "    e = e.reshape(x.shape + (-1,))\n",
        "\n",
        "    # Compute the V-dimensional scores (logits) \n",
        "    # [batch_size, max_length, V]\n",
        "    s = self.logits_predictor(e)\n",
        "\n",
        "    # For numerical stability, we prefer to parameterise the Categorical using logits, rather than probs.\n",
        "    # It would be equivalent (up to numerical precision) to use: Categorical(probs=F.softmax(s, -1))\n",
        "    return td.Categorical(logits=s)\n",
        "```    \n",
        "</details>"
      ],
      "metadata": {
        "id": "1W_bVC0Dmn5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary> <b> Solution for loss method </b> </summary>\n",
        "\n",
        "```python\n",
        "def loss(self, x):   \n",
        "    \"\"\"\n",
        "    Compute a scalar loss from a batch of sentences.\n",
        "    The loss is the negative log likelihood of the model estimated on a single batch:\n",
        "        - 1/batch_size * \\sum_{s} log P(x[s]|theta)\n",
        "\n",
        "    x: [batch_size, max_length]    \n",
        "    \"\"\"\n",
        "    # [batch_size]\n",
        "    loss = - self.log_prob(x)    \n",
        "    # []\n",
        "    return loss.mean(0)        \n",
        "```        \n",
        "</details>"
      ],
      "metadata": {
        "id": "xCp-k3TIezCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also obtain samples from the model:"
      ],
      "metadata": {
        "id": "a_tNbvLK9cb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_lm.sample(5, max_length=20)"
      ],
      "metadata": {
        "id": "En3VysIstjW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot statistics of sampled strings, for example, length and word frequency:"
      ],
      "metadata": {
        "id": "2BceKrvc9gBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = toy_lm.sample(100, max_length=20)\n",
        "_ = plt.hist(( sample > 0).sum(-1), bins='auto')\n",
        "_ = plt.xlabel(\"Length distribution before training\")"
      ],
      "metadata": {
        "id": "0K2BaK-m4bay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "flat_samples = sample.flatten()\n",
        "word_counts = Counter(flat_samples[flat_samples > 0].numpy())\n",
        "wcs = np.array([(w, n) for w, n in word_counts.items()])\n",
        "_ = plt.plot(wcs[:,0], wcs[:,1]/wcs[:,1].sum(), 'x')\n",
        "_ = plt.xlabel(\"Word id\")\n",
        "_ = plt.ylabel(\"Frequency before training\")"
      ],
      "metadata": {
        "id": "ODoUOx0g456b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can train the model using gradient-based optimisation:"
      ],
      "metadata": {
        "id": "vI5-62hq9kQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = opt.Adam(toy_lm.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "XEfQ5Mltss4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tqdm(range(1000)) as bar:\n",
        "  for _ in bar:\n",
        "    toy_lm.train()\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    loss = toy_lm.loss(obs)\n",
        "    bar.set_postfix({'loss': f\"{loss:.2f}\" } )\n",
        "    \n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    "
      ],
      "metadata": {
        "id": "i8Az18f8rSX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then our samples should look less arbitrary"
      ],
      "metadata": {
        "id": "YT589emQ9piL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_lm.sample(10, 8)"
      ],
      "metadata": {
        "id": "NF1mRmgiklRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And statistics such as length and word frequency should be closer to training data:"
      ],
      "metadata": {
        "id": "TEOZNzjz9tNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = toy_lm.sample(100, max_length=20)\n",
        "_ = plt.hist((sample > 0).sum(-1), bins='auto')\n",
        "_ = plt.xlabel(\"Length distribution after training\")"
      ],
      "metadata": {
        "id": "yAbkxJb-rKA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_samples = sample.flatten()\n",
        "word_counts = Counter(flat_samples[flat_samples > 0].numpy())\n",
        "wcs = np.array([(w, n) for w, n in word_counts.items()])\n",
        "_ = plt.plot(wcs[:,0], wcs[:,1]/wcs[:,1].sum(), 'x')\n",
        "_ = plt.xlabel(\"Word id\")\n",
        "_ = plt.ylabel(\"Frequency after training\")"
      ],
      "metadata": {
        "id": "6JZ-zE0A40M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will conduct an experiment with an actual corpus, we better use GPU support for that (on Google Colab you change the runtime to GPU)."
      ],
      "metadata": {
        "id": "mdxlZ_imZHCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_device = torch.device('cuda:0')\n",
        "my_device"
      ],
      "metadata": {
        "id": "pKFUw21bYCUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did in the PyTorch tutorial, we will create a `Dataset` object and a `DataLoader` for batching:"
      ],
      "metadata": {
        "id": "BZVMkq0GZOqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "LhOlEqZbYTmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus(Dataset):\n",
        "\n",
        "    def __init__(self, corpus, tokenizer):\n",
        "        \"\"\"\n",
        "        In PyTorch we better always manipulate numerical codes, rather than text.\n",
        "        So, our Corpus object will contain a tokenizer that converts words to codes.\n",
        "\n",
        "        corpus: a list of sentences, each a string\n",
        "        tokenizer: a BPE tokenizer from sentencepiece\n",
        "        \"\"\"\n",
        "        self.corpus = list(corpus)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return corpus[idx] but BPE tokenized, converted to codes, and with the EOS symbol\"\"\"\n",
        "        return self.tokenizer.encode(self.corpus[idx], add_eos=True)"
      ],
      "metadata": {
        "id": "EPBI2HUXYkbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our neural models are **a lot** more parameter efficient than our classic models, so we could use a larger vocabulary, but to keep the tutorial lightweight, we will still use a vocabulary of size 1000."
      ],
      "metadata": {
        "id": "fJ02JHwwc6Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = fit_vocabulary(training, vocab_size=1000)\n",
        "training_tok = Corpus(training, tokenizer)\n",
        "dev_tok = Corpus(dev, tokenizer)\n",
        "test_tok = Corpus(test, tokenizer)\n",
        "len(training_tok), len(dev_tok), len(test_tok)"
      ],
      "metadata": {
        "id": "6z7xQXhHpSlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we manipulate sequences of variable length, we need to \"pad\" them all to the same length. That's because to batch them using tensors they need to look like as if they did have the same length. \n",
        "\n",
        "We do that with a special symbol that will get ignored later on."
      ],
      "metadata": {
        "id": "FgwGUkWQZtuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_longest(sequences, pad_id=0):\n",
        "    \"\"\"\n",
        "    Take a list of coded sequences and returns a torch tensor where \n",
        "    every sentence has the same length (by means of using PAD tokens)\n",
        "    \"\"\"\n",
        "    longest = max(len(seq) for seq in sequences)\n",
        "    return torch.tensor([seq + [pad_id] * (longest - len(seq)) for seq in sequences])"
      ],
      "metadata": {
        "id": "IpR_1AkiadQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See what this does to the first few sentences in the batch (they should end with a few 0s, indicating they are shorter than the longest sentence in the batch)."
      ],
      "metadata": {
        "id": "Cetno7rTaKPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad_to_longest([training_tok[0], training_tok[1], training_tok[2]])"
      ],
      "metadata": {
        "id": "B7zeMNPgaEKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we can convert batches of sentences to codes and guarantee they have the same length, we can construct a data loader to create mini batches at random:"
      ],
      "metadata": {
        "id": "-_duaGn_aOhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batcher = DataLoader(training, batch_size=10, shuffle=True, collate_fn=pad_to_longest)\n",
        "len(batcher)"
      ],
      "metadata": {
        "id": "3sHa1d5WpkdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need a batched PyTorch version of perplexity, to make sure you can run experiments with the correct code, we implement it here for you:"
      ],
      "metadata": {
        "id": "9FDyHpC7a2F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(model: NeuralNGramLM, dl, device):\n",
        "    \"\"\"\n",
        "    model: an instance of NeuralNGramLM\n",
        "    dl: a data loader for the heldout data\n",
        "    device: the PyTorch device where the model is stored\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    total_log_prob = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            total_tokens += (batch > 0).float().sum()\n",
        "            total_log_prob = total_log_prob + model.log_prob(batch.to(device)).sum()\n",
        "    return torch.exp(-total_log_prob / total_tokens)"
      ],
      "metadata": {
        "id": "Foxt9hp0n5J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have the training loop (already fully implemented for you). Do study it."
      ],
      "metadata": {
        "id": "x7u12R0t3mAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_neural_model(model, optimiser, training_corpus, dev_corpus, batch_size=200, num_epochs=10, device=torch.device('cuda:0')):\n",
        "    # we use the training data in random order for parameter estimation\n",
        "    batcher = DataLoader(training_corpus, batch_size=batch_size, shuffle=True, collate_fn=pad_to_longest)\n",
        "    # we use the dev data for evaluation during training (no need for randomisation here)\n",
        "    dev_batcher = DataLoader(dev_corpus, batch_size=batch_size, shuffle=False, collate_fn=pad_to_longest)\n",
        "\n",
        "    total_steps = num_epochs * len(batcher)\n",
        "    log = defaultdict(list)\n",
        "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
        "    log['ppl'].append(ppl)\n",
        "    \n",
        "    step = 0\n",
        "\n",
        "    with tqdm(range(total_steps)) as bar:\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            for batch in batcher:\n",
        "                lm.train()\n",
        "                optimiser.zero_grad()\n",
        "                \n",
        "                loss = lm.loss(batch.to(device))\n",
        "                        \n",
        "                loss.backward()\n",
        "                optimiser.step()\n",
        "\n",
        "                bar.set_postfix({'loss': f\"{loss.item():.2f}\", 'ppl': f\"{ppl:.2f}\"} )\n",
        "                bar.update()  \n",
        "                log['loss'].append(loss.item())\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    ppl = perplexity(lm, dev_batcher, device=device).item()\n",
        "                    log['ppl'].append(ppl)\n",
        "                \n",
        "                step += 1\n",
        "                \n",
        "    ppl = perplexity(lm, dev_batcher, device=device).item()\n",
        "    log['ppl'].append(ppl)\n",
        "    return log            "
      ],
      "metadata": {
        "id": "ZQR5TGAqd-HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment\n",
        "\n",
        "Here we demonstrate how to train and evaluate a model. \n",
        "\n",
        "After that you will conduct an experiment."
      ],
      "metadata": {
        "id": "RwXhhAUChsKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On GPU, this should take just about 2 minutes:"
      ],
      "metadata": {
        "id": "sko-agWU5Giv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all() # reset random number generators before creating your model and training it\n",
        "\n",
        "# Create our LM\n",
        "lm = NeuralNGramLM(\n",
        "    ngram_size=3, \n",
        "    vocab_size=tokenizer.vocab_size(), \n",
        "    embedding_dim=32, \n",
        "    hidden_size=64, \n",
        "    pad_id=tokenizer.pad_id(),\n",
        "    bos_id=tokenizer.bos_id(),\n",
        "    eos_id=tokenizer.eos_id(),\n",
        ").to(my_device)\n",
        "\n",
        "# construct an Adam optimiser\n",
        "optimiser = opt.Adam(lm.parameters(), lr=5e-3)\n",
        "\n",
        "print(\"Model\")\n",
        "print(lm)\n",
        "# report number of parameters\n",
        "print(\"Model size:\", lm.num_parameters())\n",
        "\n",
        "log = train_neural_model(\n",
        "    lm, optimiser, training_tok, dev_tok, \n",
        "    batch_size=200, num_epochs=10, \n",
        "    device=my_device\n",
        ")\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "_ = axs[0].plot(np.arange(len(log['loss'])), log['loss'])\n",
        "_ = axs[0].set_xlabel('steps')\n",
        "_ = axs[0].set_ylabel('training loss')\n",
        "_ = axs[1].plot(np.arange(len(log['ppl'])), log['ppl'])\n",
        "_ = axs[1].set_xlabel('steps (in 100s)')\n",
        "_ = axs[1].set_ylabel('model ppl given dev')\n",
        "_ = fig.tight_layout(h_pad=2, w_pad=2)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n# Samples\\n\\n\")\n",
        "for x in lm.sample(10, 20):\n",
        "    print(tokenizer.decode([int(a) for a in x]))\n",
        "\n",
        "test_batcher = DataLoader(test_tok, batch_size=100, shuffle=False, collate_fn=pad_to_longest)\n",
        "print(\"\\n\\n Model perplexity given test set\", perplexity(lm, test_batcher, device=my_device).item())"
      ],
      "metadata": {
        "id": "6EORnlrW3vAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"neural\">  **Graded exercise - Neural NGram LM**\n",
        "\n",
        "Train NeuralNGram LMs using the Brown corpus and a BPE vocabulary of 1000 tokens.\n",
        "\n",
        "* Train 2-gram LMs, 3-gram LMs, 4-gram LMs and 5-gram LMs\n",
        "* For each model, train it with \n",
        "    * 32 dimensions for embeddings and 64 dimensions for hidden size\n",
        "    * 64 dimensions for embeddings and 128 dimensions for hidden size    \n",
        "* For each model, plot the training loss and perplexity using the dev set throughout training. \n",
        "* For trained models, report a table with embedding size, hidden size, ngram-size, model size (in number of parameters), model perplexity using the dev set, and model perplexity using the test set\n",
        "* As always, write a few discussion points. Here are some ideas: comment on how increasing ngram-size is relatively cheap here (compared to the classic model), comment on the effect of increasing ngram-size in model perplexity, was the effect similar or different in the classic case? \n",
        "\n",
        "\n",
        "Using the best model you have (measured in perplexity given test set):\n",
        "\n",
        "* Draw samples from the model (use the same number of sentences as in the dev set)\n",
        "* Compare the length distribution of generated data with the length distribution of the dev set (use BPE-tokenization for that)\n",
        "* BPE-detokenize your samples and re-tokenize it using python `.split()`, investigate the rate at which the model creates words that did not exist in the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "QjGAF2hQegly"
      }
    }
  ]
}