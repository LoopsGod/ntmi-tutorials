{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Werkcollege1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGhr6gLUFipZ2MrR3GfYtU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/Werkcollege1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we practice identifying and estimating parameters for statistical models of linguistic data."
      ],
      "metadata": {
        "id": "5_rEDxdB4m4X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXSUHL98yNvK"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "JZhQ6JkByOh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK gives us access to the [WordNet](https://www.nltk.org/howto/wordnet.html), a resource containing rich information about the lexicon of various languages."
      ],
      "metadata": {
        "id": "vxxZoSKH4mxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw')"
      ],
      "metadata": {
        "id": "Ja3dE0Qb4yLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "id": "pGYpzsz2yRhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "z9jBtqCRzJPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, the WordNet has a repository of English lemmas, and these lemmas are categorised into parts-of-speech (syntactic function) such as nouns (n) or verbs (v)."
      ],
      "metadata": {
        "id": "q_N4oK3z4793"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(1 for _ in wn.all_lemma_names()), sum(1 for _ in wn.all_lemma_names('n')), sum(1 for _ in wn.all_lemma_names('v'))"
      ],
      "metadata": {
        "id": "NL-SHpEEyXIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check some nouns:"
      ],
      "metadata": {
        "id": "JHm2NIMN554M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lemma in list(wn.all_lemma_names('n'))[50:60]:\n",
        "  print(lemma)"
      ],
      "metadata": {
        "id": "uDYcVGN85I4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And some verbs:"
      ],
      "metadata": {
        "id": "U3AsyK0457Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lemma in list(wn.all_lemma_names('v'))[50:60]:\n",
        "  print(lemma)"
      ],
      "metadata": {
        "id": "PkiZH6In52pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few other languages are also part of the resource:"
      ],
      "metadata": {
        "id": "iMkwVENN58he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lemma in list(wn.all_lemma_names('v', lang='nld'))[50:60]:\n",
        "  print(lemma)"
      ],
      "metadata": {
        "id": "w_gr5DlB587u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyse the English data:"
      ],
      "metadata": {
        "id": "2jK5dl_26ZDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nouns = [lemma for lemma in wn.all_lemma_names('n')]\n",
        "verbs = [lemma for lemma in wn.all_lemma_names('v')]"
      ],
      "metadata": {
        "id": "gVGW0v7QzVmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_synsets_nouns = np.array([len(wn.synsets(lemma)) for lemma in nouns])\n",
        "num_synsets_verbs = np.array([len(wn.synsets(lemma)) for lemma in verbs])"
      ],
      "metadata": {
        "id": "-MyLoITO2lAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(8, 4))\n",
        "_ = ax[0].hist(num_synsets_nouns, bins='auto')\n",
        "_ = ax[0].set_xlabel('Number of senses for nouns')\n",
        "_ = ax[1].hist(num_synsets_verbs, bins='auto')\n",
        "_ = ax[1].set_xlabel('Number of senses for verbs')"
      ],
      "metadata": {
        "id": "3S76Dzamy5JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.describe(num_synsets_nouns)"
      ],
      "metadata": {
        "id": "642pxuCZ0Q1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.describe(num_synsets_verbs)"
      ],
      "metadata": {
        "id": "YugQdewj3H_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of senses decays fairly quickly. Nouns in particular do not seem as polysemous as verbs in English. The distribution for nouns is very concentrated at 1. The distribution for verbs is smoother.\n",
        "\n",
        "The number of observations in each bin decreases rather fast with increase in number of senses. Perhaps exponentially fast?"
      ],
      "metadata": {
        "id": "oZNiZulp6eoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(8, 4))\n",
        "_ = ax[0].hist(num_synsets_nouns, bins='auto', log=True)\n",
        "_ = ax[0].set_xlabel('Number of senses for nouns')\n",
        "_ = ax[1].hist(num_synsets_verbs, bins='auto', log=True)\n",
        "_ = ax[1].set_xlabel('Number of senses for verbs')"
      ],
      "metadata": {
        "id": "NyCrjlox62Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the y-axis in log scale helps us see that an exponential decay is plausible. \n",
        "\n",
        "Though we also see that nouns still concentrate badly at 1."
      ],
      "metadata": {
        "id": "waxz06Yp7KMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture the behaviour of the data in terms of \"number of senses\" in a statistical law, we need to look for a law that\n",
        "\n",
        "1. supports integers starting from 1\n",
        "2. the pmf decays (roughly) exponentially quickly\n",
        "3. the variance is not too high\n",
        "\n",
        "We can contrast properties of known laws against these objectives.\n",
        "\n",
        "The Binomial distribution does not seem appropriate: its generative story involves a known number of fixed draws, which we don't have here.\n",
        "\n",
        "The Geometric, the Poisson, and the Zipf distributions are possibly appropriate in terms of goal number 1. \n",
        "\n",
        "If we look carefuly, it looks like the mode of the data samples is always at 1, namely, the majority of nouns/verbs have a single sense. The Poisson distribution seems less adequate now. The only Poisson with a mode at 1 is Poisson(1), and if we tried to pick another Poisson (for example, in an attempt to find one with mean and variance more similar to the data, we would have to give up on matching the observed mode). \n",
        "\n",
        "The Geometric and the Zipf remain candidates for both distributions have their modes fixed at 1, for any choice of parameter. Both have pmfs that decay quickly, (roughly) exponentially quickly, so the final decision will have to depend on other properties."
      ],
      "metadata": {
        "id": "dvbQHv9A7vUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing to know about the Zipf law is that it has extremely [heavy tails](https://en.wikipedia.org/wiki/Heavy-tailed_distribution). Draws from Zipf will often deviate drammatically far from 1, even though 1 will remain the most frequent outcome.\n",
        "\n",
        "Let's enumerate a few Zipf distributions, sample from them, and describe some properties of the samples:"
      ],
      "metadata": {
        "id": "ulXP76E_-wwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for power in np.linspace(1.001, 1.5, 10):\n",
        "  x_ = st.zipf(power).rvs(size=10000)\n",
        "  print(f\"1st trial with Zipf({power:.4f})\", st.mode(x_), st.describe(x_))  \n",
        "  x_ = st.zipf(power).rvs(size=10000)\n",
        "  print(f\"2nd trial with Zipf({power:.4f})\", st.mode(x_), st.describe(x_))"
      ],
      "metadata": {
        "id": "vZS6XBeo9ej5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look the range of samples we get (property `minmax` of the describe result), those are clearly inadequate for the data we are trying to model.\n",
        "\n",
        "So, we have enough to continue with the Geometric as our first plausible attempt."
      ],
      "metadata": {
        "id": "k-QhWFmZAO_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the [Geometric MLE solution on Wikipedia](https://en.wikipedia.org/wiki/Geometric_distribution) and note that by default the scipy Geometric is the version with support $\\{1, 2, \\ldots \\}$ (not $\\{0, 1, \\ldots\\}$)."
      ],
      "metadata": {
        "id": "mnaZsaiGAZw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mle_geometric(x):\n",
        "  prob = 1 / np.mean(x)  # the scipy Geometric has support {1, ...} not {0, ...}\n",
        "  return st.geom(prob)"
      ],
      "metadata": {
        "id": "vjcwvt2i3KHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geom_n = mle_geometric(num_synsets_nouns)\n",
        "geom_n.args"
      ],
      "metadata": {
        "id": "qz4qc6nSzkzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geom_v = mle_geometric(num_synsets_verbs)\n",
        "geom_v.args"
      ],
      "metadata": {
        "id": "3Pv6qw9B3hIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_ = geom_n.rvs(size=num_synsets_nouns.size)\n",
        "v_ = geom_v.rvs(size=num_synsets_verbs.size)"
      ],
      "metadata": {
        "id": "xSwdzDzezu6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, we can capture the mean, after all the geometric parameter is directly estimated from the sample mean:"
      ],
      "metadata": {
        "id": "lhmOTVfM3xqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st.describe(n_)"
      ],
      "metadata": {
        "id": "m_MjdJL80iob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.describe(v_)"
      ],
      "metadata": {
        "id": "gJ89tl0T3vvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 2, sharex='col', sharey='col', figsize=(8, 4))\n",
        "_ = ax[0, 0].hist(num_synsets_nouns, bins='auto')\n",
        "_ = ax[0, 0].set_xlabel('Number of senses for nouns')\n",
        "_ = ax[0, 1].hist(num_synsets_verbs, bins='auto')\n",
        "_ = ax[0, 1].set_xlabel('Number of senses for verbs')\n",
        "\n",
        "_ = ax[1, 0].hist(n_, bins='auto')\n",
        "_ = ax[1, 0].set_xlabel('Geometric for nouns')\n",
        "_ = ax[1, 1].hist(v_, bins='auto')\n",
        "_ = ax[1, 1].set_xlabel('Geometric for verbs')"
      ],
      "metadata": {
        "id": "_KiMed3y09wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the Geometric assumption works better for verbs than for nouns.\n",
        "\n",
        "At this point in the course, we do not know enough to obtain a better fit for the nouns, but we will get there."
      ],
      "metadata": {
        "id": "x8mYVt7FAziJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_iBFlfIXA99Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}