{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/Rational_decision_makers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qU3xZtTeLb9"
   },
   "source": [
    "# Introduction to Decision Theory\n",
    "\n",
    "**ILOs**\n",
    "\n",
    "After this tutorial the student \n",
    "\n",
    "* understands how to make decisions under uncertainty \n",
    "* can implement a search procedure to maximise expected utility (or minimise expected loss) for common probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fTXZlphTFIG"
   },
   "source": [
    "In statistical decision theory a **rational decision maker** is one who acts as to maximise expected **utility** (or, equivalently, minimise expected **loss**).\n",
    "\n",
    "Suppose a decision maker has access to some information $X=x$ and lacks access to some other information captured by a random variable $Y$. The **uncertainty** of the decision maker is captured by a conditional probability distribution $Y|X=x$. The interests of the decision maker are captured by a utility function $u(y, h)$ or, equivalently, by a loss function $\\ell(y, h)$. \n",
    "\n",
    "The utility function assesses the benefit in choosing a candidate $h \\in \\mathcal Y$ when the true response is $y \\in \\mathcal Y$. Conversely, the loss function assesses the harm in choosing a candidate $h \\in \\mathcal Y$ when the true response is $y \\in \\mathcal Y$. In reality we *do not know* the true response, thus we let our own model fill in the response for us in expectation.\n",
    "\n",
    "This is the optimisation problem that rational decision makers solve:\n",
    "\\begin{align}\n",
    "y^\\star &= \\arg\\max_{h \\in \\mathcal Y} ~ \\mathbb E[u(Y, h)|X=x] \n",
    "\\end{align}\n",
    "if $Y$ is continuous, then this is\n",
    "\\begin{align}\n",
    "y^\\star &= \\arg\\max_{h \\in \\mathcal Y} ~ \\int p(y|x) u(y, h) \\mathrm{d}y\n",
    "\\end{align}\n",
    "where $p(y|x)$ is the probability density of $y$ given $x$; if $Y$ is discrete we have \n",
    "\\begin{align}\n",
    "y^\\star &= \\arg\\max_{h \\in \\mathcal Y} ~ \\sum_{y \\in \\mathcal Y} p(y|x) u(y, h) ~.\n",
    "\\end{align}\n",
    "Or, in English:\n",
    "* we search in the space of every possible values of $h$, which is every possible value of the random variable $Y$, for the one value whose expected utility is maximum\n",
    "* expected utility is the average utility against every possible value $y \\in \\mathcal Y$ of the random variable weighed by their probability density (or mass).\n",
    "\n",
    "Equivalently, but expressed in terms of a loss function, we have\n",
    "\\begin{align}\n",
    "y^\\star &= \\arg\\min_{h \\in \\mathcal Y} ~ \\mathbb E[\\ell(Y, h)|X=x]  ~.\n",
    "\\end{align}\n",
    "Or, in English:\n",
    "* we search in the space of every possible values of $h$, which is every possible value of the random variable $Y$, for the one value whose expected loss is minimum\n",
    "* expected loss is the average loss against every possible value $y \\in \\mathcal Y$ of the random variable weighed by their probability density (or mass).\n",
    "\n",
    "Solving the expected value can be difficult, especially for continuous random variables, since solving $\\mathbb E[u(Y, h)|X=x]$ for any given $h$ requires solving an integral. For discrete variables it is generally easier, provided that $\\mathcal Y$ is small. In general, we cannot count on expected values being easy to solve, but we can *always* estimate them via Monte Carlo (MC) if we have access to a procedure to draw samples from the model (which is available for most reasonable models). The MC estimate of an expected value is:\n",
    "\\begin{align}\n",
    "\\mathbb E[u(Y, h)|X=x]  &\\overset{\\text{MC}}{\\approx} \\frac{1}{S}\\sum_{s=1}^S u(y_s, h) \\quad \\text{ with }y_s \\sim P_{Y|X=x}\n",
    "\\end{align}\n",
    "the sample mean utility where we use $S$ samples from the conditional distribution (instead of the entire conditional distribution).\n",
    "\n",
    "Once we can compute expected utility for a single $h$, or at least estimate it using MC, we have to solve the *search* problem (that is, we need to find out which $h$ in $\\mathcal Y$ is the one with highest expected utility / or lowest expected loss). The search problem is in general intractable as well, but, whenever $\\mathcal Y$ is low-dimensional, we can at least approximate it using a grid $\\mathcal H \\subset \\mathcal Y$ of candidates:\n",
    "\\begin{align}\n",
    "y^\\text{grid} &= \\arg\\max_{h \\in \\mathcal H} ~ \\mathbb E[u(Y, h)|X=x] \\\\\n",
    " &\\overset{\\text{MC}}{\\approx} \\arg\\max_{h \\in \\text{grid}} ~ \\frac{1}{S}\\sum_{s=1}^S u(y_s, h) \\quad \\text{ with }y_s \\sim P_{Y|X=x}\n",
    "\\end{align}\n",
    "\n",
    "Next we demonstrate this with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uYdcE4MWnq6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_r8PfC-WoMP"
   },
   "source": [
    "# Categorical\n",
    "\n",
    "For the Categorical distribution we can exactly compute the expected value by enumeration of the terms in $\\mathcal Y$, we can also exactly search by having a grid that is exactly identical to $\\mathcal Y$. \n",
    "\n",
    "Here we will use a utility function that is the \"exact match\" function, it assigns utility of 1 when the candidate $h$ is equal to the target $y$, and 0 otherwise.\n",
    "\n",
    "This decision rule happens to return the mode of the Categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbRwGlB-Wpcq"
   },
   "outputs": [],
   "source": [
    "support = np.arange(10)  # mathcal Y\n",
    "probs = np.random.dirichlet(np.ones(10)) # a valid Categorical parameter\n",
    "assert np.all(probs > 0), \"Should be all positive\"\n",
    "assert np.isclose(np.sum(probs), 1., 1e-3), \"Should sum to 1\" \n",
    "grid = support # with 10 classes we can check each and every candidate\n",
    "\n",
    "def utility(y, h):\n",
    "    return np.array((y == h), dtype=float)  # this utility is 1 if the guess is equal to the true or zero otherwise\n",
    "\n",
    "def exact_expected_utility(h, support, probs):\n",
    "    return np.sum([utility(y, h) * probs[y] for y in support])\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(6, 10))\n",
    "\n",
    "_ = axs[0].plot(support, probs, 'x', label='probs')\n",
    "_ = axs[0].axvline(np.argmax(probs), c='black', label='mode')\n",
    "_ = axs[0].set_ylabel(\"probability\")\n",
    "_ = axs[0].set_xlabel(\"targets\")\n",
    "\n",
    "mu_h = [exact_expected_utility(h, support, probs) for h in grid]\n",
    "best = np.argmax(mu_h)\n",
    "_ = axs[1].plot(grid, mu_h, 'o', label='exact match')\n",
    "_ = axs[1].axvline(grid[best], c='blue', linestyle='dashed', label='optimum')\n",
    "_ = axs[1].set_ylabel(\"Expected utility\")\n",
    "_ = axs[1].set_xlabel(f\"candidates: best={grid[best]} expected_utility={mu_h[best]:.2f}\")\n",
    "_ = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqW7TGOC4GCe"
   },
   "source": [
    "# Losses and utilities for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3C9OVk34HVc"
   },
   "outputs": [],
   "source": [
    "def abs_loss(y, h):\n",
    "    return np.abs(y-h)  # this loss is the absolute value abs_loss\n",
    "\n",
    "def squared_loss(y, h):\n",
    "    return (y-h)**2\n",
    "\n",
    "def rbf_utility(y, h):\n",
    "    return np.exp(-(y-h)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5wwUg1W4jaq"
   },
   "outputs": [],
   "source": [
    "def mc_expected_fn_h(h, samples, fn):\n",
    "    \"\"\"Return E[fn(h)] approximated using a number of samples\"\"\"\n",
    "    return np.mean([fn(y, h) for y in samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwFCDWXBYZAY"
   },
   "source": [
    "#Poisson\n",
    "\n",
    "For the Poisson, we cannot always compute the expected value in closed form because the support of the Poisson is infinite. In reality, for certain choices of utility/loss functions, it is possible to solve the infinite sum exactly, but we will not work with that assumption. Rather, we will use MC estimation.\n",
    "\n",
    "Here, for illustration, we try two different loss functions, the absolute value $|y-h|$ and the squared difference $(y-h)^2$. These losses are common for numerical ordinal quantities, that's why we use it here. \n",
    "\n",
    "Oftentimes these two losses lead to the same optimum (but not always). Even when they do lead to the same optimum decision (a certain value of $h$), note that the expected loss differs tremendously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0x8WLzbYZgA"
   },
   "outputs": [],
   "source": [
    "rate = np.random.gamma(20, 2) # a valid Poisson parameter\n",
    "assert rate > 0, \"Should be  positive\"\n",
    "\n",
    "samples = np.random.poisson(rate, size=1000)\n",
    "grid = np.linspace(0, 100, 100)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, sharex=True, figsize=(6, 16))\n",
    "\n",
    "# it's good to see what distribution we have\n",
    "_ = axs[0].hist(samples, bins='auto')\n",
    "_ = axs[0].axvline(np.mean(samples), c='black', label='mean')\n",
    "_ = axs[0].set_xlabel(f\"samples from Poisson({rate:.2f})\")\n",
    "\n",
    "# search with abs loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=abs_loss) for h in grid]\n",
    "best = np.argmin(mu_h) # minimise expected loss\n",
    "_ = axs[1].plot(grid, mu_h, 'o')\n",
    "_ = axs[1].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[1].set_ylabel(\"MC expected abs loss for Poisson\")\n",
    "_ = axs[1].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "# search with squared loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=squared_loss) for h in grid]\n",
    "best = np.argmin(mu_h) # minimise expected loss\n",
    "_ = axs[2].plot(grid, mu_h, 'o')\n",
    "_ = axs[2].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[2].set_ylabel(\"MC expected squared loss for Poisson\")\n",
    "_ = axs[2].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "# search with squared loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=rbf_utility) for h in grid]\n",
    "best = np.argmax(mu_h) # maximise expected utility\n",
    "_ = axs[3].plot(grid, mu_h, 'o')\n",
    "_ = axs[3].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[3].set_ylabel(\"MC expected rbf utility for Poisson\")\n",
    "_ = axs[3].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "fig.tight_layout(h_pad=2, w_pad=2)\n",
    "_ = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqNedt37cVBe"
   },
   "source": [
    "# Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCwU8pePcV2Y"
   },
   "outputs": [],
   "source": [
    "shape = 20.\n",
    "rate = 0.5\n",
    "scale = 1/rate\n",
    "\n",
    "# numpy implements Gamma(shape, scale) instead of Gamma(shape, rate)\n",
    "# but this is okay, because scale is simply 1 / rate\n",
    "samples = np.random.gamma(shape, scale, size=1000)\n",
    "grid = np.linspace(0, 100, 100)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, sharex=True, figsize=(6, 16))\n",
    "\n",
    "# it's good to see what distribution we have\n",
    "_ = axs[0].hist(samples, bins='auto')\n",
    "_ = axs[0].axvline(np.mean(samples), c='black', label='mean')\n",
    "_ = axs[0].set_xlabel(f\"samples from Gamma({shape:.2f}, {scale:.2f})\")\n",
    "\n",
    "\n",
    "# search with abs loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=abs_loss) for h in grid]\n",
    "best = np.argmin(mu_h)\n",
    "_ = axs[1].plot(grid, mu_h, 'o')\n",
    "_ = axs[1].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[1].set_ylabel(\"MC expected abs loss for Gamma\")\n",
    "_ = axs[1].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "# search with squared loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=squared_loss) for h in grid]\n",
    "best = np.argmin(mu_h)\n",
    "_ = axs[2].plot(grid, mu_h, 'o')\n",
    "_ = axs[2].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[2].set_ylabel(\"MC expected squared loss for Gamma\")\n",
    "_ = axs[2].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "# search with squared loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=rbf_utility) for h in grid]\n",
    "best = np.argmax(mu_h)\n",
    "_ = axs[3].plot(grid, mu_h, 'o')\n",
    "_ = axs[3].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[3].set_ylabel(\"MC expected rbf utility for Gamma\")\n",
    "_ = axs[3].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "fig.tight_layout(h_pad=2, w_pad=2)\n",
    "_ = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY2m4yrc2mTO"
   },
   "source": [
    "# Other complex distributions\n",
    "\n",
    "\n",
    "We may encounter distributions that are not so simple as Poisson, Gamma, Exponential, Normal. For example, sometimes distributions are mixtures of those things. Here we demonstrate one such case with mixture of Gaussians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48lpY_Ul6Ltg"
   },
   "outputs": [],
   "source": [
    "def sample_mog(mixing_coefficients, locs, scales, size=100):\n",
    "    assert len(mixing_coefficients) == len(locs) == len(scales)\n",
    "    ys = [np.random.normal(loc, scale, size=size) for loc, scale in zip(locs, scales)]\n",
    "    ks = np.random.choice(len(mixing_coefficients), p=mixing_coefficients, replace=True, size=size)\n",
    "    return [ys[ks[i]][i] for i in range(size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5sCnm3P6MVb"
   },
   "outputs": [],
   "source": [
    "# demonstration\n",
    "_ = plt.hist(sample_mog([0.5, 0.2, 0.3], [-10, 0, 8], [1, 2, 1], size=10000), bins='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxUdoid56sC8"
   },
   "source": [
    "Decision rules are powerful precisely because they can deal with cases like this. As we will show, depending on the loss/utility we decide differently. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3T5n7E-cqFy"
   },
   "outputs": [],
   "source": [
    "locs = [-10, 2]\n",
    "scales = [1, 2]\n",
    "w = [0.3, 0.7]\n",
    "\n",
    "samples = sample_mog(w, locs, scales, size=1000)\n",
    "grid = np.linspace(-15, 15, 200)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, sharex=True, figsize=(6, 16))\n",
    "\n",
    "# it's good to see what distribution we have\n",
    "_ = axs[0].hist(samples, bins='auto')\n",
    "_ = axs[0].axvline(np.mean(samples), c='black', label='mean')\n",
    "_ = axs[0].set_xlabel(f\"samples from mixture of 2 Gaussians (MoG)\")\n",
    "\n",
    "\n",
    "# search with abs loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=abs_loss) for h in grid]\n",
    "best = np.argmin(mu_h)\n",
    "_ = axs[1].plot(grid, mu_h, 'o')\n",
    "_ = axs[1].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[1].set_ylabel(\"MC expected abs loss for MoG\")\n",
    "_ = axs[1].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "# search with squared loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=squared_loss) for h in grid]\n",
    "best = np.argmin(mu_h)\n",
    "_ = axs[2].plot(grid, mu_h, 'o')\n",
    "_ = axs[2].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[2].set_ylabel(\"MC expected squared loss for MoG\")\n",
    "_ = axs[2].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "# search with squared loss\n",
    "mu_h = [mc_expected_fn_h(h, samples, fn=rbf_utility) for h in grid]\n",
    "best = np.argmax(mu_h)\n",
    "_ = axs[3].plot(grid, mu_h, 'o')\n",
    "_ = axs[3].axvline(grid[best], c='blue', linestyle='dashed')\n",
    "_ = axs[3].set_ylabel(\"MC expected rbf utility for MoG\")\n",
    "_ = axs[3].set_xlabel(f\"Decision (best={grid[best]:.2f} optimum_expected_loss={mu_h[best]:.2f})\")\n",
    "\n",
    "\n",
    "fig.tight_layout(h_pad=2, w_pad=2)\n",
    "_ = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbPANdlk8Pcb"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "The absolute value loss is *median seeking*, that is, it prefers to choose the median of the distribution.\n",
    "\n",
    "The squared loss is *mean seeking*, it is biased towards the center of mass.\n",
    "\n",
    "The rbf utility is *density seeking*, it avoids regions of low density. Note that while abs and rbf end up choosing similar values their surfaces are completely differently, showing that rbs is averse to low density regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TDHspJ8ekxR"
   },
   "source": [
    "# Problem 2 of module3-exercises\n",
    "\n",
    "This is the solution to problem 2 in the list for week 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wEjwh_c_8Hf"
   },
   "source": [
    "This is the mixture we have there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkaBfi9Pfbun"
   },
   "outputs": [],
   "source": [
    "y = np.linspace(0, 20, 1000)\n",
    "\n",
    "f_shape, f_rate = 1.6, 2.\n",
    "f_weight = 0.25\n",
    "g_shape, g_rate = 5, 0.5\n",
    "\n",
    "def get_scipy_gamma(shape, rate):\n",
    "    return st.gamma(shape, 0, 1./rate)\n",
    "\n",
    "f = get_scipy_gamma(f_shape, f_rate)\n",
    "g = get_scipy_gamma(g_shape, g_rate)\n",
    "m = lambda x: f_weight * f.pdf(x) + (1-f_weight) * g.pdf(x)\n",
    "\n",
    "_ = plt.plot(y, f.pdf(y), linestyle='--', label=fr'$G(y|{f_shape}, {f_rate})$')\n",
    "_ = plt.plot(y, g.pdf(y), linestyle=':', label=fr'$G(y|{g_shape}, {g_rate})$')  # scipy uses shape, scale rather than shape, rate; the scale is 1/rate; scipy also has a location, we set it to 0\n",
    "_ = plt.plot(y, m(y), linestyle='-', label=fr'${f_weight} \\times G(y|{f_shape}, {f_rate})+{1-f_weight}\\times G(y|{g_shape}, {g_rate})$')\n",
    "_ = plt.xlabel(r\"$y$\")\n",
    "_ = plt.ylabel(\"probability density\")\n",
    "_ = plt.title(\"Mixture of two Gamma distributions\")\n",
    "_ = plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlrnxqDC_3c7"
   },
   "source": [
    "And these are the samples provided in the exercise (do not change the seed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANGsXOmA0mZs"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ex_samples = rng.gamma(4., 1./2, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8_E9MDl-kT7"
   },
   "outputs": [],
   "source": [
    "def gamma_mode(shape, rate):\n",
    "    assert shape > 1, \"The mode is undefined for shape <= 1\"\n",
    "    return (shape-1)/rate\n",
    "\n",
    "gamma_mode(f_shape, f_rate), gamma_mode(g_shape, g_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "sample_size = 20\n",
    "ys = np.where(rng.binomial(1, f_weight, size=20), f.rvs(sample_size, rng), g.rvs(sample_size, rng))\n",
    "\n",
    "# ys\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the candidates we consider in the exercise\n",
    "# (generally we would use a larger grid, but in the exercise we are making computations by hand)\n",
    "hs = np.array([gamma_mode(f_shape, f_rate), 4, gamma_mode(g_shape, g_rate), 12])\n",
    "print(\"Candidates:\", hs)\n",
    "print(\"Losses:\\n\", np.abs(np.round(ys[:,None],1) - hs[None,:]))\n",
    "print(\"Expected losses:\\n\", np.round(np.abs(np.round(ys[:,None],1) - hs[None,:]).mean(0), 1))\n",
    "i = np.abs(ys[:,None] - hs[None,:]).mean(0).argmin()\n",
    "print(f\"Optimum (using 4 candidates from exercise): {hs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a large grid\n",
    "large_grid = np.linspace(0, 20, 1000)\n",
    "# And a more robust MC estimator\n",
    "rng = np.random.RandomState(42)\n",
    "sample_size = 1000\n",
    "samples = np.where(rng.binomial(1, f_weight, size=sample_size), f.rvs(sample_size, rng), g.rvs(sample_size, rng))\n",
    "print(\"Candidates: np.linspace(0, 20, 10000)\")\n",
    "i = np.abs(samples[:,None] - large_grid[None,:]).mean(0).argmin()\n",
    "print(f\"Optimum (using a larger grid): {large_grid[i]}\") \n",
    "print(\"This is getting closer and closer to the median:\", np.median(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO5isn4WhQqMEut+tYbbcE4",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Rational decision makers",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
