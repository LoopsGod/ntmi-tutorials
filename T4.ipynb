{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide\n",
        "\n",
        "Check the guide carefully before starting."
      ],
      "metadata": {
        "id": "-28wv8VtwhYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ILOs\n",
        "\n",
        "After completing this lab you should be able to \n",
        "\n",
        "* implement the skip-gram model on PyTorch\n",
        "* implement applications of word embeddings\n",
        "* recognise biases and stereotypes that skip-gram models carry over from the data used to train them.\n"
      ],
      "metadata": {
        "id": "k2G419Prwmw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General notes\n",
        "\n",
        "* In this notebook you are expected to use $\\LaTeX$. \n",
        "* Use python3.\n",
        "* Use Torch\n",
        "* To have GPU support run this notebook on Google Colab (you will find more instructions later).\n",
        "\n",
        "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n",
        "\n",
        "\n",
        "If you need a short introduction to PyTorch [check this tutorial](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n"
      ],
      "metadata": {
        "id": "cf4ENSjqwtN5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8-BhGlnjaWP"
      },
      "source": [
        "## Table of contents\n",
        "\n",
        "* Neural Networks\n",
        "* From GLMs to NNs\n",
        "* SkipGram in PyTorch\n",
        "* Bias in embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of graded exercises\n",
        "\n",
        "Exercises have equal weights.\n",
        "\n",
        "* [Applications of word embeddings](#applications)\n",
        "* [Bias in embeddings](#bias)"
      ],
      "metadata": {
        "id": "N2tQNgfRxrNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use this notebook\n",
        "\n",
        "Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
        "\n",
        "Note that, as always, the notebook recaps theory, and contains solved quizzes. While you should probably make use of this theory recap, be careful not to spend disproportionately more time on this than you should. \n"
      ],
      "metadata": {
        "id": "HUKZiCowxxh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up\n",
        "\n",
        "Here we set up the packages that you will need to install for this tutorial."
      ],
      "metadata": {
        "id": "Dd_T9V21QLA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO-44bwnjaWR"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm\n",
        "!pip install seaborn\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install gensim\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aRjPM4kjaWS"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import math\n",
        "import numpy as np \n",
        "import time\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgba\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAJPletXjaWT"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "\n",
        "A neural network is a very flexible real-valued function, it maps some input to some output by means of a composition of differentiable parametric transformations. \n",
        "\n",
        "Being parametric means that these transformations are specified by a set of real-valued parameters, whose values we can adjust/optimise towards a certain goal (e.g., maximum likelihood given a statistical model and a dataset of observations). Being differentiable means that we can use gradient-based search for parameter estimation.\n",
        "\n",
        "Remember the GLM for text analysis? Given a document $x \\in \\mathcal X$ and a feature function $\\mathbf h: \\mathcal X \\to \\mathbb R^D$, the GLM uses linear models and nonlinear activations functions to parameterise a conditional distribution over the possible values of a response random variable $Y$ taking on values in $\\mathcal Y$. Consider, for example, a GLM for a binary response variable:\n",
        "\n",
        "\\begin{align}\n",
        "Y|X=x &\\sim \\mathrm{Bernoulli}(g(x; \\theta)) \\\\\n",
        "s &= \\mathbf w^\\top \\mathbf h(x) + b \\\\\n",
        "g(x; \\mathbf w, b) &= \\mathrm{sigmoid}(s)\\\\\n",
        "\\theta &= \\{\\mathbf w, b\\}\\\\\n",
        "&\\quad \\mathbf w \\in \\mathbb R^D, b \\in \\mathbb R\n",
        "\\end{align}\n",
        "\n",
        "The output $s$ of the linear transformation is called a *linear predictor* (it maps the feature vector $\\mathbf h(x)$ to the dimensionality of the Bernoulli parameter), the $\\mathrm{sigmoid}$ function after that is called an *activation function* (it maps the linear predictor to the correct parameter space for the Bernoulli distribution). \n",
        "\n",
        "As it turns out the GLM is a very shallow neural network (NN)! It is made of a composition of two functions (the linear transformation and the activation), which are differentiable with respect to the trainable parameters. In a GLM, the data point, represented by its feature vector $\\mathbf h(x)$, and the parameters interact linearly. In a neural network more generally, we would allow that interaction to be non-linear. \n",
        "\n",
        "We had mentioned that one of the limitations of GLMs is the need for a pre-specified feature functon. Now, with NNs, we are going to *parameterise* the feature function as well!\n",
        "\n",
        "Before we can do this, you need [an introduction to pytorch](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n",
        "\n",
        "<details>\n",
        "<summary>Why another package when we already know some JAX?</summary>\n",
        "\n",
        " JAX is a good didactic tool to give you an understanding of the role of automatic differentiation and to introduce you to gradient-based optimisation, but, in the long run, we need a software package that offers more ready-to-go code, so that you can count on certain important functionalities, pytorch is one of the best options out there, it's highly regarded amonsgt academics and in the industry, it is also the choice in the UvA's MSc AI (in case you decide to join that programme later on).\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Roadmap**\n",
        "\n",
        "* After taking the introduction to PyTorch you can work through the implemntation of the Skip-Gram model.\n",
        "* A correct implementation is already provided but we prepared a couple of ungraded exercises for you. Once you are ready to move on to the graded exercises, you can use the solution we provided to make sure that your skip-gram model is implemented correctly.\n",
        "* In the final part, you will conduct an investigation of skip-gram emebeddings using a pre-trained model.\n"
      ],
      "metadata": {
        "id": "R_NwutCASgYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From GLMs to NNs\n",
        "\n",
        "**We will now make a transition from GLMs to NNs, so you can see what really changes.**\n",
        "\n",
        "Let's take a Bernoulli GLM for a special type of binary classification as a running example. \n",
        "\n",
        "**Task.** We have to classify a pair of words $(t, c)$, each from the same vocabulary $\\mathcal W$ of $V$ words, into $y=1$ if they typically co-occur in text or $y=0$ if they typically do not co-occur in text. For us, $t$ is the \"target word\", and $c$ is the candidate context word. The response variable $y$ indicates whether $c$ is indeed a typical word within the context of $t$. The notion of context we will use is a small window around $t$ in sentences from a corpus.\n",
        "\n",
        "**Model.** We want a conditional model:\n",
        "\n",
        "\\begin{align}\n",
        "Y|T=t, C=c &\\sim \\mathrm{Bernoulli}(g(t, c; \\theta)) \n",
        "\\end{align}\n",
        "\n",
        "*In a GLM*, we might have designed a feature function $\\mathbf h: \\mathcal W \\times \\mathcal W \\to \\mathbb R^D$ and predicted the Bernoulli parameter as follows:\n",
        "\\begin{align}\n",
        "\\\\\n",
        "s &=\\mathbf w^\\top \\mathbf h(t, c) + b\\\\\n",
        "g(t, c; \\theta) &= \\mathrm{sigmoid}(s) \\\\\n",
        "\\theta &= \\{\\mathbf w, b\\}\\\\\n",
        "&\\quad \\mathbf w \\in \\mathbb R^D, b \\in \\mathbb R\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "The main reasons for having a feature function were two:\n",
        "1. to map a pair $(t, c)$ to the real coordinate space $\\mathbb R^D$, so that we can use a parametric function to predict the parameters of our statistical model;\n",
        "2. to capture in the coordinates of $\\mathbf h(t, c)$ patterns or features (or *predictors*, as they are called in statistics) that are useful for the task.\n",
        "\n",
        "As it turns out, the main reason for *manually* developing our feature functions is the second one. There are plently of ways to map document to *a real coordinate space*, but these ways are too naive an not task-driven. But now that we have a general mechanism (i.e., neural networks) to let the data and the parameters of the model interact nonlinearly, we can start with some simple strategy for step 1, and learn complex transformations of it to realise step 2 automatically.\n"
      ],
      "metadata": {
        "id": "UsCSjwOUkAqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In neural network model*, we replace our hand-crafted feature function by a trainable parametric function. \n",
        "\n",
        "For that, let's start with mapping words from the vocabulary $\\mathcal W$ to a relatively simple real coordinate space. If our vocabulary has $V = \\mathcal |W|$ words in it, a fixed-dimensional representation of any word that treats words as categorical outcomes is the so-called **one-hot encoding**. The $\\mathrm{onehot}_V: \\mathcal W \\to \\mathbb R^V$ encoding function maps elements from a finite set of size $V$ to points in $\\mathbb R^V$ and it is such that the mapping is one-to-one. For any word $w \\in \\mathcal W$ the output of the function is a vector with $V$ coordinates, most of which are set to $0$ except a single coordinate that uniquely identifies $w$:\n",
        "\\begin{align}\n",
        "\\mathbf v &= \\mathrm{onehot}_V(w) \\\\\n",
        "v_i &\\in \\{0, 1\\}\\\\\n",
        "\\sum_{i=1}^V v_i &=1\n",
        "\\end{align}\n",
        "\n",
        "For example, if we have a vocabulary $\\{\\text{cat}, \\text{dog}, \\text{rabbit}\\}$, a possible one-hot encoding function is:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathrm{onehot}_3(w) &=\n",
        "\\begin{cases}\n",
        "(1, 0, 0)^\\top &\\text{if }w = \\text{cat}\\\\\n",
        "(0, 1, 0)^\\top &\\text{if }w = \\text{dog}\\\\\n",
        "(0, 0, 1)^\\top &\\text{if }w = \\text{rabbit}\\\\\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "If we apply this to both the target and the context word, we will end up with two $V$-dimensional vectors. \n",
        "\n",
        "We can now use parametric functions to map these two $V$-dimensional elements to other spaces (for example, the space of logits, and then the space of probability values).\n",
        "\n",
        "Suppose we have a matrix $\\mathbf E \\in \\mathbb R^{V\\times D}$ of parameters, with one row per word in the vocabulary, each row containing $D$ columns. We then matrix-multiply it with the onehot encoding of the target and context words:\n",
        "\\begin{align}\n",
        "\\mathbf u &= \\mathbf E^\\top \\mathrm{onehot}_V(t) \\\\\n",
        "\\mathbf v &= \\mathbf E^\\top \\mathrm{onehot}_V(c) \n",
        "\\end{align}\n",
        "for each word, this will give us a $D$-dimensional vector that we refer to as **word embedding** (because it \"embeds\" words in a $D$-dimensional space). Look how this operation is essentially turning words into $D$-dimensional feature vectors, but we do not have to hand-design the feature values, instead those are parameters of the model and they are stored in the matrix $\\mathbf E$, which we call the **embedding matrix**. Moreover, we get to choose the dimensionality $D$.\n",
        "\n",
        "Now we could get a step closer towards predicting a Bernoulli parameter by computing a dot product $\\mathbf u^\\top \\mathbf v \\in \\mathbb R$, and we can finally get a Bernoulli parameter by constraining this dot product with the sigmoid activation function. \n",
        "\n",
        "All in all, the following is a perfectly valid model for our task:\n",
        "\\begin{align}\n",
        "Y|T=t, C=c &\\sim \\mathrm{Bernoulli}(g(t, c; \\theta)) \\\\\n",
        "\\mathbf u &= \\mathbf E^\\top \\mathrm{onehot}_V(t) \\\\\n",
        "\\mathbf v &= \\mathbf E^\\top \\mathrm{onehot}_V(c) \\\\\n",
        "s &= \\mathbf u^\\top \\mathbf v \\\\\n",
        "g(t, c; \\theta) &= \\mathrm{sigmoid}(s) \\\\\n",
        "\\theta &= \\{\\mathbf E\\}\\\\\n",
        "&\\quad \\mathbf E \\in \\mathbb R^{V\\times D}\n",
        "\\end{align}\n",
        "\n",
        "What we just described above is in fact the skip-gram model of word representation. Next, we will implement it in pytorch."
      ],
      "metadata": {
        "id": "C1C3Fg7Z5ho6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SkipGram in PyTorch\n",
        "\n",
        "Now we will apply our pytorch knowledge to develop a model of word representation, the skip-gram model.\n",
        "\n",
        "Given two words $(t, c)$ in a vocabulary $\\mathcal W$ of $V$ words, the skip-gram model predicts the probability with which the two words co-occur within a window of fixed size, thus specifying a Bernoulli distribution:\n",
        "\n",
        "\\begin{align}\n",
        "    Y \\mid T=t, C=c &\\sim \\mathrm{Bernoulli}(g(t, c; \\theta)) \\\\    \n",
        "    \\mathbf u &= \\mathrm{embed}_D(t; \\theta_{\\text{emb}}) \\\\\n",
        "    \\mathbf v &= \\mathrm{embed}_D(c; \\theta_{\\text{emb}}) \\\\\n",
        "    s &= \\mathbf u^\\top \\mathbf v \\\\\n",
        "    g(t, c; \\theta) &= \\sigma(s) \\\\\n",
        "    \\theta &= \\theta_{\\text{emb}} = \\{\\mathbf E\\} \\\\\n",
        "    &\\quad\\text{ with } \\mathbf E \\in \\mathbb R^{V \\times D}\n",
        "\\end{align}\n",
        "\n",
        "where we use the notation $\\mathrm{embed}_D(t; \\theta_{\\text{emb}})$ as a shortcut for $\\mathbf E^\\top \\mathrm{onehot}_V(t)$. Note that we use the *same embedding layer* twice, once to encode the target word, once to encode the candidate context word. From the description, we can tell it is the same embedding layer that gets used twice because both times the parameters were the same (i.e., $\\theta_{\\text{emb}}$).\n",
        "\n",
        "The skip-gram model stores in its embedding matrix $D$ feature values for each known word, but these features are parameters themselves, rather than hand-crafted. This means we will be able to learn them via maximum likelihood estimation. The skip-gram model creates an artificial task (namely, predicting whether words belong to the same context window) in order to learn a representation of words that is useful for detecting distributional similarity. For that reason, the skip-gram model can be seen as a way to operationalise the **distributional hypothesis** from linguistics. If you are not familiar with the distributional hypothesis, check [Chapter 6 of the textbook](https://web.stanford.edu/~jurafsky/slp3/6.pdf).\n"
      ],
      "metadata": {
        "id": "iw5tHpipp2aO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus\n",
        "\n",
        "Let's start by preprocessing a corpus of English text."
      ],
      "metadata": {
        "id": "N_wEF6qdFlD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "KSIVxucep8pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"treebank\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "K4pQxGpdv5oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "id": "YQ9gM_CXqJtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treebank.sents()[0]"
      ],
      "metadata": {
        "id": "mPaG7ShuqOTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def preprocess(docs, stopwords=frozenset(stopwords.words('english'))):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    new_docs = []\n",
        "    for doc in tqdm(docs):\n",
        "        new_doc = []\n",
        "        for w in doc:\n",
        "            w = w.lower()\n",
        "            if w in stopwords:\n",
        "                continue            \n",
        "            w = re.sub(r'[^\\w\\s]', '', w)\n",
        "            if not w:\n",
        "                continue\n",
        "            w = lemmatizer.lemmatize(w)            \n",
        "            new_doc.append(w)\n",
        "        new_docs.append(new_doc)\n",
        "    return new_docs"
      ],
      "metadata": {
        "id": "_KdZYb71rmLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = preprocess(treebank.sents())"
      ],
      "metadata": {
        "id": "quTOWJXArAgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0][:10]"
      ],
      "metadata": {
        "id": "WR18FPpzUjPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus), sum(len(doc) for doc in corpus)"
      ],
      "metadata": {
        "id": "Tun_3JcXP6L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary\n",
        "\n",
        "Then let's create a data structure to manage our word-to-integer correspondences, our vocabulary."
      ],
      "metadata": {
        "id": "U3m5C3WSFiNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "class Vocab:\n",
        "\n",
        "    def __init__(self, corpus: list, min_freq=1):        \n",
        "        \"\"\"\n",
        "        corpus: list of documents, each document is a list of tokens, each token is a string\n",
        "        min_freq: words that occur less than this value are discarded\n",
        "        \"\"\"\n",
        "        # Make the vocabulary of known words\n",
        "\n",
        "        # Count word occurrences\n",
        "        counter = Counter(chain(*corpus))\n",
        "        # sort them by frequency\n",
        "        sorted_by_freq_tuples = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
        "        \n",
        "        # Special tokens\n",
        "        self.pad_token = \"-PAD-\"\n",
        "        self.unk_token = \"-UNK-\"\n",
        "        self.pad_id = 0\n",
        "        self.unk_id = 1\n",
        "        self.known_words = [self.pad_token, self.unk_token]\n",
        "        self.counts = [0, 0]\n",
        "        \n",
        "        # Vocabulary\n",
        "        self.word2id = OrderedDict()                \n",
        "        self.word2id[self.pad_token] = self.pad_id\n",
        "        self.word2id[self.unk_token] = self.unk_id\n",
        "        self.min_freq = min_freq\n",
        "        for w, n in sorted_by_freq_tuples: \n",
        "            if n >= min_freq: # discard infrequent words\n",
        "                self.word2id[w] = len(self.known_words)\n",
        "                self.known_words.append(w)\n",
        "                self.counts.append(n)\n",
        "        \n",
        "        # store the counts for later\n",
        "        self.counts = np.array(self.counts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.known_words)\n",
        "\n",
        "    def __getitem__(self, word): # return the id (int) of a word (str)\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def encode(self, doc: list, pad_left=0, pad_right=0):\n",
        "        \"\"\"\n",
        "        Transform a document into a numpy array of integer token identifiers.\n",
        "        doc: list of tokens, each token is a string\n",
        "        pad_left: number of prefix padding tokens \n",
        "        pad_right: number of suffix padding tokens\n",
        "        \n",
        "        Return: numpy array with shape [pad_left + len(doc) + pad_right]\n",
        "        \"\"\"\n",
        "        return np.array([self.word2id.get(w, self.unk_id) for w in chain([self.pad_token] * pad_left, doc, [self.pad_token] * pad_right)])\n",
        "\n",
        "    def batch_encode(self, docs: list, pad_left=0, pad_right=0):\n",
        "        \"\"\"\n",
        "        Transform a batch of documents into a numpy array of integer token identifiers.\n",
        "        This will pad the shorter documents to the length of the longest document.\n",
        "        docs: a list of documents\n",
        "        pad_left: number of prefix padding tokens\n",
        "        pad_right: number of suffix padding tokens\n",
        "\n",
        "        Return: numpy array with shape [len(docs), longest_doc + pad_left + pad_right]\n",
        "        \"\"\"\n",
        "        max_len = max(len(doc) for doc in docs)\n",
        "        return np.stack([self.encode(doc, pad_left=pad_left, pad_right=pad_right + max_len-len(doc)) for doc in docs])\n",
        "\n",
        "    def decode(self, ids, strip_pad=False):\n",
        "        \"\"\"\n",
        "        Transform a np.array document into a list of tokens.\n",
        "        ids: np.array with shape [num_tokens] \n",
        "        strip_pad: whether PAD tokens should be deleted from the output\n",
        "\n",
        "        Return: list of strings with size [num_tokens - num_padding]\n",
        "        \"\"\"\n",
        "        if strip_pad:\n",
        "            return [self.known_words[id] for id in ids if id != self.pad_id]\n",
        "        else:\n",
        "            return [self.known_words[id] for id in ids]\n",
        "\n",
        "    def batch_decode(self, docs, strip_pad=False):\n",
        "        \"\"\"\n",
        "        Transform a np.array collection of documents into a collection of lists of tokens.\n",
        "        ids: np.array with shape [num_docs, max_length] \n",
        "        strip_pad: whether PAD tokens should be deleted from the output\n",
        "\n",
        "        Return: list of documents, each a list of tokens, each token a string\n",
        "        \"\"\"\n",
        "        return [self.decode(doc, strip_pad=strip_pad) for doc in docs]\n",
        "\n",
        "    def sample(self, size, p=None, alpha=1., rng=None):\n",
        "        \"\"\"\n",
        "        Sample words from the vocabulary. Word w is sampled with probability proportional to power(count(w), alpha).\n",
        "\n",
        "        size: shape of sample (it can be a number e.g., size=10, or a shape size=(5, 10))\n",
        "        Return: numpy array of samples.\n",
        "        \"\"\"\n",
        "        if rng is None:\n",
        "            rng = np.random\n",
        "        if p is None:\n",
        "            p = np.power(self.counts, alpha)\n",
        "            p = p / p.sum()\n",
        "        return rng.choice(len(self), p=p, size=size)    "
      ],
      "metadata": {
        "id": "VNYJ_bM01icD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(corpus, min_freq=1)\n",
        "len(vocab), vocab.known_words[:10]"
      ],
      "metadata": {
        "id": "XkBXy4fv2NlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.decode(vocab.encode(\"this is really AWESOME\".split(), pad_left=2, pad_right=3), strip_pad=True)"
      ],
      "metadata": {
        "id": "T__-Z0Dv29ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.batch_decode(vocab.batch_encode([\"this is a real day\".split(), \"a nice day\".split()], pad_left=2, pad_right=3), strip_pad=True)"
      ],
      "metadata": {
        "id": "y_FfGpPV_k_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.batch_decode(vocab.sample((3, 5)))"
      ],
      "metadata": {
        "id": "bgQwA-gI_C_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Now we can create a pytroch dataset, a collection of data points the model learns from. \n",
        "\n",
        "A data point in the skip-gram model is a triple $(t, c, y)$ where $t$ is a target word, $c$ is a context word and $y$ indicates whether $c$ occurred in a window centered around $t$. We will create this data set by mining positive examples of co-occurrence from the corpus and artificially creating negative examples with so-called \"negative sampling\" (or sampling context words from the vocabulary, rather than from the context window)."
      ],
      "metadata": {
        "id": "iCv1vm65Fy3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_triples(corpus, vocab, radius=2, num_neg=2, alpha=0.75, rng=None):\n",
        "    \"\"\"\n",
        "    corpus: a list of documents, each document is a list of tokens, each token is a string\n",
        "    vocab: a Vocabulary object initialised with the corpus\n",
        "    radius: how many words are in the left or right context of the target word\n",
        "    num_neg: number of words to be sampled as negative examples for each word found within the context window\n",
        "    alpha: closer to 0 will make negative samples be more uniform, closer to 1 will make the frequency reproduce\n",
        "     the frequency in the corpus, larger than 1 will concentrate even more on the most frequent words\n",
        "    rng: random number generator (np.random.RandomState)\n",
        "\n",
        "    Return a generator for triples (t,c,y) where t is a target word, c is either a word within context (then y is 1), or a word sampled\n",
        "     at random from the vocabulary (then y is 0)\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random\n",
        "    N = len(corpus)    \n",
        "    p = np.power(vocab.counts, alpha)\n",
        "    p /= p.sum()\n",
        "    \n",
        "    for d in tqdm(rng.permutation(np.arange(N))):\n",
        "        doc = corpus[d]\n",
        "        positions = rng.permutation(np.arange(len(doc)))\n",
        "        neg_samples = rng.choice(len(vocab.known_words), p=p, size=len(doc) * 2 * radius * num_neg)\n",
        "        n = 0\n",
        "        for i in positions:\n",
        "            if doc[i] not in vocab.word2id:\n",
        "                continue\n",
        "            num_pos = 0\n",
        "            # positive samples from the left\n",
        "            for j in range(max(0, i - radius), i):\n",
        "                yield doc[i], doc[j], 1\n",
        "                num_pos += 1\n",
        "            # positive samples from the right\n",
        "            for j in range(i + 1, min(len(doc), i + 1 + radius)):\n",
        "                yield doc[i], doc[j], 1\n",
        "                num_pos += 1\n",
        "            # negative samples\n",
        "            for j in range(num_pos * num_neg):\n",
        "                yield doc[i], vocab.known_words[neg_samples[n + j]], 0\n",
        "            n += num_pos * num_neg"
      ],
      "metadata": {
        "id": "3rzowTyv3517"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualise this"
      ],
      "metadata": {
        "id": "BCSxHO8JFIYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n, (t, c, y) in zip(range(15), make_triples(corpus, vocab, radius=4, num_neg=1, rng=np.random.RandomState(42))):\n",
        "    print(f\"n={n} t={t} c={c} y={y}\")"
      ],
      "metadata": {
        "id": "p17b6kw_FI_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a Dataset:"
      ],
      "metadata": {
        "id": "DXZqo9VF7yOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "\n",
        "class SkipGramDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, vocab, triple_generator):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            size - Number of data points we want to generate\n",
        "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.triples = list(triple_generator)        \n",
        "        \n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return len(self.triples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        t, c, y = self.triples[idx]\n",
        "        # our dataset class already converts strings to integers using the vocabulary\n",
        "        return self.vocab[t], self.vocab[c], y"
      ],
      "metadata": {
        "id": "3Pf3A_nGDdeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_data = SkipGramDataset(vocab, make_triples(corpus, vocab, radius=4, num_neg=1, rng=np.random.RandomState(42)))"
      ],
      "metadata": {
        "id": "0_ubvJlrEESo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how many triples we mined from the training corpus\""
      ],
      "metadata": {
        "id": "8IiPnHSVUkVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(skipgram_data)"
      ],
      "metadata": {
        "id": "uqaqPAyHEJic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n, (t, c, y) in zip(range(15), skipgram_data):\n",
        "    print(f\"n={n} t={t} c={c} y={y}\")"
      ],
      "metadata": {
        "id": "zKEXes0X5Ky9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader\n",
        "\n",
        "Finally, we use a pytorch data loader to obtain batches from."
      ],
      "metadata": {
        "id": "Tm3bF8ltEWCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "skipgram_data_loader = DataLoader(skipgram_data, batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "T6sDTXRNEUjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at 2 batches of 3 data points each, both with and without shuffling the dataset:"
      ],
      "metadata": {
        "id": "afW0aozNGqh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n, batch in zip(range(2), DataLoader(skipgram_data, batch_size=3, shuffle=False)):\n",
        "    print(f\"n={n} batch={batch}\")\n",
        "\n",
        "for n, batch in zip(range(2), DataLoader(skipgram_data, batch_size=3, shuffle=True)):\n",
        "    print(f\"n={n} batch={batch}\")    "
      ],
      "metadata": {
        "id": "_s9S4DhD-5rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module\n",
        "\n",
        "We will now develop a pytorch module for the skip-gram model.\n",
        "\n",
        "To better match the theory, the forward pass of our module will return a batch of Bernoulli distributions, one per target-context pair in the input.\n",
        "\n",
        "**Ungraded exercise.** Study the SkipGram module and complete its design, it is only missing the mapping from $(t, c)$ to a logit, and then the mapping from the Bernouli pmf and $y$ to a loss. The solution is presented a few cells down below."
      ],
      "metadata": {
        "id": "yc4UUBHUG3Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as td\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab: Vocab, embed_dim: int, pad_id=0):\n",
        "        \"\"\"\n",
        "        vocab: the Vocab object for our corpus and model\n",
        "        embed_dim: the dimensionality we want for our embedding space\n",
        "        pad_id: the id of the PAD token (0)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # we use the torch auxiliary module nn.Embedding\n",
        "        # which is a very efficient implementation of an embedding layer\n",
        "        self.vocab = vocab\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=len(vocab), \n",
        "            embedding_dim=embed_dim, \n",
        "            padding_idx=pad_id\n",
        "        )        # this is the embed_D layer of the theory\n",
        "\n",
        "    def predict_logit(self, target, context):\n",
        "        \"\"\"\n",
        "        target: a batch of word ids (one per target word in the batch)\n",
        "        context: a batch of word ids (one per candidate context word in the batch)\n",
        "\n",
        "        Return a batch of logits (one per target-context pair in the batch)\n",
        "        \"\"\"\n",
        "        # This is a quiz with solution, you will find the solution in the next cell, \n",
        "        # but you can try to implement it yourself first\n",
        "        raise NotImplementedError(\"Implement me!\")\n",
        "\n",
        "    def forward(self, target, context):\n",
        "        \"\"\"\n",
        "        target: a batch of word ids (one per target word in the batch)\n",
        "        context: a batch of word ids (one per candidate context word in the batch)\n",
        "\n",
        "        Return a batch of Bernoulli distributions (one per target-context pair in the batch)\n",
        "        \"\"\"\n",
        "        # [batch_size]\n",
        "        logits = self.predict_logit(target, context)\n",
        "        return td.Bernoulli(logits=logits)\n",
        "\n",
        "    def loss(self, t, c, y):        \n",
        "        \"\"\"\n",
        "        t: a batch of word ids (one per target word in the batch)\n",
        "        c: a batch of word ids (one per candidate context word in the batch)\n",
        "        y: a batch of binary labels (one per target-context pair in the batch)\n",
        "\n",
        "        Return a single scalar value that is the negative of the log-likelihood of the \n",
        "        current model given the observations in the batch.\n",
        "        \"\"\"\n",
        "        # one Bernoulli distribution per (t, c) pair in the batch\n",
        "        py_xc = self(target=t, context=c)\n",
        "\n",
        "        # Now, knowing that the observed binary outcome is y, you should compute the loss.\n",
        "        #  This is a quiz with solution, you will find the solution in the next cell, \n",
        "        #  but you can try to implement it yourself first\n",
        "        raise NotImplementedError(\"Implement me!\")\n",
        "\n",
        "    def np_embedding_matrix(self):\n",
        "        \"\"\"\n",
        "        Converts the embedding matrix to a numpy array\n",
        "        \"\"\"\n",
        "        return self.embed.weight.detach().cpu().numpy()        "
      ],
      "metadata": {
        "id": "RQU9GN4W0dgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how you construct the skipgram model:"
      ],
      "metadata": {
        "id": "Tdeivo5qp6el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "skipgram = SkipGram(vocab, 32).to(device) # by default it's on CPU, you can change it to cuda:0 if you like\n",
        "skipgram"
      ],
      "metadata": {
        "id": "zwnP-KX5FDXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here you should be able to visualise the Bernoulli distributions predicted for a few data points. \n",
        "\n",
        "Note that we have not yet trained the model."
      ],
      "metadata": {
        "id": "uMMuISSZp8u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n, (batch_t, batch_c, batch_y) in zip(range(2), DataLoader(skipgram_data, batch_size=3, shuffle=False)):\n",
        "    pY_given_t_and_c = skipgram(batch_t.to(device), batch_c.to(device))\n",
        "    bern_params = pY_given_t_and_c.probs\n",
        "    prob_observations = torch.exp(pY_given_t_and_c.log_prob(batch_y.to(device).float()))\n",
        "    for i, (t, c, y, bern_param, prob_y) in enumerate(zip(vocab.decode(batch_t), vocab.decode(batch_c), batch_y, bern_params, prob_observations)):\n",
        "        print(f\"batch={n} instance={i} g(t={t}, c={c})={bern_param:.2} hence Bernoulli({y}|{bern_param:.2})={prob_y:.2}\")\n",
        "    print(\"Loss for this batch:\", skipgram.loss(batch_t.to(device), batch_c.to(device), batch_y.to(device)))\n",
        "    print()"
      ],
      "metadata": {
        "id": "PLXdWJnDHafF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "\n",
        "<summary> <b> Solution: python code for the SkipGram model.</b> </summary>\n",
        "\n",
        "```python\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as td\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab, embed_dim, pad_id=0):\n",
        "        \"\"\"\n",
        "        vocab: the Vocab object for our corpus and model\n",
        "        embed_dim: the dimensionality we want for our embedding space\n",
        "        pad_id: the id of the PAD token (0)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # we use the torch auxiliary module nn.Embedding\n",
        "        # which is a very efficient implementation of an embedding layer\n",
        "        self.vocab = vocab\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=len(vocab), \n",
        "            embedding_dim=embed_dim, \n",
        "            padding_idx=pad_id\n",
        "        )        # this is the embed_D layer of the theory\n",
        "\n",
        "    def predict_logit(self, target, context):\n",
        "        \"\"\"\n",
        "        target: a batch of word ids (one per target word in the batch)\n",
        "        context: a batch of word ids (one per candidate context word in the batch)\n",
        "\n",
        "        Return a batch of logits (one per target-context pair in the batch)\n",
        "        \"\"\"\n",
        "        # [batch_size, embed_dim]\n",
        "        w = self.embed(target)\n",
        "        # [batch_size, embed_dim]\n",
        "        c = self.embed(context)        \n",
        "        # Dot product\n",
        "        # [batch_size]\n",
        "        logits = torch.sum(w * c, dim=-1)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, target, context):\n",
        "        \"\"\"\n",
        "        target: a batch of word ids (one per target word in the batch)\n",
        "        context: a batch of word ids (one per candidate context word in the batch)\n",
        "\n",
        "        Return a batch of Bernoulli distributions (one per target-context pair in the batch)\n",
        "        \"\"\"\n",
        "        # [batch_size]\n",
        "        logits = self.predict_logit(target, context)\n",
        "        return td.Bernoulli(logits=logits)\n",
        "\n",
        "    def loss(self, t, c, y):        \n",
        "        \"\"\"\n",
        "        t: a batch of word ids (one per target word in the batch)\n",
        "        c: a batch of word ids (one per candidate context word in the batch)\n",
        "        y: a batch of binary labels (one per target-context pair in the batch)\n",
        "\n",
        "        Return a single scalar value that is the negative of the log-likelihood of the \n",
        "        current model given the observations in the batch.\n",
        "        \"\"\"\n",
        "        # one Bernoulli distribution per (t, c) pair in the batch\n",
        "        py_xc = self(target=t, context=c)\n",
        "        return - py_xc.log_prob(y.float()).mean()\n",
        "\n",
        "    def np_embedding_matrix(self):\n",
        "        \"\"\"\n",
        "        Converts the embedding matrix to a numpy array\n",
        "        \"\"\"\n",
        "        return self.embed.weight.detach().cpu().numpy()        \n",
        "```        \n",
        "\n",
        "---\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "6Hmzn8T3pF4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimisation\n",
        "\n",
        "For optimisation, we will use something a bit more sophisticated than the standard SGD algorithm, we will use the Adam optimiser. Adam is a version of SGD with improved convergence properties due to a sophisticated treatment to learning rates.\n",
        "\n",
        "We still need to pick an initial learning rate (`lr`). Whereas in JAX we implemented L2 regularisation by hand, in pytorch the optimiser class can take care of that for us (which is very convenient!), all we need to do is to set some positive weight to the argument `weight_decay` which is the importance of the L2 regulariser (0 means no regularisation)."
      ],
      "metadata": {
        "id": "mpWfr-ixHVE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "sg_optimizer = Adam(skipgram.parameters(), lr=5e-3, weight_decay=1e-6)"
      ],
      "metadata": {
        "id": "-PTiwrZhHYnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On CPU this will take 5 to 10 minutes:"
      ],
      "metadata": {
        "id": "6ZKFj6XHQk2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# reset your choice of device\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# reset model parameters\n",
        "skipgram = SkipGram(vocab, 32).to(device) # you may change it to 'cpu'\n",
        "# reset optimiser\n",
        "sg_optimizer = Adam(skipgram.parameters(), lr=5e-3, weight_decay=1e-6)\n",
        "# reset data loader \n",
        "skipgram_data_loader = DataLoader(\n",
        "    skipgram_data, \n",
        "    batch_size=100, # adjust the batch size, on GPU you can have bigger batches (eg 1000)\n",
        "    shuffle=True\n",
        ")\n",
        "# adjust the number of epochs, the more, the better\n",
        "num_epochs = 5 # with larger batches you will probably need more epochs\n",
        "total_steps = num_epochs * len(skipgram_data_loader)\n",
        "step = 0\n",
        "log = defaultdict(list)\n",
        "\n",
        "with tqdm(range(total_steps), desc='MLE') as bar:  \n",
        "    # we pass over the entire data a number of times\n",
        "    for epoch in range(num_epochs):            \n",
        "        skipgram.train()\n",
        "        for t, c, y in skipgram_data_loader:\n",
        "            t = t.to(device)\n",
        "            c = c.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            sg_optimizer.zero_grad()\n",
        "                                    \n",
        "            # Negative log likelihood of model given batch of observations            \n",
        "            loss = skipgram.loss(t=t, c=c, y=y)\n",
        "            loss.backward()\n",
        "            sg_optimizer.step()\n",
        "\n",
        "            log['loss'].append(loss.item())       \n",
        "            bar.set_postfix({'epoch': epoch + 1, 'step': f\"{step:4d}\", 'loss': f\"{loss.item():.4f}\"}) \n",
        "            bar.update()\n",
        "            step += 1"
      ],
      "metadata": {
        "id": "6GGN7XJF69Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load word2vec embeddings, find top-10 professions for woman and man."
      ],
      "metadata": {
        "id": "ZxVmt1bsqmn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.plot(np.arange(len(log['loss'])), log['loss'], '.')"
      ],
      "metadata": {
        "id": "pA-WMYt5Buw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions** Below we provide you with a few helper functions that you can use to inspect the embedding of a word, to compare two words in terms of the cosine similarity of their embeddings, and to find the top-k elements of a numpy array. Study them and play with them for a bit, next you will work on an exercise for which they will be useful.\n",
        "\n"
      ],
      "metadata": {
        "id": "DYG18WZrQuMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's get the embedding matrix from the trained model in numpy format:"
      ],
      "metadata": {
        "id": "BAxyaHS1ZG6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E = skipgram.np_embedding_matrix()\n",
        "E.shape"
      ],
      "metadata": {
        "id": "uNPmtj3BQHSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the embedding for a word by indexing the matrix using the word id in the vocabulary:"
      ],
      "metadata": {
        "id": "lCj5iQ1tZKiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E[vocab['person']]"
      ],
      "metadata": {
        "id": "dM08J11JQPpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a helper code to make it easier:"
      ],
      "metadata": {
        "id": "abQMS0kYZPCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(word, vocab, E):\n",
        "    \"\"\"\n",
        "    word: a word (str)\n",
        "    vocab: the Vocabulary object for our corpus and model\n",
        "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
        "\n",
        "    Return the D-dimensional embedding of the word as np.array object.\n",
        "    \"\"\"\n",
        "    if word not in vocab.word2id:\n",
        "        raise ValueError(f\"{word} is OOV\")\n",
        "    wid = vocab.word2id[word]\n",
        "    return E[wid]\n",
        "\n",
        "assert np.alltrue(embed('person', vocab, E) == E[vocab['person']])"
      ],
      "metadata": {
        "id": "_GjweiTCXAI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can compare words using cosine similarity of their embeddings. Here is how we do that:"
      ],
      "metadata": {
        "id": "tMvbD72OZXYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(word1, word2, vocab, E):  \n",
        "    \"\"\"\n",
        "    word1: a word (str)\n",
        "    word2: another word (str)\n",
        "    vocab:  the Vocabulary object for our corpus and model\n",
        "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
        "\n",
        "    Return the cosine similarity (a real number) of the two words in the embedding space of our model.\n",
        "    \"\"\"  \n",
        "    # [D]    \n",
        "    u = embed(word1, vocab, E)\n",
        "    # [D]\n",
        "    v = embed(word2, vocab, E)\n",
        "    return np.sum(u * v) / (np.sqrt(np.sum(u * u)) * np.sqrt(np.sum(v * v)))"
      ],
      "metadata": {
        "id": "7TgiW-3kUYVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarity('car', 'truck', vocab, E), cos_similarity('car', 'automobile', vocab, E)"
      ],
      "metadata": {
        "id": "Iu8AclU6W3Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a functio to help you whenever you need to find the top-k values in a numpy array. It will be useful in an exercise later on."
      ],
      "metadata": {
        "id": "FyT1PSBBZ4_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def np_topk(array, k=10):\n",
        "    \"\"\"\n",
        "    array: a list or a numpy np.array\n",
        "    k: number of top elements to be returned\n",
        "\n",
        "    Return the top-k elements and their indices in the array.\n",
        "    \"\"\"\n",
        "    array = np.array(array)\n",
        "    ids = np.argsort(-array)  # argsort finds the lowest values, so we use -array to find the highest values\n",
        "    # return top-k values, return the indices of the top-k values\n",
        "    return array[ids][:k], ids[:k]\n",
        "\n",
        "assert np.alltrue(np_topk([10, 20, 30, 40], 2)[0]  == np.array([40, 30]))\n",
        "assert np.alltrue(np_topk([10, 20, 30, 40], 2)[1]  == np.array([3, 2]))"
      ],
      "metadata": {
        "id": "MJFsezj5YlrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"applications\"> **Graded Exercise - Applications of word embeddings**\n",
        "\n",
        "In this exercise you will develop a few nice applications of word embeddings. \n",
        "\n",
        "1. `topk_words`: given a *word* you will find the words that are closest to it in embedding space using cosine similarity.\n",
        "\n",
        "There is a list of words that you should test your function with, you will see it below. For each word in the list, display the 10 words that are nearest in embedding space. Make sure to display the information in a way that's convenient for grading (e.g., using a table from `tabulate` or something similar).\n",
        "\n",
        "2. `doesnt_match`: given a *list of words* you will find the odd word in the list, this \"odd\" word is the one that is on average the least cosine-similar to the other words in the list.\n",
        "\n",
        "Again, there is a list of test cases for you. Make sure to display the information in a convenient format for grading.\n",
        "\n",
        "3. `word_analogy`: given two lists of words make a representation $\\mathbf v$ where the words in the *positive* list contribute positively to $\\mathbf v$, the words in the *negative* list contribute negative to $\\mathbf v$, and then return the 10 words that are cosine-closest to $\\mathbf v$ in embedding space. \n",
        "\n",
        "Again, there is a list of test cases for you. Make sure to print the information in a readable way for grading.\n",
        "\n",
        "\n",
        "**Guidelines** The grade will depend mostly on the correctness of your implementation but also on whether you displayed the requested information in a human-readable way (so the grader does not have to necessarily interact with your code). The grade *will not* depend on whether your model captures meaningful similarities in its embedding space. Unfortunately, the dataset you are using is too small for you to train a good model, and training a very good model would also require training it for much longer.\n"
      ],
      "metadata": {
        "id": "2HQuHjIpaYHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topk_words(word, vocab, E, k=10):    \n",
        "    \"\"\"\n",
        "    word: a word (str)\n",
        "    vocab:  the Vocabulary object for our corpus and model\n",
        "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
        "    k: how many nearest neighbours we want to find\n",
        "\n",
        "    Return a python list with the k words (each a string) that are nearest to the input word\n",
        "     in embedding space according to cosine similarity. You can use any of the functions provided earlier, and you\n",
        "     can also create additional ones if you need them.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ],
      "metadata": {
        "id": "tBvmp7cXbKqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in ['car', 'person', 'woman', 'man']:\n",
        "    print(\"Make sure to test topk_words using:\", word)"
      ],
      "metadata": {
        "id": "3D7gHAywdPaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doesnt_match(words, vocab, E):\n",
        "    \"\"\"\n",
        "    words: a list of words (each a str)\n",
        "    vocab:  the Vocabulary object for our corpus and model\n",
        "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
        "    \n",
        "    Return the word in the list that is least cosine-similar to every other word in the list on average.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ],
      "metadata": {
        "id": "Wb3kqL5ecr2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word_list in [['car', 'automobile', 'wall'], ['car', 'bridge', 'wall']]:\n",
        "    print(\"Make sure to test doesnt_match using:\", word_list)"
      ],
      "metadata": {
        "id": "JNBsP04aWe8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_analogy(positive: list, negative: list, vocab: Vocab, E, k=10):\n",
        "    \"\"\"\n",
        "    positive: a list of words (each a str) that contribute positively to the similarity\n",
        "    negative: a list of words (each a str) that contribute negatively to the similarity\n",
        "    vocab:  the Vocabulary object for our corpus and model\n",
        "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
        "    k: number of nearest neighbours\n",
        "    \n",
        "    Return the top-k words in terms of cosine similarity with the embedding you obtain by\n",
        "     summing the embedding of the words in the positive list \n",
        "     and subtracting the embedding of the words in the negative list. \n",
        "    That is, you will retrieving the neighbours of the vector:\n",
        "        \\sum_{w in positive} emb(w) - \\sum_{w in negative} emb(w)\n",
        "\n",
        "    The return is a list of neighouring words (each a str).\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ],
      "metadata": {
        "id": "FQck6nJTexkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pos_list, neg_list in [(['woman', 'president'], ['man']), (['street', 'person'], ['bridge'])]:\n",
        "    print(f\"Make sure to make analogies for: postive={pos_list} negative={neg_list}\")"
      ],
      "metadata": {
        "id": "j2IrdKiWe3-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias in embeddings \n",
        "\n",
        "\n",
        "In this section you will experiment with a strong pretrained embedding model that is very similar to skipgram, it's called GloVe. We are not using skipgram because the available models are much too large for this notebook, but GloVe is a **very strong** competitor. \n",
        "\n",
        "Instead of training GloVe, which would be too demanding, we will download a trained one, and interact with it using `gensim`, a very robust python package for word embeddings.  We will experiment with the same applications that you coded above, but this time you will use gensim code, this way if you made mistakes earlier, they won't affect the quality of this experiment.\n",
        "\n",
        "The goal of this exercise is that you visualise biases that embedding models carry over from their training data. A statistical objective (like MLE) is *all about statistics* and not at all about *core human values*. When we download text from the web, it may contain all sorts of stereotypes that are inadequate in many situations. For example, if we download text with gender bias and train our models, those harmful are statistics present in the data will most likely be also present in our models. There's no statistical incentive in their training objective to get rid of correlations that we think are inadequate or outdated or simply harmful. For now, we will not work on fixing the models, we will just investigate them and see that the problem is real. If you are curious to see ways to address the problem, you can check this [excellent paper](https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf), though note that checking it is optional at this point."
      ],
      "metadata": {
        "id": "5rKnCEQ0wljN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, make sure to install [gensim](https://radimrehurek.com/gensim/) by running the cell below:"
      ],
      "metadata": {
        "id": "jTw9SL2RiZEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Pp6SxEBZoV7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use gensim's downloader to obtain a trained model:"
      ],
      "metadata": {
        "id": "VC0Vtu8DikVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "EaO0zq_Prwxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shouldn't take too long:"
      ],
      "metadata": {
        "id": "zRQK3RrOiqKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "4VUahbuYrxMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the model in many ways, we can embed a word:"
      ],
      "metadata": {
        "id": "BT6bC7UiitJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.get_vector('person')"
      ],
      "metadata": {
        "id": "c7QRQyFqi0fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can retrieve similar words:"
      ],
      "metadata": {
        "id": "1Dv5EhRli8yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.similar_by_word('person', 10)"
      ],
      "metadata": {
        "id": "EsZw1YK6jL0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find words that do not match"
      ],
      "metadata": {
        "id": "vJ8jEcIxjVOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.doesnt_match(['car', 'wall', 'automobile'])"
      ],
      "metadata": {
        "id": "Bu3fWwKqjUJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make word analogies"
      ],
      "metadata": {
        "id": "cofYWEHajem3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
      ],
      "metadata": {
        "id": "H1BVIoWpjgUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And more (you can see some examples [here](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format) if you like)."
      ],
      "metadata": {
        "id": "1ufe9eDkji1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a list of occupations that we got from one of the [research papers that initiated this whole investigation](https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf). These occupations are *not* gender-marked words in English, they are gender-neutral. Yet, the statistics of English data used to train GloVe are biased towards making harmful stereotypical associations with words like `woman` and `man`."
      ],
      "metadata": {
        "id": "pj9Rx3LUjs4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "occupations = [\n",
        "    \"homemaker\",\n",
        "    \"nurse\",\n",
        "    \"receptionist\",\n",
        "    \"librarian\",\n",
        "    \"socialite\",\n",
        "    \"hairdresser\",\n",
        "    \"nanny\",\n",
        "    \"bookkeeper\",\n",
        "    \"stylist\",\n",
        "    \"housekeeper\",\n",
        "    \"maestro\",\n",
        "    \"skipper\",\n",
        "    \"protege\",\n",
        "    \"philosopher\",\n",
        "    \"capitain\",\n",
        "    \"architect\",\n",
        "    \"financier\",\n",
        "    \"warrior\",\n",
        "    \"broadcaster\",\n",
        "    \"magician\"\n",
        "]"
      ],
      "metadata": {
        "id": "tPfaoQeXjsCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"bias\"> **Graded Exercise - Bias in embeddings**\n",
        "\n",
        "Using gensim functionality (code and model):\n",
        "\n",
        "1. plot the similarity of each occupation word in the list to both `woman` and `man`\n",
        "2. also, plot the difference in similarity towards `woman` with similarity towards `man` and order the occupation words by this difference. \n",
        "\n",
        "Make remarks about what you see in (1) and (2).\n",
        "\n",
        "3. Use algorithms such as `most_similar` (for word analogies), `doesnt_match` and `similar_by_word` to discover additional harmful associations in embedding space. If you want, you can investigate a different type of bias. Be **very** careful here and **very conscious** as you will likely encounter terrible associations. The goal here is not to ridicule the victims of these associations, the goal here is that you grow worried about careless use of NLP, and that you join responsible researchers in a) making careful use of NLP, and b) developing NLP that pushes back from and overcome sources of harm.  \n",
        "\n",
        "**Guideline** We will grade parts 1 and 2 in terms of the quality of your plots and the remarks you make. We will not grade part 3 as a function of how many biases you uncovered, nor as a function of whether we agree with them or not. Instead, we will use the information you display and the remarks you make as a way to assess the effort you put into it.\n"
      ],
      "metadata": {
        "id": "A37-PPqZg_bZ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "2022/T4_student",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}