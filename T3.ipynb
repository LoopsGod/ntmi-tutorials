{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLeLl2YVu4vv"
      },
      "source": [
        "# Guide\n",
        "\n",
        "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
        "* Note that, as always, the notebook recaps theory, and contains solved exercises. While you should probably make use of this theory recap, be careful not to spend disproportionately more time on this than you should. The theory here is more condensed, and should be easier to understand after week 3's reading and after the highlights discussed in class (HC3a).\n",
        "* We recommend you read the theory part before the LC3 session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APh_4TfPRQ13"
      },
      "source": [
        "## ILOs\n",
        "\n",
        "After completing this lab you should be able to \n",
        "\n",
        "* develop generalised linear models of text classification and text regression in Python using sklearn and jax\n",
        "* estimate parameters via gradient-based optimisation\n",
        "\n",
        "**General notes**\n",
        "\n",
        "* In this notebook you are expected to use $\\LaTeX$. \n",
        "* Use python3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SasxJwnHRrUq"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "* Setting up\n",
        "* Theory\n",
        "    * Feature functions\n",
        "    * Generalised linear models\n",
        "        * Bernoulli GLM\n",
        "        * Categorical GLM\n",
        "        * Poisson GLM\n",
        "    * General principles for prescribing GLMs\n",
        "    * Parameter estimation\n",
        "        * Stochastic gradient-based optimisation\n",
        "        * Regularisation\n",
        "* Binary classifier\n",
        "    * Naive Bayes\n",
        "    * Binary GLMs on Jax\n",
        "* Python/Jax GLM class\n",
        "    * Binary classifier experiment\n",
        "    * Poisson regressor experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zZjqmE3Rmld"
      },
      "source": [
        "## Table of graded exercises\n",
        "\n",
        "Exercises have equal weight.\n",
        "\n",
        "* [Subjectivity classifier](#ex-bernoulli)\n",
        "* [Number of Tokens Data and Baseline](#ex-numtokens)\n",
        "* [Poisson regression](#ex-poisson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqxvaCboSFNN"
      },
      "source": [
        "# Setting up\n",
        "\n",
        "Make sure you have colab configure to use 4 spaces for TAB (Tools/Settings/Editor/Indentation). If you change your Runtime to GPU (Runtime/Change runtime type) model training will be faster, but for this tutorial a GPU is not strictly necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SuKXLEzu9xJ"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install jax\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install seaborn\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGS20mlRvCvW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import jax\n",
        "from jax import device_put\n",
        "\n",
        "import json\n",
        "import gzip\n",
        "import urllib.request\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, mean_absolute_error\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('toktok')\n",
        "nltk.download('sentence_polarity')\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYH2-7LlSfoQ"
      },
      "source": [
        "# Theory\n",
        "\n",
        "We often need to analyse documents in terms of diverse properties. Where these properties are discrete and finite (e.g., negative/neutral/positive, or spam/no-spam, or politics/sport/science) we talk about text analysis as **text classification** (or text categorisation), where these properties are numerical we talk about text analysis as **text regression**. \n",
        "\n",
        "For the rest of this notebook, we use $X$ to denote a random document in the space $\\mathcal X$ of all possible documents. A document for us is a sequence $x=\\langle w_1, \\ldots, w_l\\rangle$ of $l = |x|$ words, where each word belongs to a vocabulary $\\mathcal W$ of $V = |\\mathcal W|$ words. We use $y \\in \\mathcal Y$ to denote a target (or response) variable, which will be from a countably finite set $\\{1, \\ldots, K\\}$ in text classification, or from a countably infinite set (e.g., $\\mathbb N_0$ or $\\mathbb N_1$) or an uncountable set (e.g., $\\mathbb R$, $\\Delta_{3-1}$) in text regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWwuQCrZp5j"
      },
      "source": [
        "## Feature Functions\n",
        "\n",
        "Before we can talk about our key modelling idea, we need to talk about feature functions. \n",
        "\n",
        "A **vector-valued feature function** \n",
        "\\begin{equation}\n",
        "  \\mathbf h: \\mathcal X \\to \\mathbb R^D\n",
        "\\end{equation}\n",
        "maps a document $x \\in \\mathcal X$ to a $D$-dimensional *feature vector*. The feature vector is something that \"describes\" a document along $D$ numerical dimensions, each of which quantifies an aspect of the problem that's believed to play some significant role in the kind of analysis we are making. \n",
        "\n",
        "In this notebook we will use feature functions based on count vectors. The simplest such feature function has a dimension for each known word (i.e., $D=V$), the $\\mathbf(x)$ is such that $h_d(x) = \\sum_{i=1}^l [x_i = d]$ is the number of times the word corresponding to dimension $d$ occurs in $x$. We may also normalise this by sequence length, in that case, $h_d(x) = \\frac{1}{l}\\sum_{i=1}^l [x_i = d]$ is one definitoin of *term-frequency* (or tf). In some applications it also makes sense to discount the importance of words based on whether they are themselves too frequent to be informative, this can be achieved by the inverse-document-frequency (or idf). For count-based, tf-based, and tf-idf-based features we will use robust implementations from [`scikitlearn`](https://scikit-learn.org/stable/modules/feature_extraction.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqiAg9eCUUBm"
      },
      "source": [
        "## Generalised Linear Models\n",
        "\n",
        "Our new general tool for text analysis (both classification and regression) is a *generalised linear model* (GLM). \n",
        "\n",
        "A GLM is a conditional model of $Y|X=x$ where we compute the *parameter* of our statistical model with the help of a parametric linear transformation of $\\mathbf h(x)$. The linear transformation has its own parameters (the coefficients we multiply and the biases we add), which we will denote generically by $\\boldsymbol\\theta$. \n",
        "\n",
        "A GLM is a conditional model of $Y|X=x$, as always, the conditional is governed by a known parametric family that we get to choose (e.g., Bernoulli, Categorical, Binomial, Geometric, Poisson, Zeta, etc.). To fully specify a model, we need to predict from $x$ the statistical parameter of the distribution, let's denote that parameter by $g(x; \\boldsymbol\\theta)$, where $\\boldsymbol\\theta$ is the collection of trainable parameters of the model.\n",
        "\n",
        "In a GLM, $g(x; \\boldsymbol\\theta)$ is a (possibly nonlinear) transformation $a(\\cdot)$ of a linear function of $\\boldsymbol\\theta$ and $\\mathbf h(x)$. That is:  \n",
        "\\begin{equation}\n",
        "g(x; \\boldsymbol\\theta) = a(\\underbrace{\\mathbf w^\\top \\mathbf h(x) + b)}_{\\text{1 linear predictor}}\n",
        "\\end{equation}\n",
        "with $\\mathbf w \\in \\mathbb R^D$ and $b \\in \\mathbb R$, for a single-parameter distribution; or\n",
        "\\begin{equation}\n",
        "\\mathbf g(x; \\boldsymbol\\theta) = \\mathbf a(\\underbrace{\\mathbf W^\\top \\mathbf h(x) + \\mathbf b}_{K\\text{ linear predictors}})\n",
        "\\end{equation}\n",
        "with $\\mathbf W \\in \\mathbb R^{D \\times K}$ and $\\mathbf b \\in \\mathbb R^K$, for a $K$-parameters distribution.\n",
        "\n",
        "The function $a(\\cdot)$ is used to constrain the quantity  $\\mathbf w^\\top \\mathbf h(x) + b$, also called **linear predictor**, to the valid range of parameters for the statistical distribution we chose. This function is called *inverse link function* in statistics, and it is called **activation function** in deep learning.\n",
        "\n",
        "In the multiparameter case, the activation function $\\mathbf a(\\cdot)$ is vector-valued. This tutorial will mostly concentrate on the single parameter case, which is sufficient for most applications of binary classification and ordinal regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbvyWvjdjv3f"
      },
      "source": [
        "### Bernoulli GLM\n",
        "\n",
        "For example, in spam classification we want to map a document $x \\in \\mathcal X$ to the probability of it being spam or not, let's denote this probability by $g(x; \\boldsymbol\\theta)$ and recall that $0 \\le g(x; \\theta) \\le 1$. \n",
        "\n",
        "The statistical model of interest is\n",
        "\\begin{align}\n",
        "Y|X=x \\sim \\mathrm{Bernoulli}(g(x; \\boldsymbol\\theta))\n",
        "\\end{align}\n",
        "where $g(x; \\boldsymbol\\theta)$ is the *probability* of labelling a document as spam. \n",
        "\n",
        "Let's define this function $g$. As documents are not objects in a numerical space, our GLMs will all start by mapping $x$ to a feature vector via a feature function $\\mathbf h(x)$. One possible GLM for this binary classification problem is the following:\n",
        "\\begin{align}\n",
        "  g(x; \\boldsymbol\\theta) &= \\mathrm{sigmoid}(\\mathbf w^\\top \\mathbf h(x) + b)\n",
        "\\end{align}\n",
        "where $\\mathbf w \\in \\mathbb R^{D}$ and $b \\in \\mathbb R$ are the parameters of the model $\\boldsymbol\\theta = \\{\\mathbf w, b\\}$.\n",
        "Step by step:\n",
        "1. we map $x \\in \\mathcal X$ to a feature vector $\\mathbf h(x) \\in \\mathbb R^D$; this is a deterministic step and it does not require interacting with the parameters $\\boldsymbol\\theta$ of the GLM;\n",
        "2. we take the dot-product between the feature vector and the parameter $\\mathbf w$, which gives us a scalar (a real value);\n",
        "3. we add a bias value to that scalar, obtaining a scalar (a real value);\n",
        "4. finally, the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) maps this real value to a valid probability value.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** Plot the sigmoid function for the following range of values:\n",
        "\n",
        "```python\n",
        "s = np.linspace(-10, 10, 1000)\n",
        "```"
      ],
      "metadata": {
        "id": "1---p0yzDU1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "# **SOLUTION**\n",
        "_ = plt.plot(np.linspace(-10, 10, 1000), 1/(1 + np.exp(-np.linspace(-10, 10, 1000))))\n",
        "_ = plt.xlabel('s')\n",
        "_ = plt.ylabel('1/(1+exp(-s))')\n",
        "```\n",
        "\n",
        "---    \n",
        "</details>\n"
      ],
      "metadata": {
        "id": "EGHoDHJiDgyF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkiT41Jahp-t"
      },
      "source": [
        "### Categorical GLM\n",
        "\n",
        "This idea is so powerful that we can build pretty much every text classifier and every text regressor in the book. \n",
        "\n",
        "For example, for a $K$-way classifier \n",
        "\\begin{align}\n",
        "  Y|X=x \\sim \\mathrm{Categorical}(\\mathbf g(x; \\boldsymbol\\theta))\n",
        "\\end{align}\n",
        "here $\\mathbf g(x; \\boldsymbol\\theta) \\in \\Delta_{K-1}$ is a $K$-dimensional vector of positive values that sum to 1. \n",
        "This is a valid GLM for this model:\n",
        "\\begin{align}\n",
        "\\mathbf s &= \\mathbf W^\\top\\mathbf h(x) + \\mathbf b \\\\\n",
        "\\mathbf g(x; \\boldsymbol\\theta) &= \\mathrm{softmax}(\\mathbf s)\n",
        "\\end{align}\n",
        "\n",
        "where $\\boldsymbol = \\{\\mathbf W, \\mathbf b\\}$ are the parameters of the model, $\\mathbf W \\in \\mathbb R^{D \\times K}$ is a matrix that maps $D$ inputs (the feature values) to $K$ outputs (real values), $\\mathbf b \\in \\mathbb R^K$ is a vector of biases (one real value bias per class), and [softmax](https://en.wikipedia.org/wiki/Softmax_function) turns the $K$-dimensional linear predictor $\\mathbf s$ into $K$ strictly positive numbers that sum to 1 (i.e., a probability vector in the simplex $\\Delta_{K-1}$).\n",
        "\n",
        "In deep learning literature the vector $\\mathbf s$ that we use as input to the softmax function is often called the vector of *scores* or *logits*, each one of its coordinates quantifies the importance of the corresponding class in a logarithmic scale.\n",
        "\n",
        "This model is also known as a *log-linear* model, that is because the logarithm of the softmax is a linear function of $\\mathbf h(x)$ and $\\boldsymbol\\theta$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we demonstrate that the softmax function indeed maps 3-dimensional real vectors to 3-dimensional probability vectors."
      ],
      "metadata": {
        "id": "tFA7vIbkcdR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def np_softmax(x, axis=-1):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "# let's get 1000 values for s0\n",
        "s0 = np.linspace(-3.0, 3.0, 1000)\n",
        "# for s1 and s2 we will have 1s and 0s\n",
        "s = np.vstack([s0, np.ones_like(s0), np.zeros_like(s0)]).T\n",
        "\n",
        "_ = plt.plot(s0, np_softmax(s)[:, 0], linewidth=2, label='output 0')\n",
        "_ = plt.plot(s0, np_softmax(s)[:, 1], linewidth=2, label='output 1')\n",
        "_ = plt.plot(s0, np_softmax(s)[:, 2], linewidth=2, label='output 2')\n",
        "_ = plt.plot(s0, np_softmax(s).sum(-1), ':', c='black', linewidth=2, label='elementwise sum')\n",
        "_ = plt.xlabel(r'$s_0$')\n",
        "_ = plt.ylabel('softmax output')\n",
        "_ = plt.legend()"
      ],
      "metadata": {
        "id": "tlhbEfBrFN4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqllSKbyj-jy"
      },
      "source": [
        "### Poisson GLM\n",
        "\n",
        "Let's attempt to parameterise a conditional distribution over the number of likes $Y$ that a tweet $x$ will receive, based on its content as captured by a feature function $\\mathbf h(x) \\in \\mathbb R^D$.\n",
        "\n",
        "Suppose that for this statistical model, we decide to use a Poisson distribution, then\n",
        "\\begin{align}\n",
        "  Y|X=x &\\sim \\mathrm{Poisson}(g(x; \\boldsymbol\\theta))\n",
        "\\end{align}\n",
        "because this is a Poisson, we now that $g(x; \\boldsymbol\\theta)$ must be strictly positive. So, here's a valid GLM:\n",
        "\\begin{align}\n",
        "  g(x; \\boldsymbol\\theta) &= \\exp(\\mathbf w^\\top \\mathbf h(x) + b)\n",
        "\\end{align}\n",
        "with model parameters are $\\boldsymbol\\theta = \\{\\mathbf w, b\\}$ with $\\mathbf w \\in \\mathbb R^D$ and $b \\in \\mathbb R$.\n",
        "\n",
        "Another GLM for the Poisson is \n",
        "\\begin{align}\n",
        "  g(x; \\boldsymbol\\theta) &= \\mathrm{softplus}(\\mathbf w^\\top \\mathbf h(x) + b)\n",
        "\\end{align}\n",
        "which uses the softplus function $\\log(1 + e^s)$. The softplus function, like the exp, turns any real number into a strictly positive number. Unlike the exp, the softplus is approximately linear as $s$ increases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** Plot the both the exponential and the softplus function over the following range of values:\n",
        "\n",
        "```python\n",
        "s = np.linspace(-10, 10, 1000)\n",
        "```\n",
        "\n",
        "Use subplots so you can visualise hwo they differ for larger positive $s$."
      ],
      "metadata": {
        "id": "1rtZpzfdBOWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
        "_ = axs[0].plot(np.linspace(-10, 10, 1000), np.exp(np.linspace(-10, 10, 1000)))\n",
        "_ = axs[0].set_xlabel('s')\n",
        "_ = axs[0].set_ylabel('exp(s)')\n",
        "_ = axs[1].plot(np.linspace(-10, 10, 1000), np.log(1 + np.exp(np.linspace(-10, 10, 1000))))\n",
        "_ = axs[1].set_xlabel('s')\n",
        "_ = axs[1].set_ylabel('softplus(s)')\n",
        "fig.tight_layout(h_pad=1, w_pad=1)\n",
        "```\n",
        "\n",
        "---    \n",
        "</details>\n"
      ],
      "metadata": {
        "id": "bGKkahW3BtYn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfOTsJZdxEX-"
      },
      "source": [
        "## General principles for prescribing GLMs\n",
        "\n",
        "1. Start with choosing  the family you will model with. This depends on the type of data you have (e.g., binary means Bernoulli, $K$-way classification usually means Categorical, ordinal regression usually means Poisson, but it could be some other distribution over natural numbers or a subset thereof, like the Binomial, continuous regression might require a Normal distribution, regressing to vectors of proportions might require a Dirichlet distribution, etc.). You don't need to know all these distributions by heart, when needed, we will give you information about them that will help you judge their relevance in context.\n",
        "\n",
        "2. The input is text, but GLMs operate with inputs in the real coordinate space, so you need a vector-valued feature function $\\mathbf h(x)$.\n",
        "\n",
        "3. In a GLM, the input $\\mathbf h(x)$ and the parameters $\\boldsymbol\\theta$ interact linearly. This constrains us to either operations like dot product, matrix multiplication and scalar or vector addition. Whether we have \"dot product plus scalar\" or \"matrix multiplication plus vector\" depends exclusively on the dimensionality we need for the linear predictor. If we need a single scalar, we wil use the former. If we need a vector, we will use the latter.\n",
        "\n",
        "4. Example 1: the Poisson parameter is a single scalar, thus we know that we need to map from $\\mathbf h(x)$ to a single scalar, as we achieve with \"dot product plus scalar\". Example 2: the Categorical parameter is a vector, thus we know that we need to map from $\\mathbf h(x)$ to a $K$-dimensional vector, as we achieve with \"matrix multiplication plus vector\".\n",
        "\n",
        "5. Finally, the statistical parameter is generally constrained to a subset of the real numbers, so we need an activation function that constrains the linear predictor accordingly. Example 1: the Poisson parameter is *strictly positive* by definition, so we need to wrap the linear function around something whose output is *never negative* and *never 0*, no matter which real-valued input we give it. The exponential function does that for us. There are other activation functions that achieve the same result, but the exponential is convenient for certain reasons (e.g., it's logarithm is linear). In our implementation below we will see other options. Example 2: the Categorical parameter must be a probability vector, the softmax function can realise that constraint for us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter estimation\n",
        "\n",
        "Given a training set $\\mathcal D = \\{(x^{(n)}, y^{(n)})\\}_{n=1}^N$ of $N$ observed input-target pairs, we would ideally choose the parameter vector $\\boldsymbol\\theta$ that maximises the log-likelihood of the model:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) &= \\sum_{n=1}^N \\log P(Y=y^{(n)}|X=x^{(n)}; \\boldsymbol \\theta)  \\\\\n",
        "\\boldsymbol\\theta^\\star &= \\arg\\max_{\\boldsymbol\\theta}~\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) \n",
        "\\end{align}\n",
        "\n",
        "Unlike tabular CPDs, there is no simple expression for the MLE of a GLM. But, we can employ a gradient-based search. This search uses the gradient $\\nabla_{\\boldsymbol \\theta}\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta)$ to iteratively update an existing $\\boldsymbol \\theta$, starting from an initial guess $\\boldsymbol \\theta^{(0)}$, which is typically a random initialisation of the parameters.\n",
        "\n",
        "At iteration $t$, the update rule is\n",
        "\\begin{equation}\n",
        "\\boldsymbol \\theta^{(t+1)} = \\boldsymbol \\theta^{(t)}  + \\gamma_t \\nabla_{\\boldsymbol \\theta^{(t)}}\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta^{(t)})\n",
        "\\end{equation}\n",
        "where the log-likelihood is assessed using the current parameters, we obtain the gradient for it, and combine it with the current parameters to get the next iterate. The quantity $\\gamma_t > 0$ is called a *learning rate*.\n",
        "\n",
        "<details>\n",
        "    <summary><b>Maximisation vs minimmisation</b></summary>\n",
        "\n",
        "If you have seen this formula before with a *minus* rather than a *plus* for the gradient, don't worry, it is the same notion. You *sum* the gradient if you are maximising the log-likelihood, and you *subtract* the gradient if you are minimising the negative log-likelihood. The two procedures yield the exact same optimum.\n",
        "\n",
        "---    \n",
        "</details>\n"
      ],
      "metadata": {
        "id": "WtvPFOeI8lpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient-based optimisation\n",
        "\n",
        "Assessing each one of the terms of the log-likelihood function \n",
        "\n",
        "Assessing the log-likelihood of a certain value of the parameter vector $\\boldsymbol\\theta$ requires assessing the probability mass (or density, for continuous variables) of each one of our observations under the current value of the parameter. Each one such assessment on its own is not at all challenging, but assessing all the $N$ terms can be challenging for large $N$.\n",
        "\n",
        "Fortunately, we can use a stochastic gradient procedure, which still has the same guarantees as the deterministic procedure.\n",
        "\n",
        "At each iteration $t$, we compute an approximation to $\\nabla_{\\boldsymbol \\theta^{(t)}}\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta^{(t)})$. This approximation is a Monte Carlo (MC) estimate obtained using $S < N$ data points uniformly sampled from $\\mathcal D$:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_{\\boldsymbol \\theta^{(t)}}\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta^{(t)}) &\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{n=1}^S \\nabla_{\\boldsymbol \\theta^{(t)}} \\log P(Y=y|X=x; \\boldsymbol \\theta^{(t)})\n",
        "\\end{align}\n",
        "\n",
        "We can obtain this gradient by essentially pretending, at each iteration $t$, that the log-likelihood function depends only on a small *batch* of $S$ observations drawn from the training set:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal L_{\\mathcal B}(\\boldsymbol\\theta^{(t)}) = \\frac{1}{S}\\sum_{s=1}^S \\log P(Y=y^{(s)}|X=x^{(s)}; \\boldsymbol \\theta^{(t)})\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "-y2IiYGU-NaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularisation\n",
        "\n",
        "Oftentimes, we have **many** features, and thus **many parameters**. This gives models the capacity to discover correlations that have no real predictive power (e.g., that [capivaras](https://en.wikipedia.org/wiki/Capybara) implies a *negative* sentiment, simply because the 1 time that word was seen in the data was in a document labelled with the negative class, for example the document might have been \"I loved the capivaras but overall the horrible weather ruined the trip\"). \n",
        "\n",
        "These are called **spurious correlations** and we would rather not be mislead by them. \n",
        "\n",
        "In an attempt to get rid of them, we employ a so called **regulariser**. This is a penalty on the objective based on the norm of the parameter vector, and usually employ the L2 norm.\n",
        "\n",
        "The L2 norm of a vector is:\n",
        "\\begin{equation}\n",
        "\\mathcal R(\\mathbf v) = \\sqrt{\\sum_{d=1}^D v_d^2}\n",
        "\\end{equation}\n",
        "For a collection of parameter vectors we sum the L2 norms of each vector. \n",
        "\n",
        "Thus, the final objective for *regularised maximum likelihood estimation* is \n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol\\theta^\\star = \\arg\\max_{\\boldsymbol\\theta}~\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) - \\lambda \\mathcal R(\\boldsymbol\\theta)\n",
        "\\end{equation}\n",
        "where $\\lambda \\ge 0$ is a hyperparameter used to control the importance of the regulariser. There's no way to choose the value of $\\lambda$ directly from the training data, we have to try various values and test the model's performance on heldout data."
      ],
      "metadata": {
        "id": "1QrGXlj-Bj4Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7IUWL0aln_b"
      },
      "source": [
        "# Binary classifier\n",
        "\n",
        "Here we use the `subjectivity` corpus from NLTK to train binary classifiers. We will train a Naive Bayes classifier using scikitlearn and a Bernoulli GLM which we will develop in JAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u09CRt2Hu4vz"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import subjectivity  # binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5KbhWdhu4v0"
      },
      "outputs": [],
      "source": [
        "labels = subjectivity.categories()\n",
        "C = len(labels)\n",
        "print(\"{}-way classification:\\n{}\".format(C, '\\n'.join(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnMGDKpPu4v1"
      },
      "source": [
        "As usual, we will split our observations in three disjoint sets, 80% for training, 10% for whatever development purposes we have, and 10% for testing the generalisation of our classifier at the end. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8qvqgPVu4v1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def prepare_nltk_corpus(nltk_corpus, categories, seed=23, BOS='<s>', EOS='</s>'):\n",
        "    \"\"\"\n",
        "    Prepare an nltk text categorization corpus in a sklearn friendly format.\n",
        "    \n",
        "    This function is very similar to what you saw in T2, but here we add BOS tokens in addition to EOS tokens \n",
        "    (while the BOS token has no effect in NBC with unigram conditionals, \n",
        "    it can be useful for some of the feature-richer classifiers we will develop here).\n",
        "    \n",
        "    :param nltk_corpus: something like sentence_polarity\n",
        "    :param categories: a list of categories (each a string), \n",
        "        sklearn will treat categories as 0-based integers, thus we will map the ith element in this list to y=i\n",
        "    :param seed: for reproducibility\n",
        "    :param BOS: if not None, start every sentence with a single BOS token\n",
        "    :param EOS: if not None, end every sentence with a single EOS token\n",
        "    :return: training, dev, test\n",
        "        each an np.array such that \n",
        "        * array[:, 0] are the inputs (documents, each a string)\n",
        "        * array[:, 1] are the outputs (labels)\n",
        "    \"\"\"\n",
        "    pairs = []    \n",
        "    prefix = [BOS] if BOS else []\n",
        "    suffix = [EOS] if EOS else []\n",
        "    for label in categories:  # here we pair doc (as a single string) and label (string)\n",
        "        # this time we will concatenate the EOS symbol to the string\n",
        "        pairs.extend((' '.join(prefix + s + suffix), label) for s in nltk_corpus.sents(categories=[label]))\n",
        "    # we turn the pairs into a numpy array\n",
        "    # np arrays are very convenient for the indexing tools np provides, as we will see\n",
        "    pairs = np.array(pairs)\n",
        "    # it's good to shuffle the pairs\n",
        "    rng = np.random.RandomState(seed)    \n",
        "    rng.shuffle(pairs)\n",
        "    # let's split the np array into training (80%), dev (10%), and test (10%)\n",
        "    num_pairs = pairs.shape[0]\n",
        "    # we can use slices to select the first 80% of the rows\n",
        "    training = pairs[0:int(num_pairs * 0.8),:]\n",
        "    # and similarly for the next 10%\n",
        "    dev = pairs[int(num_pairs * 0.8):int(num_pairs * 0.9),:]\n",
        "    # and for the last 10%\n",
        "    test = pairs[int(num_pairs * 0.9):,:] \n",
        "    return training, dev, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MHE7lMyu4v1"
      },
      "source": [
        "Separate our corpus into training/dev/test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwmUaINGu4v2"
      },
      "outputs": [],
      "source": [
        "so_training, so_dev, so_test = prepare_nltk_corpus(subjectivity, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HInVHrPNu4v2"
      },
      "outputs": [],
      "source": [
        "so_training.shape, so_dev.shape, so_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** As always, begin by inspecting your data."
      ],
      "metadata": {
        "id": "qSNp929mdFPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "\n",
        "# **SOLUTION**\n",
        "\n",
        "_ = plt.hist([len(x.split()) for x, y in so_training], bins='auto')\n",
        "_ = plt.xlabel(\"Length in tokens\")\n",
        "_ = plt.show()\n",
        "\n",
        "_ = plt.hist([y for x, y in so_training], bins='auto')\n",
        "_ = plt.ylabel(\"Frequency\")\n",
        "_ = plt.xlabel(\"Class\")\n",
        "_ = plt.show()\n",
        "\n",
        "print(tabulate(so_training[4:6], headers=['doc', 'label']))\n",
        "```\n",
        "\n",
        "---    \n",
        "</details>\n"
      ],
      "metadata": {
        "id": "5g3K64I3jwSf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcYKb1SGU9dS"
      },
      "source": [
        "Helper code to map named labels to 0-based integers and back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PugoF4SY2MbF"
      },
      "outputs": [],
      "source": [
        "def label_as_string(y, vocab={True: 'subj', False: 'obj', 1: 'subj', 0: 'obj'}):\n",
        "    \"\"\"Map from boolean or integer to a string label\"\"\"\n",
        "    return [vocab[b] for b in y]\n",
        "\n",
        "def label_as_int(y, vocab={'subj': 1, 'obj': 0}):\n",
        "    \"\"\"Map from string to integer\"\"\"\n",
        "    return np.array([vocab[b] for b in y], dtype=int)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UteCynrnVQdL"
      },
      "outputs": [],
      "source": [
        "label_as_string([True, False]), label_as_int(['subj', 'obj'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Klxu7hlyCX"
      },
      "source": [
        "## Naive Bayes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcbnc40xVfXJ"
      },
      "source": [
        "Let's start with a classifier we already know, this will also help us get comfortable with feature functions. \n",
        "\n",
        "The NBC can be represented in terms of feature functions. Suppose a feature function $\\mathbf h(x)$ maps $x$ to a space where each coordinate $d$ corresponds to a word in the vocabulary, and $h_d(x)$ is the number of times that word occurs in $x$. This would make $\\mathbf h(x) \\in \\mathbb R^V$ a $V$-dimensional vector, but most of its coordinates would be in fact 0, since only up to $l = |x|$ words occur in $x$. If we use the notation $(f, n) \\in \\mathbf h(x)$ to denote all feature-count pairs for which the feature is a word and the count is not zero, we can rewrite the joint probability of the NB classifier as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "    P_{YX}(y, x) = P_Y(y)\\prod_{(f, n) \\in \\mathbf h(x)} P_{F|Y}(f|y)^n\n",
        "\\end{equation}\n",
        "\n",
        "In class we discussed how this view of NBC can be used to generalise it to feature types are not just words, but in this tutorial we will continue working with word counts.\n",
        "\n",
        "To represent feature functions efficiently sklearn uses *vectorizers*. These classes turn a string into a sparse vector of coded features. Internally this builds a vocabulary of features and a data structure that is sparse like nested dicts, but much more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yM6fpsXVfKw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od8mvtCfVj8h"
      },
      "source": [
        "Take a moment to read the documentation of `CountVectorizer?` and play a bit with some examples.\n",
        "\n",
        "The count vectorizer can be used to implement NBC. We simply need to count the unigrams, which in sklearn style is done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pocCtTmVpX4"
      },
      "outputs": [],
      "source": [
        "toy_vectorizer = CountVectorizer(ngram_range=(1,1), min_df=2)\n",
        "# let's start with the first 5 sentences in the training set, just to see what this does\n",
        "toy_vectorizer.fit(so_training[:5, 0])  \n",
        "print(tabulate(so_training[:5], headers=['doc', 'label']))\n",
        "print(f\"The feature space contains {len(toy_vectorizer.get_feature_names_out())} features.\\nThey are:\")\n",
        "for fname in toy_vectorizer.get_feature_names_out():\n",
        "    print(f\" F={fname}\")\n",
        "print(\"Note that skleanr is pre-processing the text for us, getting rid of some punctuation marks and English stopwords. We also chose to keep only words that occurred at least twice (with min_df=2).\")\n",
        "print(\"To apply the feature function to text we use the method *transform*:\")\n",
        "h_x05 = toy_vectorizer.transform(so_training[:5, 0])\n",
        "print(f\" which gives us an object of shape {h_x05.shape}, as expected.\")\n",
        "print(h_x05.shape)\n",
        "print(\"But that object is a sparse array (the zeros are not stored in it):\")\n",
        "print(h_x05)\n",
        "print(\"In some occasions we may need a dense numpy array, when that's the case we can use *toarray*:\")\n",
        "print(h_x05.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With count features we can implement the NB classifier. In sklearn it is called `MultinomialNB`. \n",
        "\n",
        "**Exercise with solution** Check its documentation. Train a CountVectorizer with `min_df=1` using all of the training data. Then use it to train a MultinomialNB with alpha (the Laplace smoothing coefficient) set to 0.7. Evaluate its performance on the *test set*, report the results via sklearn's `classification_report` as well as `ConfusionMatrixDisplay.from_predictions`.\n",
        "\n",
        "You should obtain about 92% macro F1.\n",
        "\n",
        "This tutorial is *not* about NBC, we train it just for comparison and to teach you a nice sklearn trick. Don't let this exercise take too much of your time."
      ],
      "metadata": {
        "id": "OapzoU0vevE2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfAzpStdu4v3"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "# **SOLUTION**\n",
        "nb_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "nb_vectorizer.fit(so_training[:, 0])\n",
        "nb_cls = MultinomialNB(alpha=0.7)\n",
        "nb_cls.fit(nb_vectorizer.transform(so_training[:,0]), so_training[:, 1])\n",
        "\n",
        "print(\"Classification report\")\n",
        "y_pred = nb_cls.predict(nb_vectorizer.transform(so_test[:, 0]))\n",
        "print(classification_report(so_test[:, 1], y_pred))\n",
        "print(\"Confusion matrix\")\n",
        "_ = ConfusionMatrixDisplay.from_predictions(so_test[:,1], y_pred)\n",
        "```\n",
        "\n",
        "---    \n",
        "</details>\n",
        "\n",
        "\n",
        "<details>\n",
        "    <summary><b>Curious how we got to alpha=0.7?</b></summary>\n",
        "\n",
        "We got sklearn to perform a grid search using cross validation on the training set for us:\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "cls_nb = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer(ngram_range=(1,1))),\n",
        "        ('clf', MultinomialNB(alpha=0.7)),                 \n",
        "    ]\n",
        ")\n",
        "# See the hyperparameters of the model (stuff like ngram_range and Laplace coefficient)\n",
        "print(cls_nb.get_params())\n",
        "# We can search for values of the hyperparameters of interest using cross validation\n",
        "nb_grid = GridSearchCV(cls_nb, param_grid={'clf__alpha': np.linspace(0.1, 1., 10)}, cv=3)\n",
        "nb_grid.fit(so_training[:, 0], so_training[:, 1])\n",
        "print(nb_grid.best_params_)\n",
        "print(classification_report(so_dev[:,1], nb_grid.predict(so_dev[:, 0])))\n",
        "```\n",
        "\n",
        "---    \n",
        "</details>\n"
      ],
      "metadata": {
        "id": "4tsBuB23f84K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELy5fJlAeL92"
      },
      "source": [
        "## Binary GLMs on Jax\n",
        "\n",
        "To train GLMs we need gradient-based optimisation. Nowadays we do not compute derivatives by hand, nor implement automatic differentiation, we use software that does that for us, efficiently and realiably. \n",
        "\n",
        "In this tutorial you will use JAX, for you've already seen it in ML.\n",
        "\n",
        "Because we need JAX's automatic differentiation, we will need to express our model's computations using building blocks available in JAX. Luckily for us, JAX implements something very much like numpy and something very much like scipy, so all we've been using so far in the course is likely already available in JAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgU8HDoje43F"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import jax.scipy as jsp\n",
        "from jax import grad, value_and_grad, random\n",
        "from jax.nn import softplus, softmax, sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fk5xAxDfF4X"
      },
      "outputs": [],
      "source": [
        "# we need a random generator in order to create arrays in JAX\n",
        "key = random.PRNGKey(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiUeDXY4nbW6"
      },
      "source": [
        "Here we will \n",
        "\n",
        "1. describe the necessary functions to parameterise a Bernoulli GLM\n",
        "2. implement a stochastic gradient-based procedure for parameter estimation.\n",
        "\n",
        "Roughly this is what we need to do:\n",
        "\n",
        "1. obtain a feature function $\\mathbf h$ and compute it for all our data points\n",
        "2. obtain some initial parameters for the linear function\n",
        "3. implement the linear function that outputs the linear predictor\n",
        "4. constrain the linear predictor to being a probability value\n",
        "5. write a training loop, where we use some subset of the data to estimate the value of the log-likelihood function, use the gradient of that value with respect to model parameters to improve our parameter estimates\n",
        "\n",
        "So, let's get started. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature function\n",
        "\n",
        "For feature function we are going to start with word counts (ie, `CountVectorizer`) and then we will normalise those into `tf-idf` features (see `TfidfTransformer`), which will help us take frequency information into account."
      ],
      "metadata": {
        "id": "5eIeN0fDlVxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "so_ff = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer(ngram_range=(1,1), min_df=5)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "    ]\n",
        ")\n",
        "so_ff.fit(so_training[:, 0], so_training[:, 1])"
      ],
      "metadata": {
        "id": "khMgBavjlbeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so_training_h_sparse = so_ff.transform(so_training[:, 0])\n",
        "so_dev_h_sparse = so_ff.transform(so_dev[:, 0])\n",
        "so_test_h_sparse = so_ff.transform(so_test[:, 0])"
      ],
      "metadata": {
        "id": "QPqLYEUIl1sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRZcw4hGf7C4"
      },
      "outputs": [],
      "source": [
        "# The number of input features is the $D$ in the formulas above.\n",
        "num_features = so_training_h_sparse.shape[1]\n",
        "num_features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing the Bernoulli parameter"
      ],
      "metadata": {
        "id": "hKooRFs5mK-y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHTyXRWanSwp"
      },
      "source": [
        "To compute the Bernoulli parameter we need a linear function, which needs weights and a bias. So here we obtain those.\n",
        "\n",
        "Let's obtain $\\boldsymbol\\theta$ for binary classification using `num_features` input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ8Nntm2fnCm"
      },
      "outputs": [],
      "source": [
        "def init_params_linear1(num_features, key):\n",
        "    \"\"\"\n",
        "    num_features: dimensionality of the feature space\n",
        "    key: a JAX random generator\n",
        "\n",
        "    Return w with shape [num_features] and bias with shape []\n",
        "    \"\"\"\n",
        "    w = random.uniform(key, shape=(num_features,))\n",
        "    b = random.uniform(key)\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-pJ4zsX3ktu"
      },
      "outputs": [],
      "source": [
        "key, param_key = random.split(key, 2) \n",
        "w, b = init_params_linear1(num_features, param_key)\n",
        "assert w.shape == (num_features,)\n",
        "assert b.shape == ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-m_V3Jkngva"
      },
      "source": [
        "**Exercise with solution** \n",
        "\n",
        "Implement a linear function of $\\mathbf h(x)$ and $\\boldsymbol\\theta$ with a single real-valued output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH_PdRkkgon5"
      },
      "outputs": [],
      "source": [
        "def linear1(inputs, *, w, b):\n",
        "    \"\"\"\n",
        "    inputs: a collection of inputs [batch_size, num_features]\n",
        "      we normally program our models supporting the ability to perform the same operations\n",
        "      for multiple documents at once\n",
        "      we do so by \"batching\" documents together, so our first dimension is used \n",
        "      to iterate over different documents\n",
        "    w: parameters (vector of size num_features)\n",
        "      see that the parameters do not depend on batch size\n",
        "      that's because parameters are a property of the model and batch size isn't\n",
        "    b: parameters (single scalar)\n",
        "\n",
        "    Return the following computation\n",
        "        \\sum_{d=1}^D x[d] * w[d] + b\n",
        "    for each document x[n] amongst the inputs.\n",
        "    \"\"\"   \n",
        "    raise NotImplementedError(\"Implement me!\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the dev set is not too large, we will convert it to a dense numpy array, so that we can use it within JAX."
      ],
      "metadata": {
        "id": "phAZVIVHm4FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "so_dev_h = so_dev_h_sparse.toarray()\n",
        "\n",
        "# DO NOT use .toarray for the training data, you could run out of memory"
      ],
      "metadata": {
        "id": "dyArQ6Qfm1hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike sklearn, JAX does not understand string labels, so we must convert our labels to integers:"
      ],
      "metadata": {
        "id": "Db8znaGZKMnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "so_training_y = label_as_int(so_training[:, 1])\n",
        "so_dev_y = label_as_int(so_dev[:, 1])\n",
        "so_test_y = label_as_int(so_test[:, 1])"
      ],
      "metadata": {
        "id": "NtULSvoaKEuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can test our `linear1` function:"
      ],
      "metadata": {
        "id": "S3nN3VdEm9zd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VlrUDrdoa_z"
      },
      "outputs": [],
      "source": [
        "# We can give it a single document, then we get a single output\n",
        "assert linear1(so_dev_h[0:1], w=w, b=b).shape == (1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7GbD_YHonXa"
      },
      "outputs": [],
      "source": [
        "# or we can give it 10 documents, then we get 10 outputs\n",
        "assert linear1(so_dev_h[0:10], w=w, b=b).shape == (10,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Og0fDl2gpXv"
      },
      "outputs": [],
      "source": [
        "# or we can give it the entire dev set, and get one output per doc in the dev set\n",
        "assert linear1(so_dev_h, w=w, b=b).shape == (so_dev_h.shape[0],)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "def linear1(inputs, *, w, b):\n",
        "    \"\"\"\n",
        "    inputs: a collection of inputs [batch_size, num_features]\n",
        "      we normally program our models supporting the ability to perform the same operations\n",
        "      for multiple documents at once\n",
        "      we do so by \"batching\" documents together, so our first dimension is used \n",
        "      to iterate over different documents\n",
        "    w: parameters (vector of size num_features)\n",
        "      see that the parameters do not depend on batch size\n",
        "      that's because parameters are a property of the model and batch size isn't\n",
        "    b: parameters (single scalar)\n",
        "\n",
        "    Return the following computation\n",
        "        \\sum_{d=1}^D x[d] * w[d] + b\n",
        "    for each document x[n] amongst the inputs.\n",
        "    \"\"\"    \n",
        "    # elementwise product\n",
        "    # [batch_size, num_features]\n",
        "    out = w * inputs  # by default this multiplication happens elementwise along the last dimension of the tensor\n",
        "    # reduce the feature dimension via sum \n",
        "    # [batch_size]\n",
        "    out = jnp.sum(out, axis=-1) \n",
        "    # add bias\n",
        "    out = out + b\n",
        "    # [batch_size]\n",
        "    return out\n",
        "```\n",
        "\n",
        "---    \n",
        "</details>"
      ],
      "metadata": {
        "id": "ZsH3yaamW2lW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN7ZE69lpbeO"
      },
      "source": [
        "For this statistical model, a binary classifier, we need to map $x$ to a probability value (the Bernoulli parameter), but the linear function is not constrained accordingly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS3_pcASpUXr"
      },
      "outputs": [],
      "source": [
        "assert jnp.alltrue(linear1(so_dev_h, w=w, b=b) >= 0.) and jnp.alltrue(linear1(so_dev_h, w=w, b=b) <= 1.) == False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LnmksvKo_DM"
      },
      "source": [
        "For that, jax offers the sigmoid function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS8jeQkgpJLQ"
      },
      "outputs": [],
      "source": [
        "def make_prob(linear_predictor):\n",
        "    \"\"\"\n",
        "    Properly constrains the linear predictor in order to parameterise a Bernoulli distribution.\n",
        "\n",
        "    linear_predictor: an array of linear predictors with shape [batch_size]\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MlPa1Bippwp"
      },
      "source": [
        "which constrains everything as we wanted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03YQX7V6pM6N"
      },
      "outputs": [],
      "source": [
        "assert jnp.alltrue(make_prob(linear1(so_dev_h, w=w, b=b)) >= 0.) and jnp.alltrue(make_prob(linear1(so_dev_h, w=w, b=b)) <= 1.) == True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "def make_prob(linear_predictor):\n",
        "    \"\"\"Apply the sigmoid activation to the linear predictor\"\"\"\n",
        "    return sigmoid(linear_predictor)  # this is a JAX function\n",
        "```        \n",
        "---    \n",
        "</details>"
      ],
      "metadata": {
        "id": "JxiszxdDAHFd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MCEn-h03umS"
      },
      "source": [
        "**Exercise with solution** Assess the Bernoulli log pmf, make sure to use JAX differentiable code. Luckily for us JAX has a [`jax.scipy.sats`](https://jax.readthedocs.io/en/latest/jax.scipy.html#jax-scipy-stats) module, which contains many of our favourite distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_vjIxrp3wPj"
      },
      "outputs": [],
      "source": [
        "def bernoulli_logpmf(y, p):\n",
        "    \"\"\"\n",
        "    y: a batch of binary values with shape [batch_size]\n",
        "    p: a batch of probability values with shape [batch_size]\n",
        "\n",
        "    Return \\log Bernoulli(y[n]|p[n]) for each element y[n] in the batch.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert jnp.allclose(jnp.exp(bernoulli_logpmf(np.array([1, 1, 1]), np.array([0.9, 0.5, 0.1]))), np.array([0.9, 0.5, 0.1]), 1e-6)\n",
        "assert jnp.allclose(jnp.exp(bernoulli_logpmf(np.array([0, 0, 0]), np.array([0.9, 0.5, 0.1]))), np.array([0.1, 0.5, 0.9]), 1e-6)"
      ],
      "metadata": {
        "id": "FjsX6S6eYsGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "def bernoulli_logpmf(y, p):\n",
        "    \"\"\"\n",
        "    y: a batch of binary values with shape [batch_size]\n",
        "    p: a batch of probability values with shape [batch_size]\n",
        "\n",
        "    Return \\log Bernoulli(y[n]|p[n]) for each element y[n] in the batch.\n",
        "    \"\"\"\n",
        "    return jax.scipy.stats.bernoulli.logpmf(y, p=p)    \n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ],
      "metadata": {
        "id": "FA8MAGrRYax6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImk2GCgptF8"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Check this, if you'd like to draw samples from Bernoulli in JAX</b></summary>\n",
        "\n",
        "```python\n",
        "y_sample = random.bernoulli(key, p=make_prob(linear1(bin_dev_h, w=w, b=b)))\n",
        "```\n",
        "---    \n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viCOZ-_m4FOC"
      },
      "source": [
        "We can also check the performance of the most probable clas (for an untrained model):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MozIzzP34JDi"
      },
      "outputs": [],
      "source": [
        "def bernoulli_mode(p):\n",
        "    return jnp.where(p > 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert jnp.alltrue(bernoulli_mode(np.array([0.51, 0.1, 0.8])) == np.array([1, 0, 1]))"
      ],
      "metadata": {
        "id": "1-H7GsSwAyRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** Test the performance of the untrained Bernoulli GLM using the mode of the predicted Bernoulli distributions for the documents in the dev set.\n"
      ],
      "metadata": {
        "id": "7ixmfQp1oI-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "\n",
        "```python\n",
        "# **SOLUTION**\n",
        "y_best = bernoulli_mode(make_prob(linear1(so_dev_h, w=w, b=b)))\n",
        "print(classification_report(so_dev[:,1], label_as_string(y_best), zero_division=0))\n",
        "_ = ConfusionMatrixDisplay.from_predictions(so_dev[:,1], label_as_string(y_best))\n",
        "```\n",
        "\n",
        "---    \n",
        "</details>"
      ],
      "metadata": {
        "id": "VMAwyzdvBEO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** Use this untrained model to plot the Bernoulli pmf for the 16 first documents in the dev set.\n",
        "\n",
        "This model is not trained yet, so don't be surprised if the plots aren't interesting yet."
      ],
      "metadata": {
        "id": "qgQKb1UXaJuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "# **SOLUTION**\n",
        "\n",
        "import textwrap \n",
        "\n",
        "fig, axs = plt.subplots(4, 4, sharex=True, figsize=(16, 16))\n",
        "\n",
        "print(\"The plots show samples from the conditional model Y|X=x.\\nThe red vertical line shows the observation Y=y for X=x.\")\n",
        "\n",
        "for i in range(16):\n",
        "    x, y = so_dev[i, 0], so_dev[i, 1]\n",
        "    h = so_ff.transform([x]).toarray()\n",
        "    prob = make_prob(linear1(h, w=w, b=b)).item()\n",
        "\n",
        "    _ = axs[i//4,i%4].set_title(\"\\n\".join(textwrap.wrap(x, 30)))\n",
        "    _ = axs[i//4,i%4].plot(label_as_string([0, 1]), [1-prob, prob], 'x') \n",
        "    _ = axs[i//4,i%4].axvline(y, c='red')\n",
        "fig.tight_layout(h_pad=1, w_pad=1)    \n",
        "```\n",
        "\n",
        "---    \n",
        "</details>"
      ],
      "metadata": {
        "id": "ymqKpvVIcPs-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEw5kIsb40Pp"
      },
      "source": [
        "### Parameter estimation\n",
        "\n",
        "For a Bernoulli GLM, our likelihood function is:\n",
        "\\begin{equation}\n",
        "\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) = \\sum_{n=1}^N \\log \\mathrm{Bernoulli}(y^{(n)}|g(x^{(n)}; \\boldsymbol \\theta)) \n",
        "\\end{equation}\n",
        "\n",
        "And for stochastic gradient-based optimisation we will approximate it using mini batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj7LhAPfsAlY"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_regulariser(w, b):\n",
        "    \"\"\"Here we implement the L2 regulariser for you\"\"\"\n",
        "    return jax.numpy.linalg.norm(w.flatten(), 2) + jax.numpy.linalg.norm(b.flatten(), 2)"
      ],
      "metadata": {
        "id": "Zj61xunDBc2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert jnp.isclose(l2_regulariser(np.array([1., 2., 3.]), np.array(4.)), np.sqrt(1**2 + 2**2 + 3**2) + np.sqrt(4**2), 1e-3)"
      ],
      "metadata": {
        "id": "_0CiRH5YDPzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** Use [jax.scipy.stats](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.stats.bernoulli.logpmf.html#jax.scipy.stats.bernoulli.logpmf) functions to define a *loss function* for the Bernoulli GLM model. Our loss function is the **negative** of the L2-regularised log-likelihood function."
      ],
      "metadata": {
        "id": "xgCXpwbIEk_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPFEgSI0sQQb"
      },
      "outputs": [],
      "source": [
        "def bernoulli_loss(w, b, *, inputs, targets, l2weight=0.): \n",
        "    \"\"\"\n",
        "    w: weights of the linear model with shape [num_features]\n",
        "    b: bias of the linear model with shape []\n",
        "    inputs: h(x) with shape [batch_size, num_features]\n",
        "    targets: y with shape [batch_size]\n",
        "    l2weight: this is the lambda > 0 in the formula, the weight of the regulariser\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_inputs = np.array([[1., 0.], [0., 1.], [0., 0.]])\n",
        "test_case_targets = np.array([1, 0, 1])\n",
        "test_case_w = np.array([-1., 2.])\n",
        "test_case_b = np.array(1.)\n",
        "# the linear predictor is correct\n",
        "assert jnp.allclose(linear1(test_case_inputs, w=test_case_w, b=test_case_b), np.array([0., 3., 1]), 1e-3), \"Did you remember to compute dot-product with w and add the bias b?\"\n",
        "# the probability is correct\n",
        "assert jnp.allclose(make_prob(np.array([0., 3., 1.])), np.array([0.5, 0.952574, 0.73105]), 1e-3), \"Did you use the sigmoid function?\"\n",
        "# the regulariser is correct\n",
        "assert jnp.isclose(l2_regulariser(test_case_w, test_case_b), np.array(3.23606), 1e-3), \"Did you change the regulariser code?\"\n",
        "assert bernoulli_loss(test_case_w, test_case_b, inputs=test_case_inputs, targets=test_case_targets).shape == (), \"Did you remember to take the mean?\"\n",
        "# the (unregulariser) loss function is correct\n",
        "assert jnp.isclose(bernoulli_loss(test_case_w, test_case_b, inputs=test_case_inputs, targets=test_case_targets), - np.mean([np.log(0.5), np.log(1-0.952574), np.log(0.73105)]), 1e-3), \"Did you remember the minus?\"\n",
        "# the regulariseed loss function is correct\n",
        "assert jnp.isclose(bernoulli_loss(test_case_w, test_case_b, inputs=test_case_inputs, targets=test_case_targets, l2weight=1.0), - np.mean([np.log(0.5), np.log(1-0.952574), np.log(0.73105)]) + np.array(3.23606), 1e-3), \"Did you remember the minus?\""
      ],
      "metadata": {
        "id": "u43SmuW7FN8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```python\n",
        "def bernoulli_loss(w, b, *, inputs, targets, l2weight=0.): \n",
        "    \"\"\"\n",
        "    w: weights of the linear model with shape [num_features]\n",
        "    b: bias of the linear model with shape []\n",
        "    inputs: h(x) with shape [batch_size, num_features]\n",
        "    targets: y with shape [batch_size]\n",
        "    l2weight: this is the lambda > 0 in the formula, the weight of the regulariser\n",
        "    \"\"\"\n",
        "    s = linear1(inputs, w=w, b=b)\n",
        "    p = make_prob(s)\n",
        "    l2 = l2_regulariser(w=w, b=b)\n",
        "    return - jax.scipy.stats.bernoulli.logpmf(targets, p=p).mean(0) + l2weight * l2 \n",
        "```\n",
        "\n",
        "---    \n",
        "</details>"
      ],
      "metadata": {
        "id": "cvYQr5h4BXuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will pack the loss code for you in a way that can be used for automatic differentiation, and we will then compute the gradient with respect to $\\mathbf w$ and $b$:"
      ],
      "metadata": {
        "id": "sXMFDMW3JDEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPuU-6B1tyzM"
      },
      "outputs": [],
      "source": [
        "from functools import partial  # partial allows us to fix some of the arguments of a function\n",
        "\n",
        "loss_fn = partial(\n",
        "    bernoulli_loss, # we want to fix some of the arguments of bernoulli_loss\n",
        "    inputs=so_dev_h,   # namely, the part that concerns the observed data, both h(x)\n",
        "    targets=so_dev_y,  # and y\n",
        "    l2weight=0.     # we also want to fix the hyperparameters, \n",
        ") # this creates a loss function which is a function of w and b (exactly as we would like it to be)\n",
        "\n",
        "# Now we tell JAX to evaluate the loss_fn for the parameter values (w, b) that we currently have\n",
        "#  and compute partial derivatives for them\n",
        "loss, (grad_w, grad_b) = value_and_grad(loss_fn, (0, 1))(w, b)\n",
        "# the partial derivatives together form the gradient vector, which always has the same dimensionality as the parameter vector\n",
        "assert grad_w.shape == (num_features,)\n",
        "assert grad_b.shape == ()\n",
        "# the loss, of course, is a scalar \n",
        "assert loss.shape == ()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is it, you have all the ingredients needed to prescribe the model (its parameterisation in terms of $\\mathbf h(x)$ and $\\boldsymbol\\theta$, to use it (i.e., compute log probability for a given $(x, y)$ pair), and to train it (i.e., estimate its paramters using regularised MLE).\n",
        "\n",
        "We will wrap everything nicely for you into a single class, so that you can experiment with it."
      ],
      "metadata": {
        "id": "_R324aSUK1X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python/JAX GLM Class\n",
        "\n",
        "**Ungraded exercises** Study the class below, it implements a general purpose GLM for a conditional distribution with a 1-dimensional statistical parameter. This can be used, for example, for binary classification and for Poisson regression. This class lacks implementation for a couple of methods, but you should not attempt to implememtn it. Instead, those are implemented in classes that specialise this one. For example, study the BinaryClassifier class that we provide right after GLM1. That class, will complete the specfication of GLM1 by making it a Bernoulli conditional model."
      ],
      "metadata": {
        "id": "6RNZljvp63zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GLM1:\n",
        "    \"\"\"\n",
        "    This class contains all functionality that are shared across GLMs for single parameter distributions.\n",
        "    Subclasses of this class must only implement the parts that are specific to their choice of distribution.\n",
        "    For example, \n",
        "        GLM1 uses the mode for prediction, but how to compute the mode depends on which distribution we have.\n",
        "        GLM1 knows that the activation function is needed in order to constrain the linear predictor correctly, \n",
        "         but the choice of activation function will depend on the distribution we have.        \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_function, seed=0):\n",
        "        self.feature_function = feature_function        \n",
        "        self.num_features = len(feature_function.get_feature_names_out())\n",
        "        self.key = random.PRNGKey(seed)\n",
        "\n",
        "    def sparse_h(self, x):\n",
        "        \"\"\"\n",
        "        Map documents to scipy sparse features\n",
        "        x: a list of documents, each a string\n",
        "\n",
        "        Return scipy sparse matrix (from feature function).\n",
        "        \"\"\"\n",
        "        return self.feature_function.transform(x)\n",
        "\n",
        "    def feature_names(self):\n",
        "        return self.feature_function.get_feature_names_out()\n",
        "    \n",
        "    def random_parameters(self):\n",
        "        \"\"\"\n",
        "        Return randomly initialised weights and biases\n",
        "        \"\"\"\n",
        "        w = random.uniform(self.key, shape=(self.num_features,))\n",
        "        b = random.uniform(self.key) \n",
        "        return w, b\n",
        "\n",
        "    def l2(self, w, b):\n",
        "        \"\"\"Compute the L2 regulariser\"\"\"\n",
        "        return jax.numpy.linalg.norm(w.flatten(), 2) + jax.numpy.linalg.norm(b.flatten(), 2)\n",
        "\n",
        "    def linear(self, inputs, *, w, b):\n",
        "        \"\"\"\n",
        "        inputs: a collection of inputs [batch_size, num_features]\n",
        "        we normally program our models supporting the ability to perform the same operations\n",
        "        for multiple documents at once\n",
        "        we do so by \"batching\" documents together, so our first dimension is used \n",
        "        to iterate over different documents\n",
        "        w: parameters (vector of size num_features)\n",
        "        see that the parameters do not depend on batch size\n",
        "        that's because parameters are a property of the model and batch size isn't\n",
        "        b: parameters (single scalar)\n",
        "\n",
        "        Return x * w + b\n",
        "        \"\"\"    \n",
        "        # elementwise product\n",
        "        # [batch_size, num_features]\n",
        "        out = w * inputs  # by default this multiplication happens elementwise along the last dimension of the tensor\n",
        "        # reduce the feature dimension via sum \n",
        "        # [batch_size]\n",
        "        out = jnp.sum(out, axis=-1) \n",
        "        # add bias\n",
        "        out = out + b\n",
        "        # [batch_size]\n",
        "        return out\n",
        "\n",
        "    def activation(self, linear_predictor):\n",
        "        \"\"\"\n",
        "        Constrain the linear predictor\n",
        "\n",
        "        linear_predictor: w*h(x)+b with shape [batch_size]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Implement me in a subclass\")    \n",
        "\n",
        "    def g(self, inputs, *, w, b):\n",
        "        \"\"\"\n",
        "        Compute the statistical parameter of the conditional model.\n",
        "\n",
        "        inputs: h(x) with shape [batch_size, num_features]\n",
        "        w: weights with shape [num_features]\n",
        "        b: bias with shape []\n",
        "\n",
        "        Return g(x) with shape [batch_size]\n",
        "        \"\"\"\n",
        "        linear_predictor = self.linear(inputs, w=w, b=b)\n",
        "        parameter = self.activation(linear_predictor)\n",
        "        return parameter\n",
        "\n",
        "    def mode(self, g):\n",
        "        \"\"\"\n",
        "        Return the mode of the distribution\n",
        "\n",
        "        g: statistical parameter of the distribution with shape [batch_size]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Implement me in a subclass!\")    \n",
        "\n",
        "    def log_p(self, targets, g):\n",
        "        \"\"\"\n",
        "        Return log probability mass (or density) of targets given the statistical parameter.\n",
        "\n",
        "        targets: y with shape [batch_size]\n",
        "        g: statistical parameter of the distribution with shape [batch_size]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Implement me in a subclass!\")\n",
        "\n",
        "    def loss(self, w, b, *, inputs, targets, l2weight=0.):\n",
        "        \"\"\"\n",
        "        Compute the regularised negative log-likelihood of the model given observed (x, y) pairs.\n",
        "\n",
        "        w: weights with shape [num_features]\n",
        "        b: bias with shape []\n",
        "        inputs: h(x) with shape [batch_size, num_features]\n",
        "        targets: y with shape [batch_size]\n",
        "        l2weight: contribution of the L2 regulariser\n",
        "\n",
        "        Return - \\sum_{n=1}^N P(Y=y[n]|X=x[n]) + L2(w,b)\n",
        "        \"\"\"\n",
        "        return - self.log_p(targets, g=self.g(inputs, w=w, b=b)).mean(0) + l2weight * self.l2(w, b) \n",
        "\n",
        "    def predict(self, inputs, *, w, b):\n",
        "        \"\"\"\n",
        "        Predict y for h using the mode of the conditional distribution.\n",
        "        \n",
        "        inputs: h(x) with shape [batch_size, num_features]\n",
        "        w: weights with shape [num_features]\n",
        "        b: bias with shape []\n",
        "\n",
        "        Return \\argmax_y P(Y=y|X=x)\n",
        "        \"\"\"\n",
        "        return self.mode(self.g(inputs, w=w, b=b))\n",
        "\n",
        "    def validate(self, inputs, targets, *, w, b):\n",
        "        \"\"\"\n",
        "        Predict y for h and compare it to y_true using an appropriate metric.\n",
        "        \n",
        "        inputs: h(x) with shape [batch_size, num_features]\n",
        "        targets: true targets with shape [batch_size]\n",
        "        w: weights with shape [num_features]\n",
        "        b: bias with shape []\n",
        "\n",
        "        Return performance metric (e.g., f1 for classification, MAE for regression)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Implement me in a subclass!\")"
      ],
      "metadata": {
        "id": "yDC7M3Mf6WPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we specialise this class to the case of Binary classification with a Bernoulli conditional model."
      ],
      "metadata": {
        "id": "vthLviltXKk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "class BinaryClassifier(GLM1):\n",
        "\n",
        "    def __init__(self, feature_function, seed=0):\n",
        "        super().__init__(feature_function, seed=seed)\n",
        "        \n",
        "    def activation(self, linear_predictor):\n",
        "        \"\"\"For Binary classification we need to parameterise a Bernoulli, thus we constrain the linear predictor using sigmoid\"\"\"\n",
        "        return sigmoid(linear_predictor)\n",
        "\n",
        "    def mode(self, g):\n",
        "        \"\"\"The mode of the Bernoulli is the outcome that receives more than 0.5 mass\"\"\"        \n",
        "        return jnp.where(g > 0.5, 1, 0)\n",
        "\n",
        "    def log_p(self, targets, g):\n",
        "        \"\"\"\n",
        "        We use JAX scipy to return the log pmf of the Bernoulli\n",
        "        \"\"\"\n",
        "        return jax.scipy.stats.bernoulli.logpmf(targets, p=g)\n",
        "\n",
        "    def validate(self, inputs, targets, *, w, b):\n",
        "        \"\"\"For binary classification we report macro F1\"\"\"\n",
        "        y_pred = self.predict(inputs, w=w, b=b)\n",
        "        metrics = classification_report(targets, y_pred, output_dict=True, zero_division=0)\n",
        "        return metrics['macro avg']['f1-score']"
      ],
      "metadata": {
        "id": "vyeziVxg712L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise with solution** Here we share some code useful for training a model. Study the steps of the training algorithm."
      ],
      "metadata": {
        "id": "fZdZBjXtXQAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def get_batcher(data_size, batch_size, replace=False, rng=np.random.RandomState(1)):  \n",
        "    \"\"\"\n",
        "    Return an iterable for indices of data points organised as batches of a given size (the last batch is potentially shorter).\n",
        "    \"\"\"  \n",
        "    if rng is None:\n",
        "        permutation = np.arange(data_size)\n",
        "    else:\n",
        "        permutation = rng.permutation(data_size)\n",
        "    i = 0\n",
        "    while i < data_size:\n",
        "      yield permutation[i:i+batch_size]\n",
        "      i += batch_size        \n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model, training_h_sparse, training_y, dev_h_sparse, dev_y, \n",
        "    num_epochs=5, batch_size=500, validate_freq=10, \n",
        "    l2weight=1e-6, lr0=10., step_overwrite=0, log=None, \n",
        "    w=None, b=None, \n",
        "    rng=None):\n",
        "    \"\"\"\n",
        "    model: an instance of GLM1 specialised to a type of distribution\n",
        "    training_h_sparse: a scipy sparse matrix of features for the training data\n",
        "    training_y: a numpy array of targets for the training data\n",
        "    dev_h_sparse: a scipy sparse matrix of features for the validation data\n",
        "    dev_y: a numpy array of targets for the validation data\n",
        "    num_epochs: total number of passes over the entire training data\n",
        "    batch_size: how many instances are used for a single gradient step        \n",
        "    validate_freq: how often (in number of gradient steps) we compute performance of the validation set\n",
        "    l2weight: the contribution of the L2 regulariser\n",
        "    lr0: the initial learning rate    \n",
        "    step_overwrite: when you continue training, you can overwrite the step number (which affects the schedule of the learning rate)\n",
        "    log: when you continue training, you can reuse an existing log\n",
        "    w: use this to continue training (instead of training from scratch)\n",
        "    b: use this to continue training (instead of training from scratch)\n",
        "    rng: numpy random number generator, use it for reproducibility\n",
        "    \"\"\"\n",
        "    \n",
        "    if w is None or b is None:  # we start with some random parameters\n",
        "        w, b = model.random_parameters()\n",
        "    \n",
        "    dev_h = dev_h_sparse.toarray()\n",
        "    data_size = training_h_sparse.shape[0]\n",
        "    if log is None:\n",
        "        log = defaultdict(list)\n",
        "    \n",
        "    # It's good to always run the model before training, \n",
        "    # just to catch any problems and to log its current performance\n",
        "    log['val_metric'].append(model.validate(dev_h, dev_y, w=w, b=b))\n",
        "    log['val_loss'].append(model.loss(w, b, inputs=dev_h, targets=dev_y, l2weight=l2weight))    \n",
        "    \n",
        "    # and we will train for a certain number of steps\n",
        "    total_steps = num_epochs * len(list(get_batcher(data_size, batch_size)))\n",
        "    step = step_overwrite  # and sometimes we have already trained for a bit  \n",
        "\n",
        "    with tqdm(range(total_steps), desc='MLE') as bar:  # tqdm is a very cool progressbar :)\n",
        "        # we pass over the entire data a number of times\n",
        "        for epoch in range(num_epochs):            \n",
        "            # but do so in random order, and process one mini batch at a time\n",
        "            for ids in get_batcher(data_size, batch_size, rng=rng):\n",
        "\n",
        "                # our learning rate decays with time, this is something needed for the theory of stochastic optimisation to check out\n",
        "                lr = lr0/np.log10(10+step)  \n",
        "\n",
        "                # we prepare our loss function\n",
        "                # essentially, we fix the data it is based on (the inputs and targets in the mini batch)\n",
        "                # and fix its hyperparameters\n",
        "                # the only things that can vary are the parameters (w, b)\n",
        "                loss_fn = partial(\n",
        "                    model.loss, \n",
        "                    inputs=training_h_sparse[ids].toarray(),  # we need dense arrays but we better not store all dense arrays (else we run out of memory)\n",
        "                    targets=training_y[ids], \n",
        "                    l2weight=l2weight\n",
        "                )\n",
        "                # here we tell JAX to evaluate the loss function at the point (w, b)\n",
        "                # and also compute its partial derivatives at that point\n",
        "                loss_value, (grad_w, grad_b) = value_and_grad(loss_fn, (0, 1))(w, b)\n",
        "                \n",
        "                # because we have a *loss* we will subtract the gradient (scaled by the learning rate)\n",
        "                # from the current parameter values\n",
        "                w -= lr * grad_w\n",
        "                b -= lr * grad_b\n",
        "\n",
        "                # Here we log some information for analysis later on\n",
        "                log['loss'].append(loss_value.item())       \n",
        "                if step % validate_freq == 0:  \n",
        "                    log['val_metric'].append(model.validate(dev_h, dev_y, w=w, b=b))\n",
        "                    log['val_loss'].append(model.loss(w, b, inputs=dev_h, targets=dev_y, l2weight=l2weight))\n",
        "                bar.set_postfix({'epoch': epoch + 1, 'step': f\"{step:4d}\", 'lr': f\"{lr:.4f}\", 'loss': f\"{loss_value:.4f}\", 'val_loss': f\"{log['val_loss'][-1]:.4f}\", 'val_metric': log['val_metric'][-1]}) \n",
        "                bar.update()\n",
        "                step += 1\n",
        "    \n",
        "    return (w, b), log"
      ],
      "metadata": {
        "id": "I3HyQUha8SOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary classifier experiment\n",
        "\n",
        "From now on, we will use the following feature function. In NBC, we use counts, but counts are not a great idea for linear and generalised linear models. That's because raw counts vary with document length, which makes the magnitude of the dot product $\\mathbf w^\\top\\mathbf h(x)$ also vary a lot with document length. \n",
        "\n",
        "It's better to take frequency information into account, we wil be using a tf-idf transformation of the raw counts, sklearn can do that for us."
      ],
      "metadata": {
        "id": "kF_Hz9kvMaGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bin_ff = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer(ngram_range=(1,1), min_df=5)), # we will be discarding tokens less frequent than 5\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "    ]\n",
        ")\n",
        "bin_ff.fit(so_training[:, 0])\n",
        "\n",
        "bin_training_h_sparse = bin_ff.transform(so_training[:, 0])\n",
        "bin_training_y = label_as_int(so_training[:, 1])\n",
        "\n",
        "bin_dev_h_sparse = bin_ff.transform(so_dev[:, 0])\n",
        "bin_dev_y = label_as_int(so_dev[:, 1])\n",
        "\n",
        "bin_test_h_sparse = bin_ff.transform(so_test[:, 0])\n",
        "bin_test_y = label_as_int(so_test[:, 1])\n",
        "\n",
        "bin_cls = BinaryClassifier(bin_ff)"
      ],
      "metadata": {
        "id": "-TPx7ATXDAKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This should take less than 10 minutes (the progress bar should give you a reliable estimate)\n",
        "# for a faster run reduce num_epochs or increase batch_size \n",
        "# of course, do expect changes in performance\n",
        "# you can use lr0 and l2weight to affect the optimiser and the objective function\n",
        "(bin_w, bin_b), bin_log = train_model(\n",
        "    bin_cls,\n",
        "    bin_training_h_sparse,\n",
        "    bin_training_y,\n",
        "    bin_dev_h_sparse,\n",
        "    bin_dev_y,\n",
        "    lr0=5, # vary this to change the initial learning rate\n",
        "    l2weight=1e-4, # vary this to control regularisation\n",
        "    batch_size=100, # on GPU you can use larger batches\n",
        "    num_epochs=10, # use more to train for longer\n",
        "    rng=np.random.RandomState(42),\n",
        ")"
      ],
      "metadata": {
        "id": "4hpj_5WtZ46l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should always visualise the training loss, the validation loss, and some performance metric. \n",
        "\n",
        "The validation loss is the log-likelihood of the model given the validation data only (not the training data). If that curve starts going up, while the training loss curve is going down, we detect overfitting (a situation where the model is memorising patterns that are specific to the training data and are of no help to classify heldout data). \n",
        "\n",
        "The metric for classification helps us see how the most probable class decision rule performs for heldout data."
      ],
      "metadata": {
        "id": "_BbHzrLQoBM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhDe2ZQw1c8K"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
        "_ = ax[0].plot(np.arange(len(bin_log['loss'])), bin_log['loss'], '.')\n",
        "_ = ax[0].set_ylabel(\"Training loss\")\n",
        "_ = ax[0].set_xlabel(\"Steps\")\n",
        "_ = ax[1].plot(np.arange(len(bin_log['val_loss'])), bin_log['val_loss'], '.')\n",
        "_ = ax[1].set_ylabel(\"Validation loss\")\n",
        "_ = ax[1].set_xlabel(\"Validation steps\")\n",
        "_ = ax[2].plot(np.arange(len(bin_log['val_metric'])), bin_log['val_metric'], '.')\n",
        "_ = ax[2].set_ylabel(\"Validation macro F1\")\n",
        "_ = ax[2].set_xlabel(\"Validation steps\")\n",
        "fig.tight_layout(h_pad=1, w_pad=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should always visualise the magnitude of the weights. If they get too large we typically have numerical instabilities and/or overfitting."
      ],
      "metadata": {
        "id": "RRe8CwJKod19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.hist(bin_w, bins='auto')"
      ],
      "metadata": {
        "id": "m_j6gOH4PxIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can have a look at the bias term:"
      ],
      "metadata": {
        "id": "IEkIdYV0P_GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bin_b"
      ],
      "metadata": {
        "id": "VJtQLyO2P9Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also order the features by importance and inspect them"
      ],
      "metadata": {
        "id": "LiWIKFGoKWPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"20 features with large positive weight\")\n",
        "for f, w in sorted(zip(bin_cls.feature_names(), bin_w), key=lambda pair: pair[1], reverse=True)[:20]:\n",
        "    print(f, w)"
      ],
      "metadata": {
        "id": "i-WGlFjJKZx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex-bernoulli\"> **Graded Exercise - Subjectivity classifier** \n",
        "\n",
        "Here you will experiment with various Bernoulli GLMs on the subjectivity dataset using the feature function we provided you with. Here we recommend 100 epochs, more can be better, less can be okay too. Without GPUs, more than 100 epochs may be too slow.\n",
        "\n",
        "1. Train for 100 epochs with regularisation 1e-4. Plot the log information (training/validation loss and metric curves). Comment on when the classifier converged (when the curves got roughly flat), if at all. Make a good quality plot for full points.\n",
        "\n",
        "2. Train for 100 epochs with and without regularisation (use l2weight of 0 and of 1e-4). Plot the log information (training/validation loss and metric curves). Also plot a histogram with the flattened vector of weights (no need for bias), compare the histograms for the variant with and without regularisation. Discuss whether you see any risk for overfitting with this model (it's not always the case that overfitting happens).\n",
        "\n",
        "3. For your model with regularisation, assess the model on the test set (display a classification report and the confusion matrix. For that model, inspect some features that received large positive weights and large negative weights. Discuss whether the features you see there are plausibly related to subjectivity (large positive weight) and objectivity (large negative weight). \n",
        "\n",
        "\n",
        "Do not expect the performance to necessarily match the NBC. This dataset is simple enough that NBC is a *strong baseline*. Also, our feature function is not very complex, because we want to keep the notebook tutorial easy to use.\n"
      ],
      "metadata": {
        "id": "JVsM2vCYo9Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTRIBUTE YOUR OWN SOLUTION"
      ],
      "metadata": {
        "id": "cgfhVy_yOCj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional extra** When you are done with the graded part of the assignment, try changing the feature function, see if you can affect the result in a meaningful way (esp, part 3 of the exercise)."
      ],
      "metadata": {
        "id": "L5OebwLFsuV7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKFYo813xEYP"
      },
      "source": [
        "## Poisson regressor experiment\n",
        "\n",
        "Here you will develop a Poisson regressor. The Poisson regressor is useful when the target is a natural number (ordered and not easiliy bounded).\n",
        "\n",
        "We created an artificial task for you (normally, we would prefer to give you a real task, for example, age detection using the [Blog authorship corpus](https://www.kaggle.com/rtatman/blog-authorship-corpus) by [Schler et al (2006)](http://www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf)), but we want to keep the tutorial lightweight so you can focus on learning). In this task we stripped the sentences in the subjectivity dataset of blank spaces, but recorded the number of space-separated tokens before we did so. The artificial task for you is: read a space-free document and predict a distribution over the number of tokens that it must have had originally.\n",
        "\n",
        "For modelling device, use a GLM that parameterises a conditional Poisson model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = json.load(gzip.open(urllib.request.urlopen(\"https://surfdrive.surf.nl/files/index.php/s/le13WJnWjDkY7MV/download\")))\n",
        "num_tokens.keys()"
      ],
      "metadata": {
        "id": "w3Z6waAorRM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_num_tokens_corpus(corpus):\n",
        "    training_x, training_y = zip(*corpus['training'])\n",
        "    training_y = np.array(training_y, dtype=int)\n",
        "    dev_x, dev_y = zip(*corpus['dev'])\n",
        "    dev_y = np.array(dev_y, dtype=int)\n",
        "    test_x, test_y = zip(*corpus['test'])\n",
        "    test_y = np.array(test_y, dtype=int)\n",
        "    return training_x, training_y, dev_x, dev_y, test_x, test_y"
      ],
      "metadata": {
        "id": "Bb2VIgzSrX__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nt_training_x, nt_training_y, nt_dev_x, nt_dev_y, nt_test_x, nt_test_y = prepare_num_tokens_corpus(num_tokens)"
      ],
      "metadata": {
        "id": "qwAmngspry73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex-numtokens\"> **Graded Exercise - Number of Tokens Data and Baseline**  As always when we encounter a new dataset, let's explore it.\n",
        "\n",
        "1. For the training set, plot the length of $x$ in characters against its length in tokens $y$; you can use `plt.plot` or you can use something more interesting like [seaborn's jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html). We assess the quality of your plots (eg, titled, labelled, with legends, visually clear, etc).\n",
        "\n",
        "2. Inspect some examples, but don't invest too much time, this task should look trivial, albeit tedious, to a human.\n",
        "\n",
        "3. Obtain a baseline based on linear regression from the feature $\\mathrm{LengthInCharacters}(x)$ to the observed length in tokens $y$. For that use the training set. For linear regression you can use [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html). This linear model will be our baseline. For a new input $x$, this linear model will predict a continuous number as number of tokens, to obtain an integer use *round*. Evaluate your baseline on the dev set using `mean_absolute_error` (MAE) from sklearn. Also plot true $y$ against the predicted value for the documents in the dev set.\n",
        "\n",
        "\n",
        "The baseline MAE is between 2 and 3."
      ],
      "metadata": {
        "id": "TjXx6Ig7s2y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTRIBUTE YOUR OWN SOLUTION"
      ],
      "metadata": {
        "id": "ubwF6tcXsPNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Poisson regressor, we will be using the following feature function. It works at the character level and basically counts character ngrams from length 2 to 5, keeping only those reasonably frequent ones, and transforms counts using tf (but not idf)."
      ],
      "metadata": {
        "id": "g7KOcgSUt7Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nt_ff = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer(ngram_range=(2,5), min_df=10, analyzer='char')),\n",
        "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "nt_ff.fit(nt_training_x)"
      ],
      "metadata": {
        "id": "DzdV3jq4siDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nt_training_h_sparse = nt_ff.transform(nt_training_x)\n",
        "nt_training_h_sparse.shape"
      ],
      "metadata": {
        "id": "-TdQTtYOs2Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nt_dev_h_sparse = nt_ff.transform(nt_dev_x)\n",
        "nt_dev_h = nt_dev_h_sparse.toarray()\n",
        "nt_dev_h_sparse.shape, nt_dev_h.shape"
      ],
      "metadata": {
        "id": "sIsCCWTns4Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex-poisson\"> **Graded Exercise - Poisson regression**  You will now design a Poisson GLM. There are striking similarities with the Bernoulli GLM, and our class structure emphasises that, but be careful to implement a **Poisson** regressor here, and not just another Bernoulli model.\n",
        "\n",
        "1. Complete the PoissonRegressor class, it only lacks implementation for two methods, namely, `activation` and `log_p`. To help you get this right, we have coded a few test cases that your implementation should pass.\n",
        "\n",
        "2. Once you have it, train the model for 50 to 100 epochs (the more you can wait, the better) with regularisation 1e-4. Plot the information logged during training. \n",
        "\n",
        "3. For the regularised model, use the dev set to report the mean absolute error of your model against the baseline. Also plot the true length against the predicted length (as predicted by the baseline and as predicted by the Poisson model). There is a good chance that your baseline works better in terms of MAE, esp given our current feature function. Make some remarks about what you see (e.g., does your model work equally well on short and long documents?). \n",
        "\n",
        "4. Inspect the features that received higher positive weight and higher negative weight, see if you recognise sequences that look like word boundaries. Presumably, detecting word boundaries is useful for this task (the baseline cannot do that).\n",
        "\n",
        "5. Finally, use your model to predict Poisson distributions for the 16 first documents in the dev set. For each Poisson, plot a histogram of samples from it, and using a vertical line also plot the observed value for $y$. Remember that once you've obtained the Poisson parameter from your GLM, you can use `np.random.poisson` to obtain samples, for example.\n",
        "\n"
      ],
      "metadata": {
        "id": "jqY1IDJsuq9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "class PoissonRegressor(GLM1):\n",
        "\n",
        "    def __init__(self, feature_function, seed=0):\n",
        "        super().__init__(feature_function, seed=seed)\n",
        "        \n",
        "    def activation(self, linear_predictor):\n",
        "        \"\"\"In Poisson regression we constrain the linear predictor to being strictly positive, which can be done with exp or softplus, for example\"\"\"\n",
        "        raise NotImplementedError(\"Implement me!\")        \n",
        "\n",
        "    def mode(self, g):        \n",
        "        \"\"\"The Poisson mode is obtained via floor\"\"\"\n",
        "        return jnp.floor(g)\n",
        "\n",
        "    def log_p(self, targets, g):\n",
        "        \"\"\"\n",
        "        We use JAX scipy to return the log pmf of the Poisson\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Implement me!\")        \n",
        "\n",
        "    def validate(self, inputs, targets, *, w, b):\n",
        "        y_pred = self.predict(inputs, w=w, b=b)\n",
        "        return mean_absolute_error(targets, y_pred) "
      ],
      "metadata": {
        "id": "ra06HsHwXGpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poi_reg = PoissonRegressor(nt_ff)"
      ],
      "metadata": {
        "id": "HrCLTR-vsaXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert jnp.alltrue(poi_reg.activation(np.random.uniform(size=1000)) > 0.), \"The Poisson parameter should be *strictly* positive (larger than 0)\""
      ],
      "metadata": {
        "id": "5OQk08WbhMCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_targets = np.array([0, 1, 2, 3, 4])\n",
        "test_case_rates = np.array([1., 1., 2., 2., 3.])\n",
        "test_case_logpmf = np.array([-1.0000005, -1.       , -1.3068521, -1.7123183, -1.7836056])\n",
        "assert jnp.allclose(poi_reg.log_p(test_case_targets, test_case_rates), test_case_logpmf, 1e-3), \"Did you implement the correct expression for Poisson's log pmf? You can use JAX scipy code for that, rather than write it from scratch.\""
      ],
      "metadata": {
        "id": "09S8C-FxiLhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The progress bar gives you a good estimate, GPU makes it faster;\n",
        "# for a faster run reduce num_epochs or increase batch_size \n",
        "# of course, do expect changes in performance\n",
        "# you can use lr0 and l2weight to affect the optimiser and the objective function\n",
        "(poi_w, poi_b), poi_log = train_model(\n",
        "    poi_reg,\n",
        "    nt_training_h_sparse,\n",
        "    nt_training_y,\n",
        "    nt_dev_h_sparse,\n",
        "    nt_dev_y,\n",
        "    lr0=5.,\n",
        "    l2weight=1e-4,\n",
        "    batch_size=100, # use more for faster training (if you are on GPU, this can be bigger)\n",
        "    num_epochs=10, # use more to find better models\n",
        "    rng=np.random.RandomState(42),\n",
        ")"
      ],
      "metadata": {
        "id": "oOOmvy5isZmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTRIBUTION YOUR OWN SOLUTION"
      ],
      "metadata": {
        "id": "xCQeWW5oRria"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jv7NzmegyNTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional extra** When you are done with the graded part of the assignment, try changing the feature function, see if you can affect the result in a meaningful way."
      ],
      "metadata": {
        "id": "d8BaIw9ELE7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jCqAoL5rLHZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2022/T3_student.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}