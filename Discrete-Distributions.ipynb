{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/Discrete-Distributions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xTIbLPRpFj6b"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete probability distributions\n",
    "\n",
    "**Sample space**\n",
    "\n",
    "A sample space is a set of things we are interested in, we normally denote it by some capital Greek letter. For example, $\\Omega$. In a coin flip, we may observe it land heads up or tails up, so the sample space of a coin flip is $\\Omega = \\{\\text{H}, \\text{T} \\}$ (using H to mean heads and T to mean tails). The sample space of the experiment 'flip two coins' is $\\Omega = \\{ \\text{HH}, \\text{HT}, \\text{TH}, \\text{TT}\\}$.\n",
    "\n",
    "\n",
    "**Event**\n",
    "\n",
    "An event is a set of outcomes from a sample space. An event groups outcomes of a sample space, we say the event 'occurred' if any of its outcomes is observed. This is a convenient way to talk about properties of or observations about random experiments.For example, in a certain game of chance, when I flip two coins, perhaps all really care about is the number of 'heads'. Then we events like\n",
    "\n",
    "1. drawing no heads\n",
    "2. drawing 1 heads\n",
    "3. drawing 2 heads\n",
    "\n",
    "These can be described as sets of outcomes as follows:\n",
    "\n",
    "1. $\\{TT\\}$\n",
    "2. $\\{\\text{HT}, \\text{TH}\\}$\n",
    "3. $\\{\\text{HH}\\}$\n",
    "\n",
    "The space of all possible events is called the **event space**. There are different possibilities for event spaces, but normally we just assume all possible subsets of the sample space are possible events. All possible sets of a countable set is what we call the *powerset* of the set. The event space which is the powerset of the sample space is denoted $\\mathbb P(\\Omega)$ or also $2^\\Omega$.\n",
    "\n",
    "**Random variable**\n",
    "\n",
    "A random variable is an abstraction that allows us to give convenient mathematical treatment to the most diverse types of random experiments (involving the most diverse types of samples spaces). \n",
    "Formally, a random variable is a function that maps elements from a set known as the random variable's *range* to events in the event space of some random experiment. We often don't need to specify all this formal machinery. Instead, we talk about the random variables in terms of outcomes from its range and we pick a range that can capture the type of data we are interested in. \n",
    "\n",
    "For example, if we are interested in the number of heads in an experiment where we flip two coins, then a random variable with range $\\{0, 1, 2\\}$ is sufficient for us. It's not really necessary to think about a sample space made of pairs of coins like $\\Omega = \\{ \\text{HH}, \\text{HT}, \\text{TH}, \\text{TT}\\}$, and an event space made of all possible subsets of this set, it's enough to say I can observe various configurations in the world some of which have 0 heads, or 1 heads, or 2 heads, and my random variable captures precisely that. \n",
    "\n",
    "Here's some of the formal notation we will be using.\n",
    "\n",
    "If $X$ is a random variable with range $\\mathcal X$ and distribution $P_X$, then $P_X(X=x)$ is the probability that $X$ takes on the value $x \\in \\mathcal X$. The *range* of a random variable is the set of values it can take on. It is common to refer to the range as the random variable's sample space (though technically speaking this is a slight abuse of terminology). \n",
    "\n",
    "The notation $X=x$ is pronounced \"$X$ takes on the value $x$\". Formally, $X=x$ evaluates to an event. So, for example, in the experiment about number of heads, $X=1$ evaluates to the event $\\{\\text{HT}, \\text{TH}\\}$, which captures all possible outcomes consistent with $1$ heads being observed.\n",
    "\n",
    "Remarks:\n",
    "* $X$ is not any one particular value of the random outcome, $X$ is at best described by its probability distribution (about which you will be learn more in a moment). \n",
    "* $x \\in \\mathcal X$ is an outcome in the range of the random variable $X$, this means that $x$ is a value that $X$ may take on (if you were to observe a realisation of the random phenomenon or experiment that $X$ is meant to model). \n",
    "* $X=x$ is the notation we use to say that of all possible events that can happen, we concentrate only on the event that captures the outcomes of a random experiment that are consistent with $x$. \n",
    "\n",
    "The two cells below illustrate the difference between a standard variable and a random variable. A standard variable gets assigned a value and never changes, unless we reassign that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "v = 1\n",
    "print([v for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random variable gets assigned a distribution of values. Each time we observe a realisation of the random variable, we in effect observe the result of a random phenomenon/experiment, which can be any one of the values that are in the range of our random variable (in the example below this can be 0 or 1). The type of distribution we choose and its parameters (we will learn more about that in a moment) determine the frequency with which we expect to see the possible outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 2, 1, 1, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "observe_X = lambda : np.random.binomial(2, 0.5)\n",
    "print([observe_X() for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability distribution**\n",
    "\n",
    "For any probability distribution, $\\forall x \\in \\mathcal X$ it holds that  $0 \\le P_X(X=x) \\le 1$ and $\\sum_{x \\in \\mathcal X} P_X(X=x) = 1$. \n",
    "\n",
    "The notation $\\forall x \\in \\mathcal X$ is pronounced \"for all $x$ in the range of the random variable $X$\".\n",
    "\n",
    "The set $\\mathrm{supp}(P_X)$ is the *support* of the distribution, that is, the subset of $\\mathcal X$ for which $P_X(X=x)>0$.\n",
    "\n",
    "An example we just used above is the Binomial distribution (you will learn more about it later), which captures the number of times we obtain in a number of random repetitions of an experiment with binary outcomes $0$ or $1$. This distribution is controlled by two parameters, the number of repetitions, and the probability of obtaining $1$ whenever we draw an outcome (this probability is kept fixed throughout all repetitions). The support of the random variable is the set of integers from $0$ to the total number of repetitions. So, if we flip a coin 3 times, the support of the Binomial distribution is $\\{0, 1, 2, 3\\}$, any other value is assigned probability 0. Later you will see the complete Binomial law, including the probability mass function that specifies the probabilities values of the different outcomes in the support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQQEbCG5HXDl"
   },
   "source": [
    "**Cumulative distribution function**\n",
    "\n",
    "For a random variable $X$ with distribution $P_X$, the cumulative distribution function (cdf) is the function $F_X(a) = \\sum_{x \\le a} P_X(X=x)$. \n",
    "Because probability values are never negative, the cdf is increasing and monotone. \n",
    "\n",
    "The cdf plays an important role in sampling algorithms. In particular, if we draw $X$ from $P_X$ and evaluate the cdf, the distribution of the result is uniform over the interval $[0, 1]$. That is, $F_X(X) \\sim \\mathcal U(0, 1)$. Conversely, if $U$ is uniformly distributed over $[0,1]$, denoted $U\\sim \\mathcal U(0, 1)$, transforming $U$ through the inverse cdf $F^{-1}_X$, also known as *quantile function*, gives us a random variable $X$ with distribution $P_X$.\n",
    "\n",
    "**Probability mass function** \n",
    "\n",
    "We often specify a probability distribution via a parametric function that relates the mass of an outcome to the outcome itself and a set of numerical parameters. The probability mass function (pmf) is a useful device to specify probability distributions through general mathematical laws (functions), rather than one probability value at a time. We normally name the pmf for convenience. \n",
    "\n",
    "For example, $f_\\theta(x) = \\theta^x(1-\\theta)^{1-x}$ is the pmf of a binary random variable that takes the value 1 with probability $\\theta$, and the value 0 with probability $1-\\theta$. The probability distribution $P_X$ such that $P_X(X=x)=f_\\theta(x)$ also goes by the name of Bernoulli distribution. \n",
    "\n",
    "Standard probability distributions are usually \"named\", which helps us remember the pmfs that prescribe them. We will see some examples later.\n",
    "\n",
    "**Expected value**\n",
    "\n",
    "The expected value (or mean) of a random variable is denoted $\\mathbb E[X]$ and defined as $\\mathbb E[X] = \\sum_{x \\in \\mathrm{supp}(p_X)} x P_X(X=x)$. \n",
    "\n",
    "**Variance**\n",
    "\n",
    "The variance of a random variable is denoted $\\mathrm{Var}(X)$ and defined as $\\mathbb E[(X - \\mathbb E[X])^2] = \\mathbb E[X^2] - \\mathbb E[X]^2$.\n",
    "\n",
    "**Mode**\n",
    "\n",
    "The modes of the distribution $P_X$ of the random variable $X$ are the values $x \\in \\mathcal X$ for which $P_X(X=x)$ is maximum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM8gX1QyGAOA"
   },
   "source": [
    "## Bernoulli\n",
    "\n",
    "The [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) is the discrete probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $1-p$.\n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Bernoulli}(p)$, then $P_X(X=x)$ is given by the Bernoulli pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Bernoulli}(x|p) = \\begin{cases}\n",
    "  p & x=1\\\\\n",
    "  1-p & x=0\\\\\n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\{0, 1\\}$\n",
    "* Mean: $\\mathbb E[X] = p$\n",
    "* Variance: $\\mathrm{var}(X)= \\mathbb E[X]^2 - \\mathbb E[X^2]=p(1-p)$\n",
    "* Mode(s): $1$ if $p > 0.5$, $0$ if $p < 0.5$, $\\{0, 1\\}$ if $p=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viqE9UcoNqp_"
   },
   "source": [
    "Bernoulli pmf and cdf using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2S1fhVQGf5p"
   },
   "outputs": [],
   "source": [
    "bern = st.bernoulli(0.3)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot([0, 1], bern.pmf([0, 1]), 'o')\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot([0, 1], bern.cdf([0, 1]), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4mQgHS9Ntsq"
   },
   "source": [
    "Drawing samples from Bernoulli using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s83F71dBNlbk"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(bern.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Bern$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5oe9DBsNxsF"
   },
   "source": [
    "## Categorical\n",
    "\n",
    "The [Categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) is the discrete probability distribution of a random variable that can take on one of $K$ possible categories, with the probability of each category separately specified. \n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Categorical}(\\pi_1, \\ldots, \\pi_K)$, then $P_X(X=x)$ is given by the Categorical pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Categorical}(x|\\pi_{1}, \\ldots, \\pi_K) = \\begin{cases}\n",
    "  \\pi_x & x \\in \\{1, \\ldots, K\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "where $\\pi_k \\ge 0$ and $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "Other common notations: $X \\sim \\mathrm{Categorical}(\\pi_{1:K})$, $X \\sim \\mathrm{Categorical}(\\pi_1^K)$, and $X \\sim \\mathrm{Categorical}(\\boldsymbol\\pi)$ with $\\boldsymbol\\pi \\in \\Delta_{K-1}$. \n",
    "\n",
    "The set $\\Delta_{K-1} \\subset \\mathbb R^K$ is called the *probability simplex*, it is the set of $K$-dimensional vectors whose coordinates are positive and sum to 1.\n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\{1, \\ldots, K\\}$\n",
    "* Mean: undefined (Categorical outcomes are normally not interpreted as ordinal values)\n",
    "* Variance: undefined (Categorical outcomes are normally not interpreted as ordinal values)\n",
    "* Mode(s): $\\{k: \\pi_k = \\max(\\pi_1, \\ldots, \\pi_K)\\} \\subseteq \\{1, \\ldots, K\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lVozjSPTYXt"
   },
   "source": [
    "We will make our own Categorical object imitating the scipy API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pt_yTAuPlpZ"
   },
   "outputs": [],
   "source": [
    "class Categorical:\n",
    "    \n",
    "    def __init__(self, probs):\n",
    "        self._probs = np.array(probs)\n",
    "        assert self._probs.size > 1, \"We need 2 or more classes\"\n",
    "        assert self._probs.shape == (self._probs.size,), \"probs must be a vector\"\n",
    "        assert np.all(self._probs >= 0), \"The coordinates of the Categorical parameter must be positve\"\n",
    "        assert self._probs.sum() == 1, \"The coordinates of the Categorical parameter must add to 1\"\n",
    "        \n",
    "    def rvs(self, size=None):\n",
    "        \"\"\"Use this to draw 1 or more samples from the distribution\"\"\"\n",
    "        # we shift by one because random.choice returns 0-based outcomes\n",
    "        return np.random.choice(len(self._probs), p=self._probs, size=size) + 1\n",
    "\n",
    "    def pmf(self, x):        \n",
    "        \"\"\"Use this to assess the probability mass of the elements of a data vector\"\"\"\n",
    "        x = np.array(x, dtype=int) - 1 # convert to 0-based\n",
    "        return self._probs[x]\n",
    "    \n",
    "    def logpmf(self, x):\n",
    "        \"\"\"Use this to assess the logarithm of the probability mass of the elements of a data vector\"\"\"\n",
    "        x = np.array(x, dtype=int) - 1 # covert to 0-based\n",
    "        return np.log(self._probs[x])\n",
    "\n",
    "    def cdf(self, x):\n",
    "      if type(x) is int:\n",
    "        return self._probs[:x].sum()\n",
    "      else:\n",
    "        x = np.array(x, dtype=int) - 1. # convert to 0-based      \n",
    "        # flags\n",
    "        b = x[:, None] >= np.arange(self._probs.size)[None, :]\n",
    "        # cdf\n",
    "        return (b * self._probs).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKVe2erTNyql"
   },
   "outputs": [],
   "source": [
    "cat = Categorical([0.2, 0.1, 0.3, 0.4])\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot([1, 2, 3, 4], cat.pmf([1, 2, 3, 4]), 'o')\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot([1, 2, 3, 4], cat.cdf([1, 2, 3, 4]), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMjnuOqqP07A"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(cat.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$X \\sim Cat$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1nW8HC4Tede"
   },
   "source": [
    "## Binomial\n",
    "\n",
    "The [Binomial distribution](https://en.m.wikipedia.org/wiki/Binomial_distribution) with parameters $n$ and $p$ is the discrete probability distribution of the number of successes in a sequence of $n$ independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability $p$) or failure (with probability $1 − p$). \n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Binomial}(n, p)$, then $P_X(X=x)$ is given by the Binomial pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Binomial}(x|n, p) = \\begin{cases}\n",
    "  \\binom{n}{x} p^x(1-p)^{n-x}& x \\in \\{0, \\ldots, n\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    " where $n > 0$, $0 \\le p \\le 1$, and $\\binom{n}{x} = \\frac{n!}{x!(n-x)!}$, pronounced $n$-choose-$k$, is the binomial coefficient.\n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\{0, \\ldots, n\\}$\n",
    "* Mean: $\\mathbb E[X] = np$\n",
    "* Variance: $\\mathrm{var}(X)= \\mathbb E[X]^2 - \\mathbb E[X^2]=np(1-p)$\n",
    "* Mode(s): $\\lfloor (n+1)p \\rfloor$ or $\\lceil (n+1)p\\rceil -1 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81JcCcaNTgVw"
   },
   "outputs": [],
   "source": [
    "binom = st.binom(10, 0.3)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(11), binom.pmf(np.arange(11)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 11))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(11), binom.cdf(np.arange(11)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkqC3OxgVtPP"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(binom.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Binomial$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wuM8m5_Tgwk"
   },
   "source": [
    "## Geometric\n",
    "\n",
    "The [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) is either one of two discrete probability distributions:\n",
    "\n",
    "* The probability distribution of the number $X$ of Bernoulli trials needed to get one success, supported on the set $\\{1, 2, \\ldots\\}$. In this case $P_X(X=x)$ is given by the pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Geometric}_1(x|p) = \\begin{cases}\n",
    "  p(1-p)^{x-1} & x \\in \\{1,2, \\ldots\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "with $p > 0$.\n",
    "\n",
    "  * Support: $\\{1, 2, \\ldots \\}$\n",
    "  * Mean: $\\frac{1}{p}$\n",
    "  * Variance: $\\frac{1-p}{p^2}$\n",
    "  * Mode: 1\n",
    "\n",
    "* The probability distribution of the number $Y = X - 1$ of failures before the first success, supported on the set $\\{0, 1, \\ldots\\}$. In this case $P_Y(Y=y)$ is given by the pmf\n",
    "\\begin{equation}\n",
    "  \\mathrm{Geometric}_0(y|p = \\begin{cases}\n",
    "  p(1-p)^{y} & x \\in \\{0, 1, \\ldots\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation} \n",
    "with $p > 0$.\n",
    "\n",
    "  * Support: $\\{0, 1, \\ldots \\}$\n",
    "  * Mean: $\\frac{1-p}{p}$\n",
    "  * Variance: $\\frac{1-p}{p^2}$\n",
    "  * Mode: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzpxtMSig-q0"
   },
   "source": [
    "Scipy's default Geometric is Geometric1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLIs3UNHThWu"
   },
   "outputs": [],
   "source": [
    "geom1 = st.geom(0.6)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(20), geom1.pmf(np.arange(20)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(20), geom1.cdf(np.arange(20)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnWYrJ8YhK_p"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(geom1.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Geometric1$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGr6a_6VhB_2"
   },
   "source": [
    "But we can shift it using a so-called location parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ketvKrS0gwx2"
   },
   "outputs": [],
   "source": [
    "geom0 = st.geom(0.6, loc=-1)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(20), geom0.pmf(np.arange(20)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(20), geom0.cdf(np.arange(20)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkCHx1GQhOso"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(geom0.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Geometric0$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm5Dz5S9Thr_"
   },
   "source": [
    "## Poisson\n",
    "\n",
    "The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.\n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Poisson}(\\lambda)$, then $P_X(X=x)$ is given by the Poisson pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Poisson}(x|\\lambda) = \\begin{cases}\n",
    "  \\frac{\\lambda^xe^{-\\lambda}}{x!} & x \\in \\mathbb N_0\\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "where $\\lambda > 0$ is the *rate* parameter.\n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\mathbb N_0$\n",
    "* Mean: $\\mathbb E[X] = \\lambda$\n",
    "* Variance: $\\mathrm{var}(X)= \\mathbb E[X]^2 - \\mathbb E[X^2]= \\lambda$\n",
    "* Mode(s): $\\{ \\lceil \\lambda \\rceil - 1, \\lfloor \\lambda \\rfloor \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZBTC0R9Tics"
   },
   "outputs": [],
   "source": [
    "poi = st.poisson(9)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(20), poi.pmf(np.arange(20)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(20), poi.cdf(np.arange(20)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsdcXAiLiZLQ"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(poi.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Poisson$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR-UQmcITjFf"
   },
   "source": [
    "## Zipf and Zeta\n",
    "\n",
    "The [Zipf distribution](https://en.wikipedia.org/wiki/Zipf's_law) and the [Zeta distribution](https://en.wikipedia.org/wiki/Zeta_distribution)\n",
    "are closely-related distributions that relate the frequency of outcomes and their rank.\n",
    "\n",
    "\n",
    "Scipy's `zipf` distribution is in fact the Zeta distribution, and that's because the two names are often used interchangeably. Even though they are similar power laws they are not identical, the Zeta generalises the Zipf removing the need to specify the total population size.\n",
    "\n",
    "The **Zipf distribution** predicts the probability of the element with rank $x$ in a population of size $N$. If $X \\sim \\mathrm{Zipf}(N, s)$ with $N>1$ and power $s>1$, $P_X(X=x)$ is given by the Zipf pmf:\n",
    "\\begin{equation}\n",
    "\\mathrm{Zipf}(x|N, s) = \\frac{1}{k^s H_{N,s}}\n",
    "\\end{equation}\n",
    "where $H_{N,s}=\\sum_{n=1}^N \\frac{1}{n^s}$.\n",
    "\n",
    "* Support: $\\mathbb N_1$\n",
    "* Mean: $\\frac{H_{N,s-1}}{H_{N,s}}$\n",
    "* Variance: see Wikipedia\n",
    "* Mode: 1\n",
    "\n",
    "The **Zeta distribution** predicts the probability of an element of rank $x$. If $X \\sim \\mathrm{Zeta}(s)$ with $s>1$, $P_X(X=x)$ is given by the Zeta pmf:\n",
    "\\begin{equation}\n",
    "\\mathrm{Zeta}(x|s) = \\frac{1}{k^s \\zeta(s)}\n",
    "\\end{equation}\n",
    "where $\\zeta(s) = \\sum_{n=1}^\\infty \\frac{1}{n^s}$ is the [Riemann Zeta function](https://en.wikipedia.org/wiki/Riemann_zeta_function).\n",
    "\n",
    "* Support: $\\mathbb N_1$\n",
    "* Mean: $\\frac{\\zeta(s-1)}{\\zeta(s)}$\n",
    "* Variance: see Wikipedia\n",
    "* Mode: 1\n",
    "\n",
    "Power law distributions are [heavy-tailed distributions](https://en.wikipedia.org/wiki/Heavy-tailed_distribution) and because of that they usually produce very large outcomes that deviate drammatically from their modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3jR-0wkmOgl"
   },
   "source": [
    "The scipy `zipf` is in fact a Zeta distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN8ox4G3TjqB"
   },
   "outputs": [],
   "source": [
    "zeta = st.zipf(1.1)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(50), zeta.pmf(np.arange(50)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(50), zeta.cdf(np.arange(50)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TedFKqcUm19p"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(zeta.rvs(size=1000), bins=100)\n",
    "_ = plt.xlabel(r'$X \\sim Zeta$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqlCLXrSm7NP"
   },
   "source": [
    "The easiest way to recognise a Zipf/Zeta distribution (or a power law in general) is to plot a log-log plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHu-Sx9pmkZY"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(np.log(zeta.rvs(size=1000)), bins=100, log=True)\n",
    "_ = plt.xlabel(r'$\\log x$ for $X \\sim Zeta$')\n",
    "_ = plt.ylabel('Log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-P99v1aTlBB"
   },
   "source": [
    "## Multinomial\n",
    "\n",
    "The [Multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) is a generalization of the binomial distribution. It models the probability of counts for each side of a $K$-sided die rolled $n$ times. For $n$ independent trials each of which leads to a success for exactly one of $K$ categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n",
    "\n",
    "A multinomial random variable is a *vector-valued* random variable with $K$ dimensions, we denote it $X \\sim \\mathrm{Multinomial}(n, \\pi_{1:K})$ for $\\pi_{1:K} \\in \\Delta_{K-1}$. The probability $P_X(X=x)$ is given by \n",
    "\\begin{equation}\n",
    "\\mathrm{Multinomial}(x|n, \\pi_{1:K}) = \\frac{n!}{\\prod_{k=1}^K x_k!} \\prod_{k=1}^K \\pi_k^{x_k}\n",
    "\\end{equation}\n",
    "for $\\{x \\in \\mathbb N^K: \\sum_{k=1}^K x_k = N\\}$.\n",
    "\n",
    "* Support: $\\{x \\in \\mathbb N^K: \\sum_{k=1}^K x_k = n\\}$, that is, the subset of $K$ dimensional positive count vectors that add to $n$.\n",
    "* Mean: per coordinate $\\mathbb E[X_k] = \\pi_k$\n",
    "* Variance: per coordinate $n\\pi_k(1-\\pi_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dba-d5JoAaG"
   },
   "source": [
    "As this is a vector-valued rv, we cannot easily plot its pmf. But we can easily use scipy to obtain draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz5iEwiGTmD6"
   },
   "outputs": [],
   "source": [
    "multi = st.multinomial(10, [0.1, 0.2, 0.7])\n",
    "x = multi.rvs(size=5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My1t3f_YoN_m"
   },
   "source": [
    "To assess the pmf for a given sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHfQQR_iXjkq"
   },
   "outputs": [],
   "source": [
    "multi.pmf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9AHNek3XqNr"
   },
   "source": [
    "# MLE\n",
    "\n",
    "We are given a data set of $N$ observations $x^(1), \\ldots, x^{(N)}$, which we assume were drawn independently from a given distribution $P_X$ whose pmf has parameter $\\theta$, i.e., $P_X(X=x)=f_\\theta(x)$. \n",
    "\n",
    "The **likelihood function** assigns the value\n",
    "\\begin{equation}\n",
    "L_{\\mathcal D}(\\theta) = \\prod_{n=1}^N P_X(X=x) = \\prod_{n=1}^N f_\\theta(x) ~.\n",
    "\\end{equation}\n",
    "\n",
    "Frequentist point estimation tells us to search for the parameter value that maximises the likelihood function given a fixed dataset, or, equivalently, maximises the logarithm of the likelihood function:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{(\\text{MLE})} &= \\arg\\max_\\theta ~ \\mathcal L_{\\mathcal D}(\\theta) \\\\\n",
    "&= \\arg\\max_\\theta ~ \\log L_{\\mathcal D}(\\theta) \\\\\n",
    "&= \\arg\\max_\\theta ~ \\sum_{n=1}^N \\log f_\\theta(x)\n",
    "\\end{align}\n",
    "\n",
    "In some cases, the MLE solution can be obtained in closed-form by solving $\\nabla_\\theta \\mathcal L_{\\mathcal D}(\\theta) = \\mathbf 0$. In other cases we have to design numerical algorithms to approximate it.  \n",
    "\n",
    "Here we state (without proof) the MLE solutions for a few classic distributions.\n",
    "\n",
    "\n",
    "**Bernoulli.** For $X \\sim \\mathrm{Bernoulli}(p)$\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{equation}\n",
    "\n",
    "**Categorical.**  For $X \\sim \\mathrm{Categorical}(\\pi_{1:K})$ the MLE per coordinate is\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_k = \\frac{\\sum_{n=1}^N [x_n = k]}{N}\n",
    "\\end{equation}\n",
    "\n",
    "**Binomial.** For $X \\sim \\mathrm{Binomial}(N, p)$\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{equation}\n",
    "\n",
    "**Geometric.** For $X \\sim \\mathrm{Geometric}_1(p)$\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{N}{\\sum_{n=1}^N x_n} \n",
    "\\end{equation}\n",
    "\n",
    "**Poisson.** For $X \\sim \\mathrm{Poisson}(\\lambda)$\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{equation}\n",
    "\n",
    "**Zipf and Zeta.** No closed-form expression, but in T1 you develop a numerical approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp0hmVcXkC3"
   },
   "source": [
    "# Recognise\n",
    "\n",
    "Consider the following very simplistic model of words in English:\n",
    "\n",
    "* An English word has 3 slots, namely, the prefix, the root, and the suffix. Example: unlikely (un-, -like-, -ly).\n",
    "* Every English word has a root.\n",
    "* With probability $p$ the prefix is non-empty.\n",
    "* With probability $q$ the suffix is non-empty.\n",
    "* In this simplistic model, we decide on whether or not to fill in the prefix slot independently of what root we have and independently of whether or not the suffix is slot is non-empty. Similarly, the suffix slot is filled in independently of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu5WjIHebFUE"
   },
   "source": [
    "**Exercise with solution** Describe the probability distribution of the number $N$ of number parts (measured as the number of non-empty slots) of English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNod7EoFYXIE"
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "To be discussed in class.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXhd5oOTbOGt"
   },
   "source": [
    "**Exercise with solution** Assume $p=q$, prescribe the distribution of the number of parts $N$ using a standard distribution from the options in section 1. Make sure to state its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s61Td10HbePB"
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "To be discussed in class.\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV29Lc3_Xnbz"
   },
   "source": [
    "# Estimate\n",
    "\n",
    "Assume we obtain a dataset containing $M$ English words, each segmented into a triple (prefix, root, suffix) where we use \"\" to denote the empty string.\n",
    "\n",
    "Dataset: $\\mathcal D = \\{(A=a_m, B=b_m, C=c_m)_{m=1}^M\\}$, where $A$ is the prefix, $B$ is the root and $C$ is the suffix.\n",
    "\n",
    "Using $\\mathcal D$ and the simplistic model of English words (the general version where $p$ and $q$ are not necessarily the same), give an expression for the MLE solutions for $p$ and $q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90BDE1nvdajJ"
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "To be discussed in class.\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvkBV-sMXn-k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBLba60VH5BQhkA+fqQtEZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2022/HC1b-prep",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
