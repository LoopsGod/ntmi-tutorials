{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T6_student",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2VO/GuGXYWu1DMnkSq91Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide\n",
        "\n",
        "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
        "* Note that, as always, the notebook contains a condensed version of the theory We recommend you read the theory part before the LC session.\n"
      ],
      "metadata": {
        "id": "jzvWfS-ELNzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ILOs\n",
        "\n",
        "After completing this lab you should be able to\n",
        "\n",
        "* develop neural sequence labellers in PyTorch\n",
        "* estimate parameters via MLE\n",
        "* predict tag sequences for novel data\n",
        "* evaluate tagging performance"
      ],
      "metadata": {
        "id": "KqR7WUDXLeME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Notes\n",
        "\n",
        "* In this notebook you are expected to use $\\LaTeX$. \n",
        "* Use python3.\n",
        "* Use Torch\n",
        "* To have GPU support run this notebook on Google Colab (you will find more instructions later).\n",
        "\n",
        "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n",
        "\n",
        "If you need a short introduction to PyTorch [check this tutorial](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n"
      ],
      "metadata": {
        "id": "YBR2bPwLL9gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "* Data\n",
        "    * Vocabulary \n",
        "    * Corpus and Data Loader\n",
        "* Text encoders\n",
        "    * Word embeddings\n",
        "    * Concatenation\n",
        "    * Average pooling\n",
        "    * RNN encoder\n",
        "    * Bidirectional RNN encoder\n",
        "* Neural Tagger\n",
        "    * Independent C-way classification\n",
        "    * Markov tagger\n",
        "    * Autoregressive tagger\n",
        "* Evaluation"
      ],
      "metadata": {
        "id": "CqDZh0QJJsOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Graded Exercises\n",
        "\n",
        "**Important.** The grader may re-run your notebook to investigate its correctness, but you do upload your notebook with the cells already run and make sure that all your answers are visible without the need to re-run the notebook. \n",
        "\n",
        "The weight of the exercise is indicated below.\n",
        "\n",
        "* POS tagging data (20%)\n",
        "* Model size (20%)\n",
        "* Comparison (60%)"
      ],
      "metadata": {
        "id": "Hg54PiqdLoN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up"
      ],
      "metadata": {
        "id": "fVnfg0kMLrsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "LzjCfsJDNL1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "QV4oRuW-XYED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "In this tutorial we will develop models for sequence labelling. So our data for this tutorial will be collections of sentences, or *corpora*, annotated with token-level tags (e.g., part-of-speech tags or named-entity tags, etc). See [Section 8.2 and 8.3 of the textbook](https://web.stanford.edu/~jurafsky/slp3/8.pdf) to learn more about the data type. \n",
        "\n",
        "We will use corpora available in NLTK."
      ],
      "metadata": {
        "id": "B8qybX6RNDKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "KfbZPjfaZDdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank, brown\n",
        "\n",
        "corpus = treebank  # Use treebank (smaller, faster)"
      ],
      "metadata": {
        "id": "7ZuAZ5Il891R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method `tagged_sents` will give us a view of tokenized sentences with their token lag tag annotation:"
      ],
      "metadata": {
        "id": "cjtCV9z3FuQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = corpus.tagged_sents(tagset='universal')[0]\n",
        "example"
      ],
      "metadata": {
        "id": "HFQdJD079FY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "def tostring(seq_pair, vertical=True, headers=['Word', 'Tag']):\n",
        "    \"\"\"\n",
        "    A sequence of pairs, each pair is a token and a tag. Use vertical=True for tabulate.\n",
        "    Return a string representing the sequence of pairs.\n",
        "    \"\"\"\n",
        "    if vertical:\n",
        "        return tabulate(list(seq_pair), headers=headers)\n",
        "    else:\n",
        "        return ' '.join(f'{w}/{t}' for w, t in seq_pair)\n",
        "\n",
        "def tostring2(tok_seq, tag_seq, vertical=True, headers=['Word', 'Tag']):\n",
        "    \"\"\"\n",
        "    A sequence of pairs, each pair is a token and a tag. Use vertical=True for tabulate.\n",
        "    Return a string representing the sequence of pairs.\n",
        "    \"\"\"\n",
        "    return tostring(zip(tok_seq, tag_seq), vertical=vertical, headers=headers)"
      ],
      "metadata": {
        "id": "lhejnWoH_BlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring(example, vertical=False))"
      ],
      "metadata": {
        "id": "N7NtsPiIF4Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring(example, vertical=True))"
      ],
      "metadata": {
        "id": "WGl7q-_AG0WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_nltk_corpus(corpus, max_length=30, num_heldout=100):\n",
        "    \"\"\"\n",
        "    Shuffle and split a corpus.\n",
        "    corpus: a corpus of tagged sequences, each sequence is a pair, each pair is a token and a tag.\n",
        "    max_length: discard sentences longer than this\n",
        "\n",
        "    Return: \n",
        "        (training word sequences, training tag sequences), \n",
        "        (dev word sequences, dev tag sequences), \n",
        "        (test word sequences, test tag sequences),         \n",
        "    \"\"\"\n",
        "    tagged_sentences = corpus.tagged_sents(tagset='universal')\n",
        "    # do not change the seed in here    \n",
        "    order = np.random.RandomState(42).permutation(np.arange(len(tagged_sentences)))    \n",
        "    word_sequences = [[w.lower() for w, t in tagged_sentences[i]] for i in order if len(tagged_sentences[i]) <= max_length]    \n",
        "    tag_sequences = [[t for w, t in tagged_sentences[i]] for i in order if len(tagged_sentences[i]) <= max_length]    \n",
        "    return (word_sequences[2*num_heldout:], tag_sequences[2*num_heldout:]), (word_sequences[num_heldout:2*num_heldout], tag_sequences[num_heldout:2*num_heldout]), (word_sequences[:num_heldout], tag_sequences[:num_heldout])"
      ],
      "metadata": {
        "id": "BNqS36sZNExH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `brown` this will take about 1 minute. For `treebank` this will take about 10 seconds."
      ],
      "metadata": {
        "id": "IHYqnVljTaw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "(training_x, training_y), (dev_x, dev_y), (test_x, test_y) = split_nltk_corpus(corpus, num_heldout=100 if corpus is treebank else 1000)\n",
        "print(f\"Number of sentences: training={len(training_x)} dev={len(dev_x)} test={len(test_x)}\")"
      ],
      "metadata": {
        "id": "7-pkD_jNOHBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"# A few training sentences\\n\\n\")\n",
        "for n in range(3):    \n",
        "    print(tostring2(training_x[n], training_y[n]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "Fk5nks5bKG0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graded exercises - POS tagging data\n",
        "\n",
        "* Plot the frequency of the tags in the dataset\n",
        "* For each possible tag, plot the conditional frequency of other tags\n",
        "* List the most frequent nouns, verbs, adjectives, and adverbs. Use top-10. \n",
        "* Find examples of words that show ambiguity of POS.\n"
      ],
      "metadata": {
        "id": "QKMDo-B-SOTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contribute your solution"
      ],
      "metadata": {
        "id": "N1UzfbrVt12C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary\n",
        "\n",
        "As always when dealing with NLP models, we need an object to maintain our vocabulary of known tokens. This time we will rely on word-tokenization rather than BPE tokenization. Fortunately, NLK corpora are already tokenized.\n",
        "\n",
        "Our vocabulary class will maintain the set of known tokens, and a dictionary to convert tokens to codes and codes back to tokens. The class will also take care of some special symbols (e.g., BOS, EOS, UNK, PAD). \n",
        "\n",
        "Finally, if later on you test your model on sentences that are not word tokenized, you can use `nlt.tokenize.word_tokenize` or any other tokenizer you like (as long as the level of tokenization is similar to the one you used for training your model."
      ],
      "metadata": {
        "id": "3qmFucRGr1Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "EcuhA0GEr8W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how you can tokenize English sentences (but remember that we don't need to redo this for the training/dev/test data from NLKT):"
      ],
      "metadata": {
        "id": "gUUct5byIkKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"This is a sentence, and this is another.\")"
      ],
      "metadata": {
        "id": "kT2gOTZtr_9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will adapt one of the classes we developed in previous tutorials, and this class will be used for maintaining both the vocabulary of known tokens and the set of known tags."
      ],
      "metadata": {
        "id": "Kt5gPbIDIrx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "class Vocab:\n",
        "\n",
        "    def __init__(self, corpus: list, min_freq=1):        \n",
        "        \"\"\"\n",
        "        corpus: list of documents, each document is a list of tokens, each token is a string\n",
        "        min_freq: words that occur less than this value are discarded\n",
        "        \"\"\"\n",
        "        # Make the vocabulary of known words\n",
        "\n",
        "        # Count word occurrences\n",
        "        counter = Counter(chain(*corpus))\n",
        "        # sort them by frequency\n",
        "        sorted_by_freq_tuples = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
        "        \n",
        "        # Special tokens\n",
        "        self.pad_token = \"-PAD-\"        \n",
        "        self.bos_token = \"-BOS-\"\n",
        "        self.eos_token = \"-EOS-\"\n",
        "        self.unk_token = \"-UNK-\"\n",
        "        self.pad_id = 0\n",
        "        self.bos_id = 1\n",
        "        self.eos_id = 2\n",
        "        self.unk_id = 3\n",
        "\n",
        "        self.known_symbols = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n",
        "        self.counts = [0, 0]\n",
        "        \n",
        "        # Vocabulary\n",
        "        self.word2id = OrderedDict()                \n",
        "        self.word2id[self.pad_token] = self.pad_id        \n",
        "        self.word2id[self.bos_token] = self.bos_id\n",
        "        self.word2id[self.eos_token] = self.eos_id\n",
        "        self.word2id[self.unk_token] = self.unk_id\n",
        "        self.min_freq = min_freq\n",
        "        for w, n in sorted_by_freq_tuples: \n",
        "            if n >= min_freq: # discard infrequent words\n",
        "                self.word2id[w] = len(self.known_symbols)\n",
        "                self.known_symbols.append(w)\n",
        "                self.counts.append(n)\n",
        "        \n",
        "        # store the counts for later\n",
        "        self.counts = np.array(self.counts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.known_symbols)\n",
        "\n",
        "    def __getitem__(self, word): # return the id (int) of a word (str)\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def encode(self, doc: list, add_bos=False, add_eos=False, pad_right=0):\n",
        "        \"\"\"\n",
        "        Transform a document into a numpy array of integer token identifiers.\n",
        "        doc: list of tokens, each token is a string\n",
        "        add_bos: whether to add the BOS token\n",
        "        add_eos: whether to add the EOS token\n",
        "        pad_right: number of suffix padding tokens \n",
        "        \n",
        "        Return: a list of codes (possibly with BOS and EOS added as well as padding)\n",
        "        \"\"\"\n",
        "        return [self.word2id.get(w, self.unk_id) for w in chain([self.bos_token] * int(add_bos), doc, [self.eos_token] * int(add_eos), [self.pad_token] * pad_right)]\n",
        "\n",
        "    def batch_encode(self, docs: list, add_bos=False, add_eos=False):\n",
        "        \"\"\"\n",
        "        Transform a batch of documents into a numpy array of integer token identifiers.\n",
        "        This will pad the shorter documents to the length of the longest document.\n",
        "        docs: a list of documents\n",
        "        add_bos: whether to add the BOS token\n",
        "        add_eos: whether to add the EOS token\n",
        "        pad_right: number of suffix padding tokens\n",
        "\n",
        "        Return: numpy array with shape [len(docs), longest_doc + add_bos + add_eos]\n",
        "        \"\"\"\n",
        "        max_len = max(len(doc) for doc in docs)\n",
        "        return np.array([self.encode(doc, add_bos=add_bos, add_eos=add_eos, pad_right=max_len-len(doc)) for doc in docs])\n",
        "\n",
        "    def decode(self, ids, strip_pad=False):\n",
        "        \"\"\"\n",
        "        Transform a np.array document into a list of tokens.\n",
        "        ids: np.array with shape [num_tokens] \n",
        "        strip_pad: whether PAD tokens should be deleted from the output\n",
        "\n",
        "        Return: list of strings with size [num_tokens - num_padding]\n",
        "        \"\"\"\n",
        "        if strip_pad:\n",
        "            return [self.known_symbols[id] for id in ids if id != self.pad_id]\n",
        "        else:\n",
        "            return [self.known_symbols[id] for id in ids]\n",
        "\n",
        "    def batch_decode(self, docs, strip_pad=False):\n",
        "        \"\"\"\n",
        "        Transform a np.array collection of documents into a collection of lists of tokens.\n",
        "        ids: np.array with shape [num_docs, max_length] \n",
        "        strip_pad: whether PAD tokens should be deleted from the output\n",
        "\n",
        "        Return: list of documents, each a list of tokens, each token a string\n",
        "        \"\"\"\n",
        "        return [self.decode(doc, strip_pad=strip_pad) for doc in docs]    "
      ],
      "metadata": {
        "id": "yMrE8lvlr2aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how this works:"
      ],
      "metadata": {
        "id": "HxTwRreJI7DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we get a vocabulary for words\n",
        "word_vocab = Vocab(training_x, min_freq=2)\n",
        "# and a vocabulary for tags\n",
        "tag_vocab = Vocab(training_y, min_freq=1)\n",
        "# you can see their sizes V and C:\n",
        "len(word_vocab), len(tag_vocab)"
      ],
      "metadata": {
        "id": "ZVR6Nfh5skV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `encode` method turns a sequence of (str) symbols into a sequence of (int) codes:"
      ],
      "metadata": {
        "id": "AM64EcW2JEoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring2(word_vocab.encode(training_x[0]), tag_vocab.encode(training_y[0]), vertical=True))"
      ],
      "metadata": {
        "id": "KY5XFhxZtTyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also have `encode` add some special symbols for us (but remember to be consistent, you should always have token sequences and tag sequences that match in length):"
      ],
      "metadata": {
        "id": "N4U7tdm-JwAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring2(word_vocab.encode(training_x[0], add_eos=True), tag_vocab.encode(training_y[0], add_eos=True)))"
      ],
      "metadata": {
        "id": "tMkuE8O6JxWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here for example, we will add BOS, EOS, and we are going to encode and decode:"
      ],
      "metadata": {
        "id": "LnoD6FZQKIM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring2(word_vocab.decode(word_vocab.encode(training_x[0], add_bos=True, add_eos=True)), tag_vocab.decode(tag_vocab.encode(training_y[0], add_bos=True, add_eos=True))))"
      ],
      "metadata": {
        "id": "dL88zQGjs6zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also encode and decode entire batches of sequences. This will use pad symbols/codes to make the sequences in the same batch have the same length:"
      ],
      "metadata": {
        "id": "dcGLLsUsKTrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vocab.batch_encode(training_x[:2], add_bos=False, add_eos=True)"
      ],
      "metadata": {
        "id": "rOAVwaWX6kA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vocab.batch_decode(word_vocab.batch_encode(training_x[:2], add_bos=False, add_eos=True), strip_pad=True)"
      ],
      "metadata": {
        "id": "_ScoTY8-6u0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus and Data Loader\n",
        "\n",
        "We will be developing our models in torch, thus we need to wrap our corpus into a `Dataset` and a `DataLoader`:"
      ],
      "metadata": {
        "id": "06BMGjPT7BFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TaggedCorpus(Dataset):\n",
        "    \"\"\"\n",
        "    Use this to give torch access to a corpus of tagged sequences.\n",
        "    This class will also know the vocab objects for tokens and tags, \n",
        "    and it will take care of coding strings into integers consistently.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus_x, corpus_y, word_vocab: Vocab, tag_vocab: Vocab):\n",
        "        \"\"\"\n",
        "        In PyTorch we better always manipulate numerical codes, rather than text.\n",
        "        So, our Corpus object will contain a vocab that converts words to codes.\n",
        "\n",
        "        corpus_x: token sequences\n",
        "        corpus_y: tag sequences\n",
        "        word_vocab: vocabulary for token sequences\n",
        "        tag_vocab: vocabulary for tag sequences\n",
        "        \"\"\"\n",
        "        self.corpus_x = list(corpus_x)\n",
        "        self.corpus_y = list(corpus_y)\n",
        "        assert len(self.corpus_x) == len(self.corpus_y), \"I need sequence pairs\"\n",
        "        assert all(len(x) == len(y) for x, y in zip(corpus_x, corpus_y)), \"A sequence pair should match in number of steps\"\n",
        "        self.word_vocab = word_vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Size of the corpus in number of sequence pairs\"\"\"\n",
        "        return len(self.corpus_x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return corpus_x[idx] and corpus_y[idx] converted to codes and with the EOS code in the end\"\"\"\n",
        "        x = self.word_vocab.encode(self.corpus_x[idx], add_bos=False, add_eos=True)\n",
        "        y = self.tag_vocab.encode(self.corpus_y[idx], add_bos=False, add_eos=True)\n",
        "        return x, y\n",
        "\n",
        "    @classmethod\n",
        "    def pad_to_longest(cls, pairs, pad_id=0):\n",
        "        \"\"\"\n",
        "        Take a list of coded sequences and returns a torch tensor where \n",
        "        every sentence has the same length (by means of using PAD tokens)\n",
        "        \"\"\"\n",
        "        longest = max(len(x) for x, y in pairs)\n",
        "        batch_x = torch.tensor([x + [pad_id] * (longest - len(x)) for x, y in pairs]) \n",
        "        batch_y = torch.tensor([y + [pad_id] * (longest - len(y)) for x, y in pairs]) \n",
        "        return batch_x, batch_y"
      ],
      "metadata": {
        "id": "3ptZKyBw7FN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we join the token sequences and tag sequences from NLTK into `Dataset` objects for training, development and testing. Note that they share the same vocabularies which were constructed using the training set alone."
      ],
      "metadata": {
        "id": "_jqOgaYwLPhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = TaggedCorpus(training_x, training_y, word_vocab, tag_vocab)\n",
        "dev = TaggedCorpus(dev_x, dev_y, word_vocab, tag_vocab)\n",
        "test = TaggedCorpus(test_x, test_y, word_vocab, tag_vocab)"
      ],
      "metadata": {
        "id": "rVrZjx0R8gYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of how we get a `DataLoader` for a corpus, we simply choose the `Dataset` object we want (training/dev/test), the batch size we want, whether we need shuffling (e.g., for training batches in SGD), and how we \"glue\" data points of different length together (i.e., a function such as `pad_to_longest` which `TaggedCorpus` provides for us)."
      ],
      "metadata": {
        "id": "I-ebPbVxLha8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batcher = DataLoader(training, batch_size=3, shuffle=True, collate_fn=TaggedCorpus.pad_to_longest)\n",
        "for batch_x, batch_y in batcher:\n",
        "    print(\"# This is how the sequence pairs in a batch come out of the data loader\\n\")\n",
        "\n",
        "    for x, y in zip(batch_x, batch_y):\n",
        "        print(tostring2(x, y))\n",
        "        print()\n",
        "    \n",
        "    print(\"# And we can always decode them for inspection\\n\")\n",
        "    # stripping padding makes it easier to read the examples\n",
        "    for x, y in zip(word_vocab.batch_decode(batch_x, strip_pad=True), tag_vocab.batch_decode(batch_y, strip_pad=True)):\n",
        "        print(tostring2(x, y))\n",
        "        print()\n",
        "    break"
      ],
      "metadata": {
        "id": "NhVrqdcI827_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text encoders\n",
        "\n",
        "In NLP applications, we often have to *encode* a piece of text. For example, that is the case in text classification as well as in sequence labelling. \n",
        "\n",
        "Assume for example, a text classifier that takes a document $x_{1:l} = \\langle x_1, \\ldots, x_l \\rangle$, where each token $x_i \\in \\mathcal W$ is from a finite vocabulary of $V$ tokens and predicts a distribution over $C$ classes from a set $\\mathcal T = \\{1, \\ldots, C\\}$."
      ],
      "metadata": {
        "id": "kjldtg5dJW7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings\n",
        "\n",
        "The very first thing we learnt about text encoding is that we need to map tokens (discrete symbols in a set of size $V$) to points in geometrical space. The embedding layer is a simple architecture for that, \n",
        "\n",
        "\\begin{align}\n",
        "    \\mathbf e_i &= \\mathrm{embed}_D(x_i; \\theta_{\\text{in}})\n",
        "\\end{align}\n",
        "\n",
        "It realises a simple lookup operation retrieving from a matrix of $V \\times D$ parameters the $D$-dimensional vector that corresponds to the $i$th token of the sequence."
      ],
      "metadata": {
        "id": "kT8JUS4nKSpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "mjCjryC8-X9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_parameters(torch_module):\n",
        "    \"\"\"A helper to count the number of parameters in a torch module\"\"\"\n",
        "    return sum(np.prod(theta.shape) for theta in torch_module.parameters())"
      ],
      "metadata": {
        "id": "NHJOVtd-NX9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_batch = torch.tensor(\n",
        "    [\n",
        "     [3, 4, 5, 2],  # assume that every sentence ends in the EOS token, and assume the code for it is 2\n",
        "     [5, 7, 2, 0]   # assume that the code for the PAD token is 0\n",
        "    ]\n",
        ")\n",
        "toy_batch"
      ],
      "metadata": {
        "id": "qj47hI4aRiA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this creates the layer with untrained parameters\n",
        "toy_emb_dim = 8\n",
        "toy_vocab_size = 10\n",
        "toy_emb = nn.Embedding(num_embeddings=toy_vocab_size, embedding_dim=toy_emb_dim)\n",
        "toy_emb"
      ],
      "metadata": {
        "id": "1PU-SYeT-bim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert num_parameters(toy_emb) == toy_vocab_size * toy_emb_dim, \"Embedding layers are built upon [V, D] matrices\""
      ],
      "metadata": {
        "id": "jkBxcGQFNLo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this embeds the tokens in the sequences in the batch\n",
        "e = toy_emb(toy_batch)\n",
        "print(e.shape)"
      ],
      "metadata": {
        "id": "O93Nc_pG-qKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert toy_emb(toy_batch).shape == toy_batch.shape + (toy_emb_dim,)"
      ],
      "metadata": {
        "id": "cv9qwmy5-0gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenation\n",
        "\n",
        "We also learnt that we can concatenate embeddings making a representation for short phrases. For example, \n",
        "\n",
        "\\begin{align}\n",
        "    \\mathbf u_i &= \\mathrm{concat}(\\mathbf e_{i-1}, \\mathbf e_i)\n",
        "\\end{align}\n",
        "\n",
        "makes a representation for the bigram $\\langle x_{i-1}, x_i \\rangle$.\n",
        "\n",
        "This type of encoding mechanism works whenever the phrase we are trying to encode has a fixed length (e.g., always 2 words, or always 3 words). But it does not work for entire documents, that's because documents vary in length, so depending on the document the encoding would have a different dimensionality."
      ],
      "metadata": {
        "id": "VMgtKxTPKxBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are different ways to achieve concatenation \n",
        "# [batch_size, max_len, emb_dim]\n",
        "e = toy_emb(toy_batch)\n",
        "# Let's get the embeddings of the first two tokens\n",
        "e0 = e[:,0]\n",
        "e1 = e[:,1]\n",
        "e0.shape, e1.shape"
      ],
      "metadata": {
        "id": "1ffpI-cQ-5IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and concatenate them\n",
        "\n",
        "print(torch.cat([e0, e1], -1).shape)\n",
        "\n",
        "assert torch.cat([e0, e1], -1).shape == (2, 2*toy_emb_dim)"
      ],
      "metadata": {
        "id": "PfbxIOVcAHsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to concatenate the vectors for all words in the sequence\n",
        "# you could just reshape the tensor as follows\n",
        "# [batch_size, max_len * emb_dim]\n",
        "u = e.reshape((toy_batch.shape[0], -1))\n",
        "print(u.shape)\n",
        "\n",
        "assert u.shape == (toy_batch.shape[0], toy_batch.shape[1] * toy_emb_dim), \"Did you change the embed_dim?\""
      ],
      "metadata": {
        "id": "GvPrCImY_U3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average pooling \n",
        "\n",
        "Whenever we need to combine a variable number of $D$-dimensional vectors into a single $D$-dimensional vector, we can use an elementwise average. This is known as *average pooling*:\n",
        "\n",
        "\\begin{align}\n",
        "    \\mathbf u &= \\frac{1}{l} \\sum_{i=1}^l \\mathbf e_i\n",
        "\\end{align}\n",
        "\n",
        "This average pooling operation takes a number $l > 0$ of $D$-dimensional inputs and returns a single $D$-dimensional output. While it achieves the task of representing the entire document $x_{1:l}$ it has serious limitations. For example, it discards any information in the order of the document."
      ],
      "metadata": {
        "id": "9TCR1DkBLOBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [batch_size, max_len, emb_dim]\n",
        "e = toy_emb(toy_batch)\n",
        "# here we replace padding embeddings by 0, so they do not affect the average\n",
        "# unsqueeze(-1) is needed because e has an extra axis compared to toy_batch\n",
        "masked_e = torch.where((toy_batch > 0).unsqueeze(-1), e, torch.zeros_like(e))\n",
        "# we sum the embeddings of the valid positions (those that are not PADs)\n",
        "# and divide by sequence length (as we always have an EOS token, length is never 0)\n",
        "avg = torch.sum(masked_e, 1) / torch.sum((toy_batch > 0).float(), -1, keepdims=True)\n",
        "print(avg.shape)\n",
        "\n",
        "# Note how the 'step' or 'time' dimension is gone\n",
        "assert avg.shape == (toy_batch.shape[0], toy_emb_dim)"
      ],
      "metadata": {
        "id": "nnEclurjA0u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN encoder\n",
        "\n",
        "The RNN encoder is very similar to the RNN generator that we learnt that we use for language modelling. The difference is very subtle, but also very important.\n",
        "\n",
        "Relative to the position we want to generate, the RNN generator can only see the past. That is because we are generating from left-to-right, and only the past is available for that. Don't be confused, during training of the language model you already know the entire sentence, but still you cannot use information beyond the past, otherwise it would be impossible to use the model after training, when you don't know what's ahead.\n",
        "\n",
        "To recap, suppose we want to generate the sequence $x_{1:l}$ and that we have embedded its words already (i.e., $\\mathbf e_j = \\mathrm{embed}_D(x_j; \\theta_{\\text{in}})$), the **RNN generator** uses an RNN cell with inputs about the past:\n",
        "\\begin{align}\n",
        "\\mathbf u_i &= \\mathrm{rnnstep}_K(\\mathbf u_{i-1}, \\mathbf e_{i-1}; \\theta_{\\text{hid}})\n",
        "\\end{align}\n",
        "the output $\\mathbf u_i$ is $K$-dimensional and it depends on the previous state of cell $\\mathbf u_{i-1}$ and the previous word $\\mathbf e_{i-1}$, through the previous state it also depends on every word before that, and in order. For the 1st state we assume $\\mathbf u_0 = \\mathbf 0$.\n",
        "\n",
        "The **RNN encoder** is not used in situations where we are interested in generating $x_{1:l}$, rather, it is used in situations where we can count on the whole of $x_{1:l}$ being available to us (for example, where we are classifying $x_{1:l}$ into a sentiment label, or where we are labelling the steps of $x_{1:l}$ using a certain tagset). Because of that, the RNN encoder *can* (and *should*) use an RNN cell that has access to information about the present (not only the past):\n",
        "\\begin{align}\n",
        "\\mathbf u_i &= \\mathrm{rnnstep}_K(\\mathbf u_{i-1}, \\underline{\\mathbf e_{i}}; \\theta_{\\text{enc}}) ~.\n",
        "\\end{align}\n",
        "See how this RNN cell is not seeing only the past, it can see the current position too (which we underline for your attention). That is, the state $\\mathbf u_i$ knows about the past (through $\\mathbf u_{i-1}$) and it also knows about the present (through $\\mathbf e_i$).\n",
        "\n",
        "The RNN encoder can be denoted compactly as \n",
        "\\begin{align}\n",
        "\\mathbf u_{1:l} &= \\mathrm{rnnenc}_K(\\mathbf e_{1:l}; \\theta_{\\text{enc}})\n",
        "\\end{align}\n",
        "it takes a sequence of vectors as input and returns a sequence of $K$-dimensional vectors as output. The last vector  $\\mathbf u_l$ in the output sequence has information about the entire document $x_{1:l}$ in order.\n",
        "\n",
        "\n",
        "See the figure for a comparison between the RNN generator and the RNN encoder:\n",
        "<img src=\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/img/rnn.jpeg\" alt=\"drawing\" width=\"500\"/>\n"
      ],
      "metadata": {
        "id": "chHImOvsCrnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM\n",
        "\n",
        "In this tutorial we will use a modern architecture for the recurrent cell called [*Long Short-Term Memory*](https://arxiv.org/pdf/1503.04069.pdf) (LSTM for short). [It's already implemented for us in torch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n",
        "\n",
        "You do not need to study the LSTM paper, what you will need to know will be explained in this tutorial. \n",
        "\n",
        "Here we briefly explain the **internal design of the LSTM**. The choice of letters we use in this part are internal to the LSTM and are not to be confused for letters used in other contexts.\n",
        "\n",
        "For a step $t$, let $\\mathbf x_t$ be an $I$-dimensional input to an LSTM (at this point it's not important whether this corresponds to a step in the token sequence of in the tag sequence, or any sequence really, it just matters that at some point in time $t$ we want to use this input to udpate the LSTM memory).\n",
        "\n",
        "At this point, the memory of an LSTM is made of two $K$-dimensional vectors called the *cell vector* $\\mathbf c_{t-1}$ and the *hidden state* $\\mathbf h_{t-1}$, each of which is $K$-dimensional. When we process the input $\\mathbf x_t$ with an LSTM, these two vectors are updated step by step as shown below:\n",
        "\n",
        "\\begin{align}\n",
        "    \\mathbf i_t &=\\mathrm{sigmoid}(\\mathrm{affine}_K(\\mathbf x_t; \\theta_1) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_2))\\\\\n",
        "    \\mathbf f_t &=\\mathrm{sigmoid}(\\mathrm{affine}_K(\\mathbf x_t; \\theta_3) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_4))\\\\\n",
        "    \\mathbf g_t &=\\tanh(\\mathrm{affine}_K(\\mathbf x_t; \\theta_5) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_6))\\\\\n",
        "    \\mathbf o_t&=\\mathrm{sigmoid}(\\mathrm{affine}_K(\\mathbf x_t; \\theta_7) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_8))\\\\\n",
        "    \\mathbf c_t &= \\mathbf f_t \\odot \\mathbf c_{t-1} + \\mathbf i_t \\odot \\mathbf g_t \\\\\n",
        "    \\mathbf h_t &= \\mathbf o_t \\odot \\tanh(\\mathbf c_t)\n",
        "\\end{align}\n",
        "\n",
        "The first four steps compute the following using the input and the hidden state: the *input gate* $\\mathbf i_t$, then the *forget gate* $\\mathbf f_t$, the *draft cell* $\\mathbf g_t$, and the *output gate* $\\mathbf o_t$. \n",
        "These are all $K$-dimensional, and the affine transformations all have their own parameters (there 8 such affine transformations in total, they map either from $I$ dimensions to $K$ dimensions, or from $K$ dimensions to $K$ dimensions, and they have biases vectors in them). The last couple of steps finally uppdate the cell and the hidden state by combining the intermediate gates and draft cell. The symbol $\\odot$ denotes elementwise multiplication. After the update the LSTM memory is made of two states $\\mathbf c_t$ and $\\mathbf h_t$, which are $K$-dimensional. Typically, the cell is internal to the LSTM and we rarely need to use it for anything outside of it. It is the hidden state after each update that we normally want to use in applications.  The torch implementation gives us access to both of them, and we will see later how to use it.\n",
        "\n",
        "Compared to our illustration of the RNN encoder, we will have token embeddings as input, and we will interpret hidden states as outputs of the LSTM. \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "BgTTQ_uWN0qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_hidden_size = 6\n",
        "toy_lstm = nn.LSTM(\n",
        "    input_size=toy_emb_dim,\n",
        "    hidden_size=toy_hidden_size,\n",
        "    num_layers=1,\n",
        "    batch_first=True,\n",
        "    bidirectional=False,\n",
        ")\n",
        "toy_lstm"
      ],
      "metadata": {
        "id": "IcyP1TxcC6Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_parameters(toy_lstm)"
      ],
      "metadata": {
        "id": "9F1CDM5UNwcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [batch_size, max_len, emb_dim]\n",
        "e = toy_emb(toy_batch)\n",
        "# [batch_size, max_len, hidden_dim]\n",
        "# internally, the LSTM maintains two vectors in the memory\n",
        "# the forward method of the LSTM class will return \n",
        "# a tensor which has the sequence of so called hidden states (this is usually what you want to use in a text encoder)\n",
        "# and tuple of tensors that can be used in case you need access to the internal \n",
        "# mechanism of the LSTM cell (we will not be using those for now)\n",
        "u, _ = toy_lstm(e)\n",
        "print(u.shape)\n",
        "\n",
        "assert u.shape == toy_batch.shape + (toy_hidden_size,)"
      ],
      "metadata": {
        "id": "dG1fCFVnDLCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional RNN encoder\n",
        "\n",
        "In reality, there's no need to limit ourselves to past ($x_{<i}$) and present ($x_i$), rather, we have access to the whole of $x_{1:l}$, so why not also use the future $x_{>i}$. We *can do that*, and we should :). An RNN cell, by design, makes computations in a single direction (e.g., left-to-right), but we can use 2 different RNN cells, one that reads the sequence in one order and another that reads the sequence in reversed order.\n",
        "\n",
        "We don't need to invent a new RNN cell for this, we can simply reverse the inputs to a standard RNN cell:\n",
        "\\begin{align}\n",
        "\\mathbf r_{1:l} &= \\mathrm{reverse}(\\mathbf e_{1:l}) \\\\\n",
        "\\mathbf v_i &= \\mathrm{rnnstep}_K(\\mathbf v_{i-1}, \\mathbf r_{i}; \\theta_{\\text{renc}}) ~.\n",
        "\\end{align}\n",
        "Because the inputs have been reversed in order. For example, in a sentence of length $l=10$, $\\mathbf v_2$ knows about $x_{9}$ through $\\mathbf r_2$ (which is $\\mathbf e_9$) and about $x_{>9}$ through $\\mathbf v_{1}$. The last state $\\mathbf v_l$ has information about the entire document $x_{1:l}$, but processed it in reversed order.\n",
        "\n",
        "Therefore a reversed RNN encoder can be denoted compactly as follows:\n",
        "\\begin{align}\n",
        "\\mathbf v_{1:l} &= \\mathrm{rnnenc}_K(\\mathrm{reverse}(\\mathbf e_{1:l}); \\theta_{\\text{renc}})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Note that we named the parameter set differently: $\\theta_{text{enc}}$ for the first RNN cell, and $\\theta_{text{renc}}$ for the second one, that's because we indeed want to have two different sets of parameters. If we used the same set of parameters for both directions, that probably would not work very well, as reading in one direction and reading in another are conceptually two different operations.\n",
        "\n",
        "The **bidirectional RNN encoder** is our prefered text encoder, it can be denoted as follows:\n",
        "\\begin{align}\n",
        "\\mathbf o_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{enc} \\cup \\theta_{\\text{renc}})\n",
        "\\end{align}\n",
        "and here are the operations that it performs:\n",
        "\\begin{align}\n",
        "\\mathbf u_{1:l} &= \\mathrm{rnnenc}_K(\\mathbf e_{1:l}; \\theta_{\\text{enc}})\\\\\n",
        "\\mathbf v_{1:l} &= \\mathrm{rnnenc}_K(\\mathrm{reverse}(\\mathbf e_{1:l}); \\theta_{\\text{renc}})\\\\\n",
        "\\mathbf o_{i} &= \\mathrm{concat}(\\mathbf u_i, \\mathbf v_{l-i+1}) & \\text{for }i \\in \\{1, \\ldots, l\\}\n",
        "\\end{align}\n",
        "\n",
        "Its outputs are $2K$-dimensional because after processing the sequence from left-to-right with the first RNN encoder and from right-to-left with the second RNN encoder, it then concatenates the two views of the process in such a way that $\\mathbf o_i$ has information about $x_i$, $x_{<i}$ and $x_{>i}$.\n",
        "\n",
        "See the figure as an illustration of how the two RNN cells can be used to obtain the bidirectional RNN encoder: \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/img/birnn.jpeg\" alt=\"drawing\" width=\"500\"/>\n"
      ],
      "metadata": {
        "id": "DIie8FNzMblI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_bilstm = nn.LSTM(\n",
        "    input_size=toy_emb_dim,\n",
        "    hidden_size=toy_hidden_size,\n",
        "    num_layers=1,\n",
        "    batch_first=True,\n",
        "    bidirectional=True,\n",
        ")\n",
        "toy_bilstm"
      ],
      "metadata": {
        "id": "xZEXWM3rDZpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert num_parameters(toy_bilstm) == 2*num_parameters(toy_lstm), \"A BiLSTM is made of two LSTMs\""
      ],
      "metadata": {
        "id": "nhi8SmRKUeJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [batch_size, max_len, emb_dim]\n",
        "e = toy_emb(toy_batch)\n",
        "# [batch_size, max_len, 2*hidden_dim]\n",
        "# as for the standard LSTM, we only use the first of its outputs, namely, \n",
        "# a tensor of states, this time the states are concatenated for two directions, \n",
        "# thus they will be twice as large\n",
        "u2, _ = toy_bilstm(e)\n",
        "print(u2.shape)\n",
        "\n",
        "assert u2.shape == toy_batch.shape + (2*toy_hidden_size,)"
      ],
      "metadata": {
        "id": "oM7iWWAGDd65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Tagger\n",
        "\n",
        "In sequence labelling we have two sequences of equal length: a word sequence $x_{1:l}$ and a tag sequence $y_{1:l}$.\n",
        "\n",
        "\n",
        "\n",
        "The word sequence $x_{1:l} = \\langle x_1, \\ldots, x_l \\rangle$, where $l$ is the sequence length, is such that token $x_i$ belongs to a vocabulary $\\mathcal W$ of $V$ known words. \n",
        "\n",
        "The tag sequence $y_{1:l} = \\langle y_1, \\ldots, y_l \\rangle$, where $l$ is the same length as the document $x_{1:l}$, is such that each tag $y_i$ belongs to a vocabulary $\\mathcal T$ of $C$ known tags.\n",
        "\n",
        "As always, our models are probability distributions, thus, for each position $i$ of $x_{1:l}$, we want to predict a distribution over the $C$ possible tags. A common aspect of most sequence labellers is that we can condition on the document $x_{1:l}$ fully, since we are not interested in learning to generate it (rather, it will always be given to us). As for the tag sequence, it depends, we may want to model the tag independently of one another given $x_{1:l}$, we may want to use some history of previous tags (as in a Markov model), we may want to use all of the already generated tags (as in an autoregressive model). We may even want to use some future tags (leading to a class of models class *conditional random fields*, this class of models is not covered in this course, but it is covered in the 3rd year course on structure prediction).\n",
        "\n",
        "In this tutorial we are going to cover the first three options.\n"
      ],
      "metadata": {
        "id": "JlqKGKLn1FSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_all(seed=42):    \n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)    \n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "seed_all()"
      ],
      "metadata": {
        "id": "z_z8N_ZpTNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf3iDZ-nhL9E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as td\n",
        "import torch.optim as opt\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a general class, which we will specialise into different taggers."
      ],
      "metadata": {
        "id": "9GLMX3_Hvmel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tagger(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, tagset_size: int, pad_id=0, bos_id=1, eos_id=2):\n",
        "        super().__init__()\n",
        "        self._vocab_size = vocab_size\n",
        "        self._tagset_size = tagset_size\n",
        "        self._pad = pad_id\n",
        "        self._bos = bos_id\n",
        "        self._eos = eos_id\n",
        "\n",
        "    # Python properties allow client code to access the property \n",
        "    # without the risk of modifying it\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self._vocab_size\n",
        "\n",
        "    @property\n",
        "    def tagset_size(self):\n",
        "        return self._tagset_size\n",
        "\n",
        "    @property\n",
        "    def pad(self):\n",
        "        return self._pad\n",
        "\n",
        "    @property\n",
        "    def bos(self):\n",
        "        return self._bos\n",
        "\n",
        "    @property\n",
        "    def eos(self):\n",
        "        return self._eos\n",
        "\n",
        "    def num_parameters(self):\n",
        "        return sum(np.prod(theta.shape) for theta in self.parameters())\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "        raise NotImplementedError(\"Each type of tagger will have a different implementation here\")\n",
        "\n",
        "    def log_prob(self, x, y):\n",
        "        \"\"\"\n",
        "        Computes the log conditional probability of each tag sequence in a batch.\n",
        "\n",
        "        x: [batch_size, max_length]\n",
        "        y: [batch_size, max_length]\n",
        "        \"\"\"\n",
        "        # one C-dimensional Categorical cpd for each token in the batch\n",
        "        cpds = self(x=x, y=y)\n",
        "        # [batch_size, max_length]        \n",
        "        logp = cpds.log_prob(y)\n",
        "        # [batch_size]\n",
        "        logp = torch.where(y != self.pad, logp, torch.zeros_like(logp)).sum(-1)\n",
        "        return logp  \n",
        "\n",
        "    def greedy(self, x):\n",
        "        \"\"\"\n",
        "        For each cpd Y[i]|X=x, predicts the mode of the cpd.\n",
        "        x: [batch_size, max_length]\n",
        "\n",
        "        Return: tag sequences [batch_size, max_length]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Each type of tagger differs here\")\n",
        "\n",
        "    def sample(self, x, sample_size=None):\n",
        "        \"\"\"\n",
        "        Per word sequence in the batch, draws a number of samples from the model, each sample is a complete tag sequence.\n",
        "\n",
        "        x: [batch_size, max_len]\n",
        "\n",
        "        Return: tag sequences with shape [batch_size, max_len] if sample_size is None\n",
        "            else with shape [sample_size, batch_size, max_len]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Each type of tagger differs here\")\n",
        "\n",
        "    def loss(self, x, y):   \n",
        "        \"\"\"\n",
        "        Compute a scalar loss from a batch of sentences.\n",
        "        The loss is the negative log likelihood of the model estimated on a single batch:\n",
        "            - 1/batch_size * \\sum_{s} log P(y[s]|x[s], theta)\n",
        "\n",
        "        x: word sequences [batch_size, max_length] \n",
        "        y: tag sequences [batch_size, max_length] \n",
        "        \"\"\"\n",
        "        return -self.log_prob(x=x, y=y).mean(0)"
      ],
      "metadata": {
        "id": "bZjt4gVDiIRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Independent C-way classification\n",
        "\n",
        "Our first tagger is in fact just a $C$-way classifier that we use to predict a distribution over $C$ tags for different positions of an input sequence conditioned on the entire input sequence.\n",
        "\n",
        "Here is the model of the $i$th tag given $x_{1:l}$:\n",
        "\\begin{align}\n",
        "Y_i | S=x_{1:l} &\\sim \\mathrm{Categorical}(\\mathbf g(i, x_{1:l}; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf g$ is a neural network. For example:\n",
        "\\begin{align}\n",
        "\\mathbf e_j &= \\mathrm{embed}_D(x_j; \\theta_{\\text{in}})  & j \\in \\{1, \\ldots, l\\}\\\\\n",
        "\\mathbf u_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{\\text{bienc}})\\\\\n",
        "\\mathbf s_i &= \\mathrm{affine}_C(\\mathbf u_i; \\theta_{\\text{out}})\\\\\n",
        "\\mathbf g(i, x_{1:l}) &= \\mathrm{softmax}(\\mathbf s_i)\n",
        "\\end{align}\n",
        "\n",
        "The bidirection RNN layer concatenates the states of two independent RNN layers, one that processes the sequence from left-to-right, another that processes it from right-to-left. \n",
        "\n",
        "Note how this model ignores every other tag in the sequence."
      ],
      "metadata": {
        "id": "ACxYbM28BK8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTagger(Tagger):\n",
        "\n",
        "    def __init__(self, vocab_size, tagset_size, word_embed_dim: int, hidden_size: int, pad_id=0, bos_id=1, eos_id=2, recurrent_encoder=True):\n",
        "        \"\"\"        \n",
        "        vocab_size: number of known words\n",
        "        tagset_size: number of known tags\n",
        "        word_embed_dim: dimensionality of word embeddings\n",
        "        hidden_size: dimensionality of hidden layers\n",
        "        recurrent_encoder: enable recurrent encoder\n",
        "        bidirectional_encoder: for a recurrent encoder, make it bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__(vocab_size=vocab_size, tagset_size=tagset_size, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id)\n",
        "        self.word_embed_dim = word_embed_dim\n",
        "        self.hidden_size = hidden_size        \n",
        "\n",
        "        self.word_embed = nn.Embedding(self.vocab_size, embedding_dim=word_embed_dim)\n",
        "        \n",
        "        if recurrent_encoder:\n",
        "            self.encoder = nn.LSTM(\n",
        "                input_size=word_embed_dim,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=1,\n",
        "                batch_first=True,\n",
        "                bidirectional=True,\n",
        "            )\n",
        "            # the bidirectional LSTM encoder produces outputs of size 2*hidden_size\n",
        "            # thus our linear layer must take 2*hidden_size inputs\n",
        "            self.logits_predictor = nn.Linear(2 * hidden_size, self.tagset_size)\n",
        "        else:\n",
        "            self.encoder = None\n",
        "            self.logits_predictor = nn.Linear(word_embed_dim, self.tagset_size)\n",
        "        \n",
        "    def forward(self, x, y=None):\n",
        "        \"\"\"\n",
        "        Parameterise the conditional distributions over Y[i] given the entire word sequence x.        \n",
        "\n",
        "        x: [batch_size, max_length]\n",
        "        y: not used by this class\n",
        "\n",
        "        Return: a batch of C-dimensional Categorical distributions, one per step of the sequence.\n",
        "        \"\"\"\n",
        "        # We begin by embedding the tokens        \n",
        "        # [batch_size, max_length, embed_dim]\n",
        "        h = self.word_embed(x)\n",
        "        if self.encoder is not None:\n",
        "            # then encoding the sequences using a bidirectional LSTM\n",
        "            # [batch_size, max_length, 2*hidden_size]\n",
        "            h, _ = self.encoder(h)\n",
        "        # finally, per step of the sequence we predict logits for the possible tags\n",
        "        # [batch_size, max_length, tagset_size]        \n",
        "        s = self.logits_predictor(h)\n",
        "        # and convert those logits to Categorical distributions\n",
        "        return td.Categorical(logits=s)\n",
        "\n",
        "    def greedy(self, x):\n",
        "        \"\"\"\n",
        "        For each cpd Y[i]|X=x, predicts the mode of the cpd.\n",
        "        x: [batch_size, max_length]\n",
        "\n",
        "        Return: tag sequences [batch_size, max_length]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        max_length = x.shape[1]\n",
        "        with torch.no_grad():\n",
        "            cpds = self(x)\n",
        "            # [batch_size, max_length]\n",
        "            y_pred = torch.argmax(cpds.probs, -1)\n",
        "            # if a position in x is padded, it should be padded in y\n",
        "            y_pred = torch.where(x != self.pad, y_pred, torch.zeros_like(y_pred) + self.pad)\n",
        "            return y_pred\n",
        "\n",
        "    def sample(self, x, sample_size=None):\n",
        "        \"\"\"\n",
        "        Per word sequence in the batch, draws a number of samples from the model, each sample is a complete tag sequence.\n",
        "\n",
        "        x: [batch_size, max_len]\n",
        "\n",
        "        Return: tag sequences with shape [batch_size, max_len] if sample_size is None\n",
        "            else with shape [sample_size, batch_size, max_len]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        max_length = x.shape[1]\n",
        "        with torch.no_grad():\n",
        "            cpds = self(x)\n",
        "            if sample_size is None:\n",
        "                shape = (1,)\n",
        "            else:\n",
        "                shape = (sample_size,)\n",
        "            # [sample_size, batch_size, max_length]\n",
        "            y_pred = cpds.sample(shape)\n",
        "            # if a position in x is padding, it must be padded in y too\n",
        "            y_pred = torch.where(x.unsqueeze(0) != self.pad, y_pred, torch.zeros_like(y_pred) + self.pad)\n",
        "            # takes care of output shape\n",
        "            if sample_size is None:\n",
        "                return y_pred.squeeze(0)\n",
        "            else:\n",
        "                return y_pred"
      ],
      "metadata": {
        "id": "vPa1nCVBhP4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_basic_tagger():\n",
        "    seed_all()\n",
        "    toy_uni_tagger = BasicTagger(\n",
        "        vocab_size=len(word_vocab), \n",
        "        tagset_size=len(tag_vocab), \n",
        "        word_embed_dim=32,\n",
        "        hidden_size=32\n",
        "    )    \n",
        "\n",
        "    assert type(toy_uni_tagger(torch.from_numpy(word_vocab.batch_encode(training_x[:2])))) is td.Categorical\n",
        "\n",
        "    assert toy_uni_tagger.log_prob(\n",
        "            torch.from_numpy(word_vocab.batch_encode(training_x[:2])), \n",
        "            torch.from_numpy(tag_vocab.batch_encode(training_y[:2]))\n",
        "        ).shape == (2,)\n",
        "\n",
        "    assert toy_uni_tagger.loss(\n",
        "            torch.from_numpy(word_vocab.batch_encode(training_x[:2])), \n",
        "            torch.from_numpy(tag_vocab.batch_encode(training_y[:2]))\n",
        "        ).shape == tuple()\n",
        "\n",
        "    assert toy_uni_tagger.sample(torch.from_numpy(word_vocab.batch_encode(training_x[:2]))).shape == word_vocab.batch_encode(training_x[:2]).shape\n",
        "\n",
        "    assert toy_uni_tagger.sample(torch.from_numpy(word_vocab.batch_encode(training_x[:2])), 3).shape == (3,) + word_vocab.batch_encode(training_x[:2]).shape\n",
        "\n",
        "    assert toy_uni_tagger.greedy(torch.from_numpy(word_vocab.batch_encode(training_x[:2]))).shape == word_vocab.batch_encode(training_x[:2]).shape\n",
        "    \n",
        "\n",
        "test_basic_tagger()"
      ],
      "metadata": {
        "id": "syu-Z_-PkNwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Markov tagger \n",
        "\n",
        "When predicting the distribution of $Y_i$, an $n$-gram tagger (a.k.a. a Markov tagger) conditions on a fixed-size history that contains the $n-1$ previous tags in the sequence. This is the model of the $i$th tag:\n",
        "\n",
        "\\begin{align}\n",
        "Y_i | S=x_{1:l}, H=y_{i-n+1:i-1} &\\sim \\mathrm{Categorical}(\\mathbf g(i, x_{1:l}, y_{i-n+1:i-1}; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf g$ is a neural network. For example:\n",
        "\\begin{align}\n",
        "\\mathbf e_j &= \\mathrm{embed}_{D_1}(x_j; \\theta_{\\text{words}}) & j \\in \\{1, \\ldots, l\\}\\\\\n",
        "\\mathbf t_k &= \\mathrm{embed}_{D_2}(y_k; \\theta_{\\text{tags}}) & k \\in \\{i-n+1, \\ldots, i-1\\}\\\\\n",
        "\\mathbf u_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{\\text{bienc}})\\\\\n",
        "\\mathbf v_i &= \\mathrm{concat}(\\mathbf u_i, \\mathbf t_{i-n+1}, \\ldots, \\mathbf t_{i-1})\\\\\n",
        "\\mathbf s_i &= \\mathrm{affine}_C(\\mathbf v_i; \\theta_{\\text{out}})\\\\\n",
        "\\mathbf g(i, x_{1:l}) &= \\mathrm{softmax}(\\mathbf s_i)\n",
        "\\end{align}\n",
        "\n",
        "This time we have two different embedding layers, one for words and one for tags. We need to embed tags so that we can condition on the previous few tags. \n",
        "We use a bidirectional rnn to encode the whole document. Then, for the $i$th position, we use the bidirectional encoding $\\mathbf u_i$ and the embeddings of the $n-1$ previous words (assume we have padded the sequence with $n-1$ BOS tokens to the left of the first position, so this works well). This state which contains some of the history prior to $Y_i$ and the whole document $x_{1:l}$ relative to the $i$th token is what we use to predict the logits for that position.\n"
      ],
      "metadata": {
        "id": "H5tVw_y_zmDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarkovTagger(Tagger):\n",
        "\n",
        "    def __init__(self, ngram_size: int, vocab_size: int, tagset_size: int, word_embed_dim: int, tag_embed_dim: int, hidden_size: int, pad_id=0, bos_id=1, eos_id=2, recurrent_encoder=True):\n",
        "        \"\"\"\n",
        "        ngram_size: longest ngram (for tag sequence)\n",
        "        vocab_size: number of known words\n",
        "        tagset_size: number of known tags\n",
        "        word_embed_dim: dimensionality of word embeddings\n",
        "        tag_embed_dim: dimensionality of tag embeddings (needed to encode the history of ngram_size-1 tags)\n",
        "        hidden_size: dimensionality of hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__(vocab_size=vocab_size, tagset_size=tagset_size, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id)\n",
        "        assert ngram_size > 1, \"This class expects at least ngram_size 2. If you want ngram_size=1, use the NeuralUnigramTagger\"        \n",
        "        self.ngram_size = ngram_size\n",
        "        self.word_embed_dim = word_embed_dim\n",
        "        self.tag_embed_dim = tag_embed_dim\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # we need to embed words in x\n",
        "        self.word_embed = nn.Embedding(vocab_size, embedding_dim=word_embed_dim)\n",
        "        # we need to embed tags in the history \n",
        "        self.tag_embed = nn.Embedding(tagset_size, embedding_dim=tag_embed_dim) \n",
        "        \n",
        "        # we need to encode word sequences\n",
        "        if recurrent_encoder:\n",
        "            self.encoder = nn.LSTM(\n",
        "                input_size=word_embed_dim,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=1,\n",
        "                batch_first=True,\n",
        "                bidirectional=True, # as we can condition on the entire sequence, we are not limited to processing the sequence in a single direction\n",
        "            )\n",
        "            # for each position i, we need to combine the encoding of x[i] in context \n",
        "            # as well as the history of ngram_size-1 tags\n",
        "            # so we use a FFNN for that:\n",
        "            self.logits_predictor = nn.Sequential(\n",
        "                nn.Linear(2 * hidden_size + (ngram_size - 1) * tag_embed_dim, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, tagset_size),\n",
        "            )\n",
        "        else:\n",
        "            self.encoder = None\n",
        "            # for each position i, we need to combine the encoding of x[i] in context \n",
        "            # as well as the history of ngram_size-1 tags\n",
        "            # so we use a FFNN for that:\n",
        "            self.logits_predictor = nn.Sequential(\n",
        "                nn.Linear(word_embed_dim + (ngram_size - 1) * tag_embed_dim, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, tagset_size),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Parameterise the conditional distributions over Y[i] given history y[:i] and all of x.\n",
        "\n",
        "        This procedure takes care that the ith output distribution conditions only on the n-1 observations before y[i].\n",
        "        It also takes care of padding to the left with BOS symbols.\n",
        "\n",
        "        x: word sequences [batch_size, max_length]\n",
        "        y: tag sequences  [batch_size, max_length]\n",
        "\n",
        "        Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
        "        \"\"\"\n",
        "\n",
        "        # Let's start by encoding the word sequences\n",
        "        # 1. we embed the words independently\n",
        "        # [batch_size, max_length, embed_dim]\n",
        "        e = self.word_embed(x)\n",
        "        if self.encoder is not None:\n",
        "            # 2. and then encode them in their left-to-right and right-to-left context\n",
        "            # [batch_size, max_length, 2*hidden_size]\n",
        "            e, _ = self.encoder(e)\n",
        "\n",
        "        # Let's now encode the history of ngram_size-1 tags\n",
        "        # 1. create a sequence of BOS symbols to be prepended to y.\n",
        "        # [batch_size, ngram_size - 1]\n",
        "        bos = torch.full((y.shape[0], self.ngram_size - 1), self.bos, device=y.device)\n",
        "        # 2. preprend it to y\n",
        "        # # [batch_size, max_length + ngram_size - 1]\n",
        "        _y = torch.cat([bos, y], 1)\n",
        "        # 3. for each output step, we will have ngram_size - 1 inputs, so we collect those from y\n",
        "        # [batch_size, max_length, ngram_size - 1]\n",
        "        history = torch.cat([_y.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(y.shape[0], 1, -1) for i in range(y.shape[1])], 1)\n",
        "        # 4. embed the tags in the history\n",
        "        # [batch_size, max_length, ngram_size - 1, tag_emb_dim]\n",
        "        history = self.tag_embed(history)\n",
        "        # 5. concatenate the embeddings for the tags in the history\n",
        "        # [batch_size, max_length, (ngram_size - 1) * tag_emb_dim]\n",
        "        history = history.reshape(y.shape + (-1,)) \n",
        "\n",
        "        # Now we can combine the encodings of x and the encodings of histories, we do so via concatenation\n",
        "        # since there's a fixed number of such encodings per step of the sequence\n",
        "        # [batch_size, max_length, 2*hidden_size + (ngram_size - 1) * tag_emb_dim]\n",
        "        u = torch.cat([e, history], -1)\n",
        "        # We are now ready to map the state of each step of the sequence to a C-dimensional vector of logits\n",
        "        # we do so using our FFNN\n",
        "        # [batch_size, max_length, tagset_size]\n",
        "        s = self.logits_predictor(u)\n",
        "\n",
        "        return td.Categorical(logits=s)\n",
        "\n",
        "    def greedy(self,x):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        max_length = x.shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # add the beginning we do not know the tag sequence\n",
        "            # but NNs work with fixed dimensional tensors, \n",
        "            # so we allocate a tensor full of BOS codes\n",
        "            y = torch.full((batch_size, max_length), self.bos, device=self.word_embed.weight.device) \n",
        "            # Per step\n",
        "            for i in range(max_length):\n",
        "                # we parameterise a cpd for Y[i]|X=x\n",
        "                # note that the forward method takes care of not conditioning on y[i] itself\n",
        "                # and only using the ngram_size-1 previous tags\n",
        "                # at this point, the tag y[i] is a dummy code\n",
        "                # the forward method recomputes all cds in the batch, this will include the cpd for Y[i]\n",
        "                # [batch_size, max_len, C] \n",
        "                cpds = self(x, y)\n",
        "                # we get their modes via argmax\n",
        "                # [batch_size, max_len]\n",
        "                modes = torch.argmax(cpds.probs, -1)\n",
        "                \n",
        "                # Here we update the current token to the freshly obtained mode\n",
        "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
        "                y[:, i] = modes[:, i]                \n",
        "            # where we had a PAD token in x, we change the y token to PAD too\n",
        "            y = torch.where(x != self.pad, y, torch.zeros_like(y) + self.pad)\n",
        "            \n",
        "            return y\n",
        "\n",
        "    def _sample(self, x):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        max_length = x.shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # add the beginning we do not know the tag sequence\n",
        "            # but NNs work with fixed dimensional tensors, \n",
        "            # so we allocate a tensor full of BOS codes\n",
        "            y = torch.full((batch_size, max_length), self.bos, device=self.word_embed.weight.device) \n",
        "\n",
        "            # Per step\n",
        "            for i in range(max_length):\n",
        "                # we parameterise a cpd for Y[i]|X=x\n",
        "                # note that the forward method takes care of not conditioning on y[i] itself\n",
        "                # and only using the ngram_size-1 previous tags\n",
        "                # at this point, the tag y[i] is a dummy code\n",
        "                # the forward method recomputes all cds in the batch, this will include the cpd for Y[i]\n",
        "                # we get their modes via argmax\n",
        "                # [batch_size, max_len, C]\n",
        "                cpds = self(x, y)\n",
        "                # [batch_size, max_len]\n",
        "                samples = cpds.sample()\n",
        "                \n",
        "                # Here we update the current token to the freshly obtained mode\n",
        "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
        "                y[:, i] = samples[:,i]\n",
        "            # where we had a PAD token in x, we change the y token to PAD too\n",
        "            y = torch.where(x != self.pad, y, torch.zeros_like(y) + self.pad)\n",
        "\n",
        "            return y\n",
        "\n",
        "    def sample(self, x, sample_size=None):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        if sample_size is None:\n",
        "            return self._sample(x)\n",
        "        else:\n",
        "            samples = [self._sample(x) for _ in range(sample_size)]\n",
        "            return torch.stack(samples)"
      ],
      "metadata": {
        "id": "5fxszj8fbGHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_markov_tagger():\n",
        "    seed_all()\n",
        "    toy_bigram_tagger = MarkovTagger(\n",
        "        ngram_size=2, \n",
        "        vocab_size=len(word_vocab), \n",
        "        tagset_size=len(tag_vocab), \n",
        "        word_embed_dim=32,\n",
        "        tag_embed_dim=16,\n",
        "        hidden_size=32\n",
        "    )\n",
        "    assert type(toy_bigram_tagger(\n",
        "        torch.from_numpy(word_vocab.batch_encode(training_x[:2])),\n",
        "        torch.from_numpy(tag_vocab.batch_encode(training_y[:2])))\n",
        "    ) is td.Categorical\n",
        "\n",
        "    assert toy_bigram_tagger.log_prob(\n",
        "            torch.from_numpy(word_vocab.batch_encode(training_x[:2])), \n",
        "            torch.from_numpy(tag_vocab.batch_encode(training_y[:2]))\n",
        "        ).shape == (2,)\n",
        "\n",
        "    assert toy_bigram_tagger.loss(\n",
        "            torch.from_numpy(word_vocab.batch_encode(training_x[:2])), \n",
        "            torch.from_numpy(tag_vocab.batch_encode(training_y[:2]))\n",
        "        ).shape == tuple()\n",
        "\n",
        "    assert toy_bigram_tagger.sample(torch.from_numpy(word_vocab.batch_encode(training_x[:2]))).shape == word_vocab.batch_encode(training_x[:2]).shape\n",
        "\n",
        "    assert toy_bigram_tagger.sample(torch.from_numpy(word_vocab.batch_encode(training_x[:2])), 3).shape == (3,) + word_vocab.batch_encode(training_x[:2]).shape\n",
        "\n",
        "    assert toy_bigram_tagger.greedy(torch.from_numpy(word_vocab.batch_encode(training_x[:2]))).shape == word_vocab.batch_encode(training_x[:2]).shape\n",
        "    \n",
        "\n",
        "test_markov_tagger()"
      ],
      "metadata": {
        "id": "V7clkoxYVd-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Autoregressive tagger \n",
        "\n",
        "When predicting the distribution of $Y_i$, an autoregressive tagger conditions on the tag sequence already generated thus far, hence it makes no Markov assumption. This is the model of the $i$th tag:\n",
        "\n",
        "\\begin{align}\n",
        "Y_i | S=x_{1:l}, H=y_{<i} &\\sim \\mathrm{Categorical}(\\mathbf g(i, x_{1:l}, y_{<i}; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf g$ is a neural network. For example:\n",
        "\\begin{align}\n",
        "\\mathbf e_j &= \\mathrm{embed}_{D_1}(x_j; \\theta_{\\text{words}}) & j \\in \\{1, \\ldots, l\\}\\\\\n",
        "\\mathbf t_k &= \\mathrm{embed}_{D_2}(y_k; \\theta_{\\text{tags}}) & k < i\\\\\n",
        "\\mathbf u_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{\\text{bienc}})\\\\\n",
        "\\mathbf v_i &= \\mathrm{rnnstep}_K(\\mathbf v_{i-1},  \\mathbf t_{i-1}; \\theta_{\\text{dec}})\\\\\n",
        "\\mathbf s_i &= \\mathrm{ffnn}_C(\\mathrm{concat}(\\mathbf u_i, \\mathbf v_i); \\theta_{\\text{out}})\\\\\n",
        "\\mathbf g(i, x_{1:l}) &= \\mathrm{softmax}(\\mathbf s_i)\n",
        "\\end{align}\n",
        "\n",
        "Again, we  have two different embedding layers, one for words and one for tags. \n",
        "Again, we use a bidirectional rnn to encode the whole document. \n",
        "Now, for the $i$th position, we use an RNN generator/decoder cell to encode the complete history of previous tags. We then predict the logits by using an FFNN to combine the history encoding with the document encoding for position $i$.\n",
        "\n",
        "There's yet another way to parameterise this model, in which we let the RNN decoder compose the features of the history with the features of the document:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf e_j &= \\mathrm{embed}_{D_1}(x_j; \\theta_{\\text{words}}) & j \\in \\{1, \\ldots, l\\}\\\\\n",
        "\\mathbf t_k &= \\mathrm{embed}_{D_2}(y_k; \\theta_{\\text{tags}}) & k < i\\\\\n",
        "\\mathbf u_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{\\text{bienc}})\\\\\n",
        "\\mathbf v_i &= \\mathrm{rnnstep}_K(\\mathbf v_{i-1}, \\mathrm{concat}(\\mathbf u_i, \\mathbf t_{i-1}); \\theta_{\\text{dec}})\\\\\n",
        "\\mathbf s_i &= \\mathrm{affine}_C(\\mathbf v_i; \\theta_{\\text{out}})\\\\\n",
        "\\mathbf g(i, x_{1:l}) &= \\mathrm{softmax}(\\mathbf s_i)\n",
        "\\end{align}\n",
        "\n",
        "Both are very good options."
      ],
      "metadata": {
        "id": "iQBNExaS2h7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoregressiveTagger(Tagger):\n",
        "\n",
        "    def __init__(self, vocab_size: int, tagset_size: int, word_embed_dim: int, tag_embed_dim: int, hidden_size: int, pad_id=0, bos_id=1, eos_id=2, recurrent_encoder=True):\n",
        "        \"\"\"\n",
        "        ngram_size: longest ngram (for tag sequence)\n",
        "        vocab_size: number of known words\n",
        "        tagset_size: number of known tags\n",
        "        word_embed_dim: dimensionality of word embeddings\n",
        "        tag_embed_dim: dimensionality of tag embeddings (needed to encode the history of ngram_size-1 tags)\n",
        "        hidden_size: dimensionality of hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__(vocab_size=vocab_size, tagset_size=tagset_size, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id)        \n",
        "        self.word_embed_dim = word_embed_dim\n",
        "        self.tag_embed_dim = tag_embed_dim\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # we need to embed words in x\n",
        "        self.word_embed = nn.Embedding(vocab_size, embedding_dim=word_embed_dim)\n",
        "        # we need to embed tags in the history \n",
        "        self.tag_embed = nn.Embedding(tagset_size, embedding_dim=tag_embed_dim) \n",
        "        \n",
        "        # we need to encode word sequences\n",
        "        if recurrent_encoder:\n",
        "            self.encoder = nn.LSTM(\n",
        "                input_size=word_embed_dim,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=1,\n",
        "                batch_first=True,\n",
        "                bidirectional=True, # as we can condition on the entire sequence, we are not limited to processing the sequence in a single direction\n",
        "            )\n",
        "        else:\n",
        "            self.encoder = None\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=tag_embed_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        if recurrent_encoder:\n",
        "            # for each position i, we need to combine the encoding of x[i] in context \n",
        "            # as well as the history of ngram_size-1 tags\n",
        "            # so we use a FFNN for that:\n",
        "            self.logits_predictor = nn.Sequential(\n",
        "                nn.Linear(3 * hidden_size, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, tagset_size),\n",
        "            )\n",
        "        else:\n",
        "            # for each position i, we need to combine the encoding of x[i] in context \n",
        "            # as well as the history of ngram_size-1 tags\n",
        "            # so we use a FFNN for that:\n",
        "            self.logits_predictor = nn.Sequential(\n",
        "                nn.Linear(word_embed_dim + hidden_size, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, tagset_size),\n",
        "            )\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Parameterise the conditional distributions over Y[i] given history y[:i] and all of x.\n",
        "\n",
        "        This procedure takes care that the ith output distribution conditions only on the n-1 observations before y[i].\n",
        "        It also takes care of padding to the left with BOS symbols.\n",
        "\n",
        "        x: word sequences [batch_size, max_length]\n",
        "        y: tag sequences  [batch_size, max_length]\n",
        "\n",
        "        Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
        "        \"\"\"\n",
        "\n",
        "        # Let's start by encoding the word sequences\n",
        "        # 1. we embed the words independently\n",
        "        # [batch_size, max_length, embed_dim]\n",
        "        e = self.word_embed(x)\n",
        "        if self.encoder is None:\n",
        "            u = e\n",
        "        else:\n",
        "            # 2. and then encode them in their left-to-right and right-to-left context\n",
        "            # [batch_size, max_length, 2*hidden_size]\n",
        "            u, _ = self.encoder(e)\n",
        "\n",
        "        # here we pad the tag sequence with BOS on the left\n",
        "        # this is how we make sure that the current position of the tag sequence\n",
        "        # is not available for conditioning (only the past is)\n",
        "        batch_size, max_len = y.shape\n",
        "        bos = torch.full((batch_size, 1), self.bos, device=y.device)\n",
        "        _y = torch.cat([bos, y], 1)\n",
        "        # [batch_size, max_len, tag_emb_dim]\n",
        "        t_in = self.tag_embed(_y[:,:max_len])\n",
        "\n",
        "        # [batch_size, max_len, hidden_size]\n",
        "        v, _ = self.decoder(t_in)\n",
        "\n",
        "        # memory = torch.zeros(1, 1, self.hidden_size, device=y.device), torch.zeros(1, 1, self.hidden_size, device=y.device)        \n",
        "        # for i in range(1, max_len + 1):\n",
        "        #     v, memory = self.decoder(t_in[:,:i], memory)\n",
        "\n",
        "        # Now we can combine the encodings of x and the encodings of histories, we do so via concatenation\n",
        "        # since there's a fixed number of such encodings per step of the sequence\n",
        "        # [batch_size, max_length, 3*hidden_size]\n",
        "        u = torch.cat([u, v], -1)\n",
        "        # We are now ready to map the state of each step of the sequence to a C-dimensional vector of logits\n",
        "        # we do so using our FFNN\n",
        "        # [batch_size, max_length, tagset_size]\n",
        "        s = self.logits_predictor(u)\n",
        "\n",
        "        return td.Categorical(logits=s)\n",
        "\n",
        "    def greedy(self,x):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        max_length = x.shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # add the beginning we do not know the tag sequence\n",
        "            # but NNs work with fixed dimensional tensors, \n",
        "            # so we allocate a tensor full of BOS codes\n",
        "            y = torch.full((batch_size, max_length), self.bos, device=self.word_embed.weight.device) \n",
        "            # Per step\n",
        "            for i in range(max_length):\n",
        "                # we parameterise a cpd for Y[i]|X=x\n",
        "                # note that the forward method takes care of not conditioning on y[i] itself\n",
        "                # and only using the ngram_size-1 previous tags\n",
        "                # at this point, the tag y[i] is a dummy code\n",
        "                # the forward method recomputes all cds in the batch, this will include the cpd for Y[i]\n",
        "                # [batch_size, max_len, C] \n",
        "                cpds = self(x, y)\n",
        "                # we get their modes via argmax\n",
        "                # [batch_size, max_len]\n",
        "                modes = torch.argmax(cpds.probs, -1)\n",
        "                \n",
        "                # Here we update the current token to the freshly obtained mode\n",
        "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
        "                y[:, i] = modes[:, i]                \n",
        "            # where we had a PAD token in x, we change the y token to PAD too\n",
        "            y = torch.where(x != self.pad, y, torch.zeros_like(y) + self.pad)\n",
        "            \n",
        "            return y\n",
        "\n",
        "    def _sample(self, x):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        max_length = x.shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # add the beginning we do not know the tag sequence\n",
        "            # but NNs work with fixed dimensional tensors, \n",
        "            # so we allocate a tensor full of BOS codes\n",
        "            y = torch.full((batch_size, max_length), self.bos, device=self.word_embed.weight.device) \n",
        "\n",
        "            # Per step\n",
        "            for i in range(max_length):\n",
        "                # we parameterise a cpd for Y[i]|X=x\n",
        "                # note that the forward method takes care of not conditioning on y[i] itself\n",
        "                # and only using the ngram_size-1 previous tags\n",
        "                # at this point, the tag y[i] is a dummy code\n",
        "                # the forward method recomputes all cds in the batch, this will include the cpd for Y[i]\n",
        "                # we get their modes via argmax\n",
        "                # [batch_size, max_len, C]\n",
        "                cpds = self(x, y)\n",
        "                # [batch_size, max_len]\n",
        "                samples = cpds.sample()\n",
        "                \n",
        "                # Here we update the current token to the freshly obtained mode\n",
        "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
        "                y[:, i] = samples[:,i]\n",
        "            # where we had a PAD token in x, we change the y token to PAD too\n",
        "            y = torch.where(x != self.pad, y, torch.zeros_like(y) + self.pad)\n",
        "\n",
        "            return y\n",
        "\n",
        "    def sample(self, x, sample_size=None):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        if sample_size is None:\n",
        "            return self._sample(x)\n",
        "        else:\n",
        "            samples = [self._sample(x) for _ in range(sample_size)]\n",
        "            return torch.stack(samples)"
      ],
      "metadata": {
        "id": "5UPrqLGm4aRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_autoreg_tagger():\n",
        "    seed_all()\n",
        "    toy_ar_tagger = AutoregressiveTagger(\n",
        "        vocab_size=len(word_vocab), \n",
        "        tagset_size=len(tag_vocab), \n",
        "        word_embed_dim=32,\n",
        "        tag_embed_dim=16,\n",
        "        hidden_size=32\n",
        "    )    \n",
        "\n",
        "    assert type(toy_ar_tagger(\n",
        "        torch.from_numpy(word_vocab.batch_encode(training_x[:2])),\n",
        "        torch.from_numpy(tag_vocab.batch_encode(training_y[:2])))\n",
        "    ) is td.Categorical\n",
        "\n",
        "    assert toy_ar_tagger.log_prob(\n",
        "            torch.from_numpy(word_vocab.batch_encode(training_x[:2])), \n",
        "            torch.from_numpy(tag_vocab.batch_encode(training_y[:2]))\n",
        "        ).shape == (2,)\n",
        "\n",
        "    assert toy_ar_tagger.loss(\n",
        "            torch.from_numpy(word_vocab.batch_encode(training_x[:2])), \n",
        "            torch.from_numpy(tag_vocab.batch_encode(training_y[:2]))\n",
        "        ).shape == tuple()\n",
        "\n",
        "    assert toy_ar_tagger.sample(torch.from_numpy(word_vocab.batch_encode(training_x[:2]))).shape == word_vocab.batch_encode(training_x[:2]).shape\n",
        "\n",
        "    assert toy_ar_tagger.sample(torch.from_numpy(word_vocab.batch_encode(training_x[:2])), 3).shape == (3,) + word_vocab.batch_encode(training_x[:2]).shape\n",
        "\n",
        "    assert toy_ar_tagger.greedy(torch.from_numpy(word_vocab.batch_encode(training_x[:2]))).shape == word_vocab.batch_encode(training_x[:2]).shape\n",
        "    \n",
        "\n",
        "test_autoreg_tagger()"
      ],
      "metadata": {
        "id": "f6qSVMLkV-LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graded exercise - Model size"
      ],
      "metadata": {
        "id": "2PLViqCVWE5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graded exercise - Model size**\n",
        "\n",
        "Below we provide 3 functions that express the size of our models in terms of the constants we choose (dimensionality, number of words, number of tags, ngram-size, etc). You should implement those functions.\n",
        "\n",
        "We have provided code for assertions that test those functions against the actual parameter count reported by `model.num_parameters()`. You can use that to verify that you got to the right number."
      ],
      "metadata": {
        "id": "LsGXCSmd_O1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_tagger_size(vocab_size, tagset_size, word_embed_dim, hidden_size):\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "VEcUW-Ai8235"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't change the arguments of the test\n",
        "assert BasicTagger(\n",
        "    vocab_size=len(word_vocab), \n",
        "    tagset_size=len(tag_vocab), \n",
        "    word_embed_dim=32,\n",
        "    hidden_size=64, \n",
        "    recurrent_encoder=True).num_parameters() == basic_tagger_size(len(word_vocab), len(tag_vocab), 32, 64)"
      ],
      "metadata": {
        "id": "83y30Kqm9PUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def markov_tagger_size(ngram_size, vocab_size, tagset_size, word_embed_dim, tag_embed_dim, hidden_size):\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "avbZ4mVZ9kE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't change the arguments of the test\n",
        "assert MarkovTagger(\n",
        "    ngram_size=2,\n",
        "    vocab_size=len(word_vocab), \n",
        "    tagset_size=len(tag_vocab), \n",
        "    word_embed_dim=32,\n",
        "    tag_embed_dim=12,\n",
        "    hidden_size=64, \n",
        "    recurrent_encoder=True).num_parameters() == markov_tagger_size(2, len(word_vocab), len(tag_vocab), 32, 12, 64)"
      ],
      "metadata": {
        "id": "0N7CxLU_9n9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoregressive_tagger_size(vocab_size, tagset_size, word_embed_dim, tag_embed_dim, hidden_size):\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "nuPu3QoB--Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't change the arguments of the test\n",
        "assert AutoregressiveTagger(\n",
        "    vocab_size=len(word_vocab), \n",
        "    tagset_size=len(tag_vocab), \n",
        "    word_embed_dim=32,\n",
        "    tag_embed_dim=12,\n",
        "    hidden_size=64,\n",
        "    recurrent_encoder=True).num_parameters() == autoregressive_tagger_size(len(word_vocab), len(tag_vocab), 32, 12, 64)"
      ],
      "metadata": {
        "id": "_SktEkOg_BW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "0R1QKLf6WWxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will conduct an experiment with an actual corpus, we better use GPU support for that (on Google Colab you change the runtime to GPU)."
      ],
      "metadata": {
        "id": "mdxlZ_imZHCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_device = torch.device('cuda:0')\n",
        "my_device"
      ],
      "metadata": {
        "id": "pKFUw21bYCUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate sequence models intrinsically, using perplexity:"
      ],
      "metadata": {
        "id": "BZVMkq0GZOqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(model, dl, device):\n",
        "    \"\"\"\n",
        "    Every sequence model can be evaluated intrinsically in terms of perplexity.\n",
        "    Perplexity is very interpretable, a perplexity value `ppl` means\n",
        "        given the context available (which varies depending on the type of model you use)\n",
        "        the uncertainty of the model about the next token has been narrowed down \n",
        "        to `ppl` possible outputs (out of the C options available for tagging).\n",
        "\n",
        "    model: one of our taggers\n",
        "    dl: a data loader for the heldout data\n",
        "    device: the PyTorch device where the model is stored\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    total_log_prob = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dl:\n",
        "            total_tokens += (batch_x != model.pad).float().sum()\n",
        "            total_log_prob = total_log_prob + model.log_prob(batch_x.to(device), batch_y.to(device)).sum()\n",
        "    return torch.exp(-total_log_prob / total_tokens)"
      ],
      "metadata": {
        "id": "Foxt9hp0n5J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because labelling is a chain of classification decisions, we can also evaluate our tagger in terms of accuracy of its decisions. For that we need a **decision rule**. Normally, in NLP, we use the **most probable tag sequence** as a decision. That is, given a sentence $x_{1:l}$ we search in the space $\\{1, \\ldots, C\\}^l$ of all tag sequences of length $l$, for the one sequence that the model assigns highest probability to (i.e., the *mode* of the conditional distribution):\n",
        "\n",
        "\\begin{align}\n",
        "y^\\star &= \\arg\\max_{c_{1:l} \\in \\{1, \\ldots, C\\}^l}~ \\log P(G=c_{1:l}|S=x_{1:l})\n",
        "\\end{align}\n",
        "\n",
        "This search is defined over an extremely large space and is generally not tractable. For some types of tagger, because of their conditional independence assumptions, this search may be doable in polynomial time (as a function of sequence length), for others this is not at all possible. \n",
        "\n",
        "For the basic tagger, which treats the tags as independent given the sentence, this search can be done exactly, because greedily maximising each step independently is equivalently to maximising the joint assignment of the entire sequence for that model.\n",
        "\n",
        "**Search for the unigram tagger**\n",
        "\n",
        "We search for the best tag in each position, which takes time $\\mathcal O(C)$ per position, \n",
        "\\begin{align}\n",
        "y^\\star_i &= \\arg\\max_{c \\in \\{1, \\ldots, C\\}}~ \\log P(Y_i=c|S=x_{1:l})\n",
        "\\end{align}\n",
        "and put them together in a sequence. The total operation takes time $\\mathcal O(l \\times C)$.\n",
        "\n",
        "**Search for the Markov tagger**\n",
        "\n",
        "The Markov tagger makes fewer conditional independence assumptions, and the search problem is a bit harder. Solving for each tag independently and concatenating the result will not give us the *true mode* of the conditional distribution. If we do that, we have a *greedy* approximation to the true mode. \n",
        "\n",
        "To search for the exact mode we need a special algorithm called *the Viterbi algorithm*, a type of *dynamic programming* algorithm that can solve the search efficiently. This is not within the scope of this course, but it will most likely be covered in a course on structured prediction (eg, in year 3).\n",
        "\n",
        "For this tutorial we will use the greedy approximation:\n",
        "\\begin{align}\n",
        "\\hat y_i &= \\arg\\max_{c \\in \\{1, \\ldots, C\\}}~ \\log P(Y_i=c|S=x_{1:l}, H=\\hat y_{i-n1+1:i-1})\n",
        "\\end{align}\n",
        "where we solve the argmax locally per tag in order from left-to-right. For each step $Y_i$ we condition on the already predicted argmax for the $n-1$ preceding steps.\n",
        "\n",
        "Once again, this is an approximation motivated by efficiency, not by correctness.\n",
        "\n",
        "**Search for the autoregressive tagger**\n",
        "\n",
        "The autoregressive tagger makes no conditional independence assumptions, and the search problem is genuinely intractable for this model. Being intractable means there is not efficient algorithm known to be able to handle it. In fact, the current hypothesis is that an efficient (by efficient we mean that it runs in polynomial time as a function of $l$) is actually impossible in standard computer architectures. Problems of this kind are called NP-complete.\n",
        "\n",
        "For this tutorial, we will again use the greedy approximation:\n",
        "\\begin{align}\n",
        "\\hat y_i &= \\arg\\max_{c \\in \\{1, \\ldots, C\\}}~ \\log P(Y_i=c|S=x_{1:l}, H=\\hat y_{<i})\n",
        "\\end{align}\n",
        "where we solve the argmax locally per position in order from left-to-right. For each step $Y_i$ we condition on the already predicted argmax for all the preceding steps.\n",
        "\n",
        "Other approximations do exist, some of them with better properties than this greedy one, but they are algorithmically more complex and not within scope for this course.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Once we have a search algorithm in place to make predictions we can compute accuracy and/or other metrics common for classification.\n"
      ],
      "metadata": {
        "id": "8yUftHTPkhMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_untagged_corpus(corpus_x, word_vocab: Vocab, tag_vocab: Vocab):        \n",
        "    \"\"\"\n",
        "    This lets us use TaggedCorpus for an untagged corpus, by pairing each \n",
        "    token sequence with an equal length PAD-tag sequence.\n",
        "\n",
        "    corpus_x: token sequences (e.g., a test set for which we do not know the tag sequence)\n",
        "    word_vocab: vocabulary of known words\n",
        "    tag_vocab: vocabulary of known tags\n",
        "\n",
        "    You can use this to help you tag a new dataset for which you do not have tags (e.g., examples you crete yourself)\n",
        "    \"\"\"\n",
        "    return TaggedCorpus(corpus_x, [[tag_vocab.pad_token] * len(seq) for seq in corpus_x], word_vocab, tag_vocab)"
      ],
      "metadata": {
        "id": "Vg4FRel3lv4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a predict function"
      ],
      "metadata": {
        "id": "BGzeKX-wv9NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, word_vocab, tag_vocab, dl, device, return_targets=False):\n",
        "    \"\"\"\n",
        "    Greedy predictions.\n",
        "    \n",
        "    model: one of our taggers\n",
        "    word_vocab:\n",
        "    tag_vocab:\n",
        "    dl: a data loader for the heldout data\n",
        "    device: the PyTorch device where the model is stored\n",
        "    return_targets: also return the targets from the data loader\n",
        "        you can use this when the actual targets are in the dataloader (e.g., for dev set)\n",
        "\n",
        "    Return \n",
        "        * a list of predictions, each a sequence of tags (already decoded)\n",
        "        * if return_targets=True, additionally return a list of targets, each a sequence of tags (already decoded)\n",
        "    \"\"\"\n",
        "    model.eval()    \n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dl:\n",
        "            # [batch_size, max_len]\n",
        "            preds = model.greedy(batch_x.to(device))\n",
        "            lengths = torch.sum(batch_x != model.pad, -1).cpu().numpy()            \n",
        "            all_preds.extend([seq[:l] for l, seq in zip(lengths, tag_vocab.batch_decode(preds.cpu(), strip_pad=False))])\n",
        "            if return_targets:                \n",
        "                all_targets.extend([seq[:l] for l, seq in zip(lengths, tag_vocab.batch_decode(batch_y, strip_pad=False))])\n",
        "\n",
        "    if return_targets:\n",
        "        return all_preds, all_targets\n",
        "    else:\n",
        "        return all_preds"
      ],
      "metadata": {
        "id": "zZV5c0WEhdk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use sklearn's classification report"
      ],
      "metadata": {
        "id": "a2atjWtPwF8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "OnlLnap0ot3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have the training loop (already fully implemented for you). Do study it."
      ],
      "metadata": {
        "id": "x7u12R0t3mAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from itertools import chain\n",
        "\n",
        "\n",
        "def flatten(seq):\n",
        "    \"\"\"flattens a python list\"\"\"\n",
        "    return list(chain.from_iterable(seq))\n",
        "\n",
        "def train_neural_model(model, optimiser, training_corpus, dev_corpus, batch_size=200, num_epochs=10, check_every=10, device=torch.device('cuda:0')):\n",
        "    \"\"\"\n",
        "    model: pytorch model\n",
        "    optimiser: pytorch optimiser\n",
        "    training_corpus: a TaggedCorpus for trianing\n",
        "    dev_corpus: a TaggedCorpus for dev\n",
        "    batch_size: use more if you have more memory\n",
        "    num_epochs: use more for improved convergence\n",
        "    check_every: use less to check performance on dev set more often\n",
        "    device: where we run the experiment\n",
        "\n",
        "    Return a log of quantities computed during training (for plotting)\n",
        "    \"\"\"\n",
        "    # we use the training data in random order for parameter estimation\n",
        "    batcher = DataLoader(training_corpus, batch_size=batch_size, shuffle=True, collate_fn=TaggedCorpus.pad_to_longest)\n",
        "    # we use the dev data for evaluation during training (no need for randomisation here)\n",
        "    dev_batcher = DataLoader(dev_corpus, batch_size=batch_size, shuffle=False, collate_fn=TaggedCorpus.pad_to_longest)\n",
        "\n",
        "    total_steps = num_epochs * len(batcher)\n",
        "    log = defaultdict(list)\n",
        "    \n",
        "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
        "    log['ppl'].append(ppl)\n",
        "    \n",
        "    preds, targets = predict(\n",
        "        model, \n",
        "        training_corpus.word_vocab, \n",
        "        training_corpus.tag_vocab, \n",
        "        dev_batcher, \n",
        "        device=device, \n",
        "        return_targets=True)    \n",
        "    acc = classification_report(flatten(targets), flatten(preds), output_dict=True, zero_division=0)['accuracy']\n",
        "    log['acc'].append(acc)\n",
        "    \n",
        "    step = 0\n",
        "\n",
        "    with tqdm(range(total_steps)) as bar:\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            for batch_x, batch_y in batcher:\n",
        "                model.train()\n",
        "                optimiser.zero_grad()\n",
        "                \n",
        "                loss = model.loss(batch_x.to(device), batch_y.to(device))\n",
        "                        \n",
        "                loss.backward()\n",
        "                optimiser.step()\n",
        "\n",
        "                bar.set_postfix({'loss': f\"{loss.item():.2f}\", 'ppl': f\"{ppl:.2f}\", 'acc': f\"{acc:.2f}\"} )\n",
        "                bar.update()  \n",
        "                log['loss'].append(loss.item())\n",
        "\n",
        "                if step % check_every == 0:\n",
        "                    ppl = perplexity(model, dev_batcher, device=device).item()\n",
        "                    log['ppl'].append(ppl)\n",
        "                    \n",
        "                    preds = predict(\n",
        "                        model, \n",
        "                        training_corpus.word_vocab, \n",
        "                        training_corpus.tag_vocab, \n",
        "                        dev_batcher, \n",
        "                        device=device, \n",
        "                        return_targets=False)    \n",
        "                    acc = classification_report(flatten(targets), flatten(preds), output_dict=True, zero_division=0)['accuracy']\n",
        "                    log['acc'].append(acc)                    \n",
        "                \n",
        "                step += 1\n",
        "                \n",
        "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
        "    log['ppl'].append(ppl)\n",
        "    \n",
        "    preds = predict(\n",
        "        model, \n",
        "        training_corpus.word_vocab, \n",
        "        training_corpus.tag_vocab, \n",
        "        dev_batcher, \n",
        "        device=device, \n",
        "        return_targets=False)    \n",
        "    acc = classification_report(flatten(targets), flatten(preds), output_dict=True, zero_division=0)['accuracy']\n",
        "    log['acc'].append(acc)\n",
        "\n",
        "    return log            "
      ],
      "metadata": {
        "id": "ZQR5TGAqd-HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment\n",
        "\n",
        "Here we demonstrate how to train and evaluate a model. \n",
        "\n",
        "After that you will conduct an experiment."
      ],
      "metadata": {
        "id": "RwXhhAUChsKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On GPU, this should take just about 2 minutes:"
      ],
      "metadata": {
        "id": "sko-agWU5Giv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all() # reset random number generators before creating your model and training it\n",
        "\n",
        "\n",
        "tagger = BasicTagger(\n",
        "    vocab_size=len(word_vocab), \n",
        "    tagset_size=len(tag_vocab), \n",
        "    word_embed_dim=64,\n",
        "    hidden_size=128,\n",
        "    recurrent_encoder=False, # no BiRNN\n",
        ").to(my_device)\n",
        "\n",
        "\n",
        "# construct an Adam optimiser\n",
        "optimiser = opt.Adam(tagger.parameters(), lr=5e-3)\n",
        "\n",
        "print(\"Model\")\n",
        "print(tagger)\n",
        "# report number of parameters\n",
        "print(\"Model size:\", tagger.num_parameters())\n",
        "\n",
        "# Train the model\n",
        "log = train_neural_model(\n",
        "    tagger, optimiser, training, dev, \n",
        "    batch_size=200, num_epochs=10, check_every=10,\n",
        "    device=my_device\n",
        ")\n",
        "\n",
        "# Plot loss and validation checks\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "_ = axs[0].plot(np.arange(len(log['loss'])), log['loss'])\n",
        "_ = axs[0].set_xlabel('steps')\n",
        "_ = axs[0].set_ylabel('training loss')\n",
        "_ = axs[1].plot(np.arange(len(log['ppl'])), log['ppl'])\n",
        "_ = axs[1].set_xlabel('steps (in 100s)')\n",
        "_ = axs[1].set_ylabel('model ppl given dev')\n",
        "_ = axs[2].plot(np.arange(len(log['acc'])), log['acc'])\n",
        "_ = axs[2].set_xlabel('steps (in 10s)')\n",
        "_ = axs[2].set_ylabel('dev acc')\n",
        "_ = fig.tight_layout(h_pad=2, w_pad=2)\n",
        "plt.show()\n",
        "\n",
        "# Predict for dev set\n",
        "y_, y = predict(\n",
        "    tagger, \n",
        "    word_vocab, \n",
        "    tag_vocab, \n",
        "    DataLoader(dev, batch_size=200, shuffle=False, collate_fn=TaggedCorpus.pad_to_longest), \n",
        "    my_device, \n",
        "    return_targets=True\n",
        ")\n",
        "\n",
        "# Compare predictions and targets\n",
        "print(classification_report(flatten(y),flatten(y_), zero_division=0))"
      ],
      "metadata": {
        "id": "hBG_u0XMe_Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graded exercise - comparison\n",
        "\n",
        "1. Using the treebank dataset, compare the three types of taggers (using the dev set). First, train them without using the recurrent encoder. Display their performance in terms of f1-score for each of the tags in a table (one column per model type). You may use the helper function below to make tables (but you can also use anything else you prefer). \n",
        "\n",
        "2. Then, for each model type, compare their performance with and without the recurrent encoder. \n",
        "\n",
        "3. For all models, display plots of training loss, dev perplexity and accuracy. \n",
        "\n",
        "4. Discuss whether you observe benefits from relaxing independence assumptions with and without the BiRNN encoder. \n",
        "\n",
        "5. Then, using the test set, compare their overall performance (across all tags). \n",
        "\n",
        "6. Finally, find one example of a datapoint where the best model makes an error for nouns, verbs, or adjectives, one example per category. What do you think went wrong?"
      ],
      "metadata": {
        "id": "tVm6wIuauAQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_table(reports, tags, metrics=['f1-score'], model_names=['basic', 'markov', 'ar', 'basic-rec', 'markov-rec',  'ar-rec']):\n",
        "    \"\"\"\n",
        "    Reorganise the classification reports of a few models into tables.\n",
        "\n",
        "    reports: dictionary from model name to sklearn's classification report\n",
        "    tags: a list of known tags \n",
        "    metrics: which metrics to get from the classification report (e.g., precision, recall, f1-score)\n",
        "    model_names: which of the reports to use and in which order to display them\n",
        "\n",
        "    Return a tabulated string that you can print.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    metrics = ['f1-score']\n",
        "    for c in tags + ['macro avg', 'weighted avg']:\n",
        "        if c in ['-BOS-', '-PAD-', '-UNK-']:\n",
        "            continue\n",
        "        row = [c]\n",
        "        for metric in metrics:\n",
        "            for model_type in model_names:\n",
        "                if c in reports[model_type]:\n",
        "                    row.append(reports[model_type][c].get(metric, None))                    \n",
        "                else:\n",
        "                    row.append(None)\n",
        "        rows.append(row)\n",
        "\n",
        "    headers = ['Class']\n",
        "    \n",
        "    for metric in metrics:\n",
        "        for model_type in model_names:\n",
        "            headers.append(f\"{metric}/{model_type}\")\n",
        "    fmt = [''] + ['.2f'] * (len(headers)-1)\n",
        "    return tabulate(rows, headers=headers, floatfmt=fmt)"
      ],
      "metadata": {
        "id": "UZnh2qiGxgqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}