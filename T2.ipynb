{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide\n",
        "\n",
        "Check the guide carefully before starting."
      ],
      "metadata": {
        "id": "lTTqtr1rkZlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ILOs\n",
        "\n",
        "After completing this lab you should be able to \n",
        "\n",
        "* understand NBC's factorisation, parameterisation, parameter estimation, and key algorithms\n",
        "* implement a Naive Bayes text classifier\n",
        "* predict and evaluate models using precision/recall\n",
        "\n",
        "**General notes**\n",
        "\n",
        "* In this notebook you are expected to use $\\LaTeX$. \n",
        "* Use python3.\n",
        "* Use NLTK (3.5) to read annotated data.\n"
      ],
      "metadata": {
        "id": "fMgFqQJLktka"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtDuDBZrhgh1"
      },
      "source": [
        "\n",
        "\n",
        "## Table of contents\n",
        "\n",
        "* [Labelled Data](#data)\n",
        "* [Theory](#theory)  \n",
        "    * [Text Classifier](#textcls)\n",
        "    * [Generative Classifier](#gencls)\n",
        "        * [Naive Bayes Classifier](#NBC)\n",
        "        * [Classification](#classification)\n",
        "        * [Parameter Estimation](#MLE)\n",
        "* [Implementation](#code) \n",
        "* [Experiment](#exp) \n",
        "* [Further analysis](#further) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of graded exercises\n",
        "\n",
        "Exercises have equal weights.\n",
        "\n",
        "* [Data](#ex-data)\n",
        "* [NBC](#ex-NBC)\n",
        "* [Grid search](#ex-grid)\n",
        "* [Error analysis](#ex-analysis)"
      ],
      "metadata": {
        "id": "9kt-B1p3kePL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How to use this notebook\n",
        "\n",
        "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
        "* Note that, as always, the notebook recaps theory, and contains solved exercises. While you should probably make use of this theory recap, be careful not to spend disproportionately more time on this than you should. The theory here is more condensed, and should be easier to understand after reading the corresponding prep material for HC2a and after the highlights discussed in class (HC2a).\n",
        "* I recommend you read the theory part before the LC2 session, aim at 1h investment, then work on the coding part during the LC, and then invest another 2h of self-study (for example, to finish up the notebook or to just review it).\n",
        "* The last section is optional for this tutorial, but note that it helps connect T1 and T2. So, you might want to work on it once you are done with the graded exercises.\n"
      ],
      "metadata": {
        "id": "Nzu5O8fZkhXd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp9RNyyRhgh4"
      },
      "source": [
        "# <a name=\"data\"> Labelled Data\n",
        "\n",
        "In this tutorial we will be looking into text classification with labelled data (that is, text that has been categorised or somehow labelled for certain attributes). \n",
        "    \n",
        "This can be binary classification, for example, \n",
        "\n",
        "* `nltk.corpus.sentence_polarity`\n",
        "* `nltk.corpus.movie_reviews`\n",
        "* `nltk.corpus.subjectivity`    \n",
        "    \n",
        "One multiclass classification, for example, \n",
        "* `nltk.corpus.brown`    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOcG72wQhgh4"
      },
      "outputs": [],
      "source": [
        "# You might need to install these:\n",
        "\n",
        "\n",
        "# !pip install pandas\n",
        "# !pip install seaborn\n",
        "# !pip install nltk\n",
        "# !pip install tabulate\n",
        "# !pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmV9U9-7hgh5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ3l6vAPhgh5"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('sentence_polarity')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZIMLagahgh6"
      },
      "source": [
        "It's always good to visualise some properties of the data we will be manipulating. Basic properties of interest are the frequency of the classes, the lenght of the documents in it. \n",
        "\n",
        "Visualising the frequency of classes can warn you that your dataset is unballanced, length of documents can warn you that some datasets can be challenging for you personal computer.\n",
        "\n",
        "We could plot these quantities any way we like, but pandas and seaborn together make it really easy to get insightful visualisations, so we will share this trick with you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNaIOY6-hgh6"
      },
      "outputs": [],
      "source": [
        "def get_corpus_df(corpus, categories=None):\n",
        "    \"\"\"\n",
        "    Return a tall dataframe for a nltk text classification corpus.\n",
        "    corpus: an instance of nltk.corpus meant for text classification\n",
        "    categories: None or a list of categories (in case we want to use just a portion of the labelled documents)\n",
        "    \"\"\"\n",
        "    if categories is None:\n",
        "        categories = corpus.categories()\n",
        "    rows = []\n",
        "    for label in categories:\n",
        "        rows.extend((label, len(x)) for x in corpus.sents(categories=[label]))\n",
        "    return pd.DataFrame(rows, columns=['label', 'length'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VRbDSMghgh7"
      },
      "source": [
        "**Subjectivity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuRNaiUGhgh7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import subjectivity\n",
        "\n",
        "subjectivity_df = get_corpus_df(subjectivity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqD5r3HIhgh7"
      },
      "outputs": [],
      "source": [
        "_ = sns.catplot(y='label', orient='h', kind='count', data=subjectivity_df).set(title=\"subjectivity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WVW7wEihgh7"
      },
      "outputs": [],
      "source": [
        "_ = sns.catplot(x='length', y='label', kind='violin', data=subjectivity_df).set(title=\"subjectivity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6MyiAVuhgh8"
      },
      "source": [
        "Two types of documents (objective and subjective) that are equally represented, and they are reasonably short documents (mostly 10-40 tokens). The distribution of document length is mostly similar for both classes, but the objective class shows a longer tail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF-f_RFAhgh8"
      },
      "source": [
        "It is also a good idea to inspect actual documents in each class. Familiarity with our datasets makes us better researchers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8nmDtNOhgh8"
      },
      "outputs": [],
      "source": [
        "for x, y in zip(subjectivity.sents(categories=['obj']), ['obj'] * 10):\n",
        "    print(y, ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTLe5VR3hgh8"
      },
      "outputs": [],
      "source": [
        "for x, y in zip(subjectivity.sents(categories=['subj']), ['subj'] * 10):\n",
        "    print(y, ' '.join(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK02HJynhgh9"
      },
      "source": [
        "<a name=\"ex-data\"> **Graded Exercise - Data** \n",
        "    \n",
        "Use the same 3 visualisation/inspections techniques as well as 1 more (which you can choose) on the following three corpora:\n",
        "\n",
        "* `nlt.corpus.sentence_polarity`\n",
        "* `nltk.corpus.movie_reviews`\n",
        "* `nltk.corpus.brown`\n",
        "\n",
        "and like we did, make some remarks about what you see. For `brown` which has many classes, you can inspect fewer documents per class, or pick 3-4 of the classes and inspect documents for those only."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTRIBUTE YOUR SOLUTION"
      ],
      "metadata": {
        "id": "PJy0vwRIjEue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HXQUuAthgh9"
      },
      "source": [
        "In this tutorial, **we will focus on sentiment classification** using the `sentence_polarity` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqtlrqAPhgh-"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAxDJvb0hgh-"
      },
      "source": [
        "# <a name=\"theory\"> Theory\n",
        "\n",
        "We suggest you work through this section prior to the live session, so you can make the most of the live session in terms of coding exercises.\n",
        "    \n",
        "This section recaps the theory, if you completed prep-work for HC2a, you could even skip it. \n",
        "\n",
        "Note that this section contains many fact-checking exercises with solution, these can be useful to test whether your initial understanding of the theory and, generally, for self-study. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3mcQZNahgh-"
      },
      "source": [
        "## <a name=\"textcls\"> Text Classifiers\n",
        "\n",
        "In NLP we often want to design systems that can read a piece of text and categorise it as an instance of one of a finite set of classes. We call those systems *text classifiers* (or text categorisers).\n",
        "    \n",
        "It is seldom the case that a piece of text can be reasonably mapped to *a single category*, for various reasons that we have already discussed: language is ambiguous, we often lack context (about the situation, the writer, the reader), and many implicit attributes of text are subjective by nature (e.g., one's opinion about a product). So, in practice we face text classification as a pipeline: we combine a *statistical model* and a *decision rule*.\n",
        "    \n",
        "The statistical model realises the mapping from a given piece of text to a *conditional probability distribution* (cpd) over the categories available. The decision rule uses the information in this distribution to elect a single category that is shown to the user.\n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBH0HHE8hgh_"
      },
      "source": [
        "Consider the case of *sentiment classification* with three sentiment levels (i.e., negative, neutral, and positive). \n",
        "\n",
        "A **probabilistic sentiment classifier** maps a document $x \\in \\mathcal X$ to a probability distribution over the sample space $\\mathcal Y = \\{-1, 0, +1\\}$, where -1 stands for negative, 0 stands for neutral, and +1 stands for positive sentiment. Note that we call $x$ a document, but it could be a sentence, a paragraph, or any granularity we like. A document $x$ is a sequence of words $x=\\langle w_{1}, \\ldots, w_{l} \\rangle$, or $w_{1:l}$ for short, where $l = |x|$ is the sequence length. Each word belongs to a vocabulary $\\mathcal W$ of known words, and the vocabulary size is $V$. \n",
        "The space of all possible documents $\\mathcal X$ is the space of all sequences made of words from $\\mathcal W$, and these sequences can be of any size, a set like that is denoted $\\mathcal W^*$.\n",
        "\n",
        "So, formally, we have a random variable $X$ taking on documents in the set $\\mathcal X$, a random variable $Y$ taking on classes in the set $\\mathcal Y$, and a random variable $W$ taking on words in a vocabulary $\\mathcal W$. A statistical model then establishes a map from any one $x \\in \\mathcal X$ to the cpd $P_{Y|X=x}$.\n",
        "\n",
        "Why does it make sense to see a classifier as a probability distribution? Well, there are a few technical reasons (we are generally very good at learning probability distributions, whereas other types of functions may be more difficult to learn), but here is an intuitive reason: probability distributions capture uncertainty due to the data (e.g., noise), due to the task definition (e.g., the task is subjective), or due to our inability to model the true underlying cognitive processes behind language understanding. See the example below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7Q1CenXhgh_"
      },
      "source": [
        "We may say `no way` \n",
        "* in response to news about a global pandemic, in which case, we probably mean to express a negative sentiment about it;\n",
        "* in response to our friend's startup being listed amongst the top-10 startups in the Netherlands, in which case we probably mean to express a positive sentiment about it;\n",
        "* in response to a classmate asking `will you skip next NTMI's class?`, and in this case `no way` might just be an objective `no`, without a negative or positive sentiment.\n",
        "\n",
        "If all we are given is the text `no way` and no additional context, nor any information about the speaker or the situation, then we cannot really determine the sentiment (not even as humans). This illustrates ambiguity/uncertainty that is inherent to the task/data. The best we can do is to pack a few assumptions into a model, use some data to estimate this model, and then let this model quantify the probability with which `no way` might express `negative`, `neutral`, or `positive` sentiment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVbcmOrhgh_"
      },
      "source": [
        "As we said in the course, the probability of a certain sentiment $y \\in \\{-1, 0, 1\\}$ given a piece of text $x$ is not a real attribute of $x$ in the world. Rather, is is a quantification of the uncertainty that a *model of sentiment analysis* can make given text and given all of the design assumptions we made. \n",
        "\n",
        "\n",
        "So, let's **design** one such model. We will pick sentiment analysis as the example, but other text classification problems fit in the same framework as long as the target classes come from a finite set of disjoint categories (e.g., sentiments, or topics, or types of named-entities, or spam vs not spam, or product categorisation in online shops, etc). Technically such sets are called *countably finite sets*.\n",
        "\n",
        "\n",
        "**Goal** We want a conditional distribution $P_{Y|X=x}$ that, given some piece of text $x$ (e.g., a sentence, a paragraph, or a document), quantifies our model's beliefs in each of the classes in the sample space $\\mathcal Y$.\n",
        "\n",
        "**Challenge** Think about it for a moment, we need a distribution over a finite number of classes, let's say we have $K$ classes. This should not be too difficult right? We have already learnt about the Categorical distribution, so we could start by attempting to use something like \n",
        "\n",
        "\\begin{align}\n",
        "    Y | X=x &\\sim \\mathrm{Cat}(\\boldsymbol\\theta^{(x)}) \\qquad \\text{with }\\boldsymbol\\theta^{(x)} \\in \\Delta_{K-1}\n",
        "\\end{align}\n",
        "\n",
        "That is, for each $x$, we have a $K$-dimensional probability vector $\\boldsymbol\\theta^{(x)}$ where $P_{Y|X}(y|x) = \\theta^{(x)}_y$ is the probability, under this model, that $x$ should be labelled with $y$.\n",
        "Recall that we called this a *tabular* representation of the Categorical cpd, the name comes from the fact that the collection of parameters necessary to prescribe all the Categorical distributions in this definition can be store in something that looks like a table (see the last section of T1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3FyGb6hgiA"
      },
      "source": [
        "**Exercises with solution** What is the problem with attempting to use a tabular representation of the conditional distributions $P_{Y|X}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLim5SS3hgiA"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "\n",
        "The space of all documents $\\mathcal X$ is practically infinite, if every conditioning document $x$ requires its own cpd with its own independent parameter vector, we will need infinitely many parameter vectors. This could never be stored. Even if we knew a clever data structures that can store such object efficiently, we could never estimate infinitely many vectors reliably, since we will only ever observe finitely many data points.\n",
        "    \n",
        "We might tell ourselves, well let's use *only* the documents that we have seen during training. In that case, we would never be able to classify a new document, and analysing new documents is the whole point of text classification in NLP.\n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx_6_xUHhgiA"
      },
      "source": [
        "There are at least two different approaches to this problem. The *generative* approach, which we discuss in *this* notebook, and the *discriminative* approach, which you will encounter in T3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUAPQ5GahgiA"
      },
      "source": [
        "## <a name=\"gencls\"> Generative Classifiers\n",
        "\n",
        "A generative classifier is built upon a *joint distribution* $P_{YX}$ over the joint sample space $\\mathcal Y \\times \\mathcal X$. We can use this model to *infer* a distribution $P_{Y|X=x}$ over target classes for any given $X=x$, and we can do so *on demand* (that is, whenever we observe a certain document $x$) without ever storing (and thus never having to estimate) one probability value for each and every possible pair $(x, y) \\in \\mathcal X \\times \\mathcal Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vixY69MRhgiB"
      },
      "source": [
        "**Exercise with solution** Suppose $\\mathcal X$ is the space of all documents (all of them, no matter the length). Suppose $\\mathcal Y = \\{-1, 0, 1\\}$. Explain in words what the joint sample space $\\mathcal X \\times \\mathcal Y$ consists of and give 2 examples of outcomes in it. How large this space is?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV4yhcN0hgiB"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "The joint sample space is made of the cross product of the two sets. As the set of all documents is infinite, the joint sample space is also infinite. The elements in it are pairs of the kind $(x, y)$ where $x\\in \\mathcal X$ is a document, and $y \\in \\mathcal Y$ is one of $3$ categories. Here are some examples of outcomes in it\n",
        "    \n",
        "* (`i love dogs`, +1)\n",
        "* (`i love dogs`, 0)\n",
        "* (`i love dogs`, -1) \n",
        "    \n",
        "Note that all outcomes are in there, whether or not they are plausible, even something as unreasonable as (`cats are better than dogs`, +1) is in there, as well as ill-formed sentences (`i am i am i`, +1).\n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1olSj1gjhgiB"
      },
      "source": [
        "It should not be obvious that using a joint distribution $P_{YX}$ rather than a collections of cpds $P_{Y|X}$ will make the problem any simpler. In fact, if we were to store one probability value per assignment $(X=x, Y=y)$, we would end up with the exact same problem as before (the tables would be much too large, we would be able to store them, and we would be able to estimate them effectively). \n",
        "\n",
        "But, a joint distribution, as it turns out, gives us an opportunity: to make simplifying assumptions about how the probability of an assignment *decomposes* in terms of probabilities of outcomes simpler than whole documents. The technical term here is **factorisation**, we will *factorise* the probability of $(X=x, Y=y)$ using cpds involving simpler variables (or rather, variables whose sample spaces are *finite* and hopefully much smaller).\n",
        "\n",
        "<details>\n",
        "    <summary><b>Factorising</b></summary>\n",
        "    Factorise means to decompose using a product. You've done this before. For example, we can factorise the expression $ab + ac$ into $a\\times (b +c)$.\n",
        "    \n",
        "You have also factorised in terms of pre-specified factors, for example, factorise 12 using prime numbers: $12 = 2\\times2\\times3$.\n",
        "    \n",
        "This is more or less what we will be doing: we will define some elementary *factors* and express probability values using those simpler factors. For us, the factors themselves are probability values, but they concern events from a smaller event space. The rules that allow us to recombine the simpler factor in order to answer whatever probability query are the rules of probability (i.e., chain rule, conditional probability, and marginal probability).\n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n",
        "To specify a joint distribution that's tractable to represent and estimate, we start with the **chain rule**. \n",
        "\n",
        "**Joint distribution (first attempt)** \n",
        "\n",
        "\\begin{align}\n",
        "    P_{YX}(y, x) &= P_Y(y)P_{X|Y}(x|y)\n",
        "\\end{align}\n",
        "\n",
        "This first attempt factors the probability of $y$ first, and then the probability of generating $x$ conditioned on $y$ being its label. This may seem counterintuitive at first, but imagine given that $y=-1$ you may indeed generate text that's very particular to negative sentiment, and not all too similar to what you would generate had $y$ been $+1$. Of course, there are exceptions to this, like our illustration with `no way`, but the intuition is still fairly reasonable. \n",
        "\n",
        "This factorisation does not *really* nail the problem just yet, but not that it does achieve something interesting. Whereas there are infinitely many cpds of the kind $P_{Y|X=x}$, one for each possible value of $x \\in \\mathcal X$, there are only $K$ cpds of the kind $P_{X|Y=y}$, one for each possible value of $y \\in \\mathcal Y$. If we can figure out a way to deal with the fact that the sample space of $P_{X|Y=y}$ is infinite (that is, we can generate any one of an inifinite set of possible documents), we should be able to get to a feasible design."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KrrPmcPhgiC"
      },
      "source": [
        "We achieved something here: we only need to specify $|\\mathcal Y|$ cpds. But we are still stuck with the fact that a tabular representation for each one of the $K$ cpds of $P_{X|Y=y}$ would remain a very large object.\n",
        "\n",
        "This time the *complex* outcome is on the generating side of the probability distribution (not on the conditioning side), so let's see if chain rule can help us once more:\n",
        "\n",
        "\\begin{align}\n",
        "    P_{X|Y}(w_{1:l}|y) &= \\prod_{i=1}^l P_{W|YH}(w_i|y, w_{<i})\n",
        "\\end{align}\n",
        "\n",
        "where the random variable $W$ denotes a random word and the random variable $H$ denotes the history of random words from left-to-right (by assumption we go from left-to-right, but any order is valid under chain rule). The notation $w_{<i}$ denotes the sequence of words before the $i$th word.\n",
        "\n",
        "This application of chain rule does a good thing and a bad thing. First, it simplifies the generating side (each factor now is responsible for assigning probability to 1 word in context), which is good for the sample space $\\mathcal W$ is finite (it has $V$ words in it). But, it complicates the conditioning side again by pushing the history into it, and the problem with the history is that it's potentially very long (and it grows longer as we advance $i$ along the document). So now again we have to estimate a very large number of cpds: one for each possible value of the conditioning context $(y, w_{<i})$ in $P_{W|YH}$.\n",
        "\n",
        "It looks like we cannot avoid having to deal with a very large space of options. And indeed, *without* making simplifying assumptions we cannot. \n",
        "\n",
        "But, these two applications of chain rule did achieve something remarkable, they show us that we can express the probability value of a labelled document $P_{YX}(y, x)$ using a product of other probability values, each probability value for a random variable with finite sample space.\n",
        "\n",
        "\\begin{align}\n",
        "    P_{YX}(y, w_{1:l}) &= P_Y(y) \\times \\prod_{i=1}^l P_{W|YH}(w_i|y, w_{<i})\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPHTlpL9hgiC"
      },
      "source": [
        "**Exercise with solution** What simplifying assumption could we make about how words depend on one another in a document in order to simplify the conditional factors $P_{X|YH}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIS6HXr0hgiC"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "We used chain rule to break the complex piece of text into smaller pieces, now we can make conditional independence assumptions to make these smaller pieces themselves simpler cpds. \n",
        "    \n",
        "For example, let's make the conditional independence assumption that any two words are independent of one another given the class.\n",
        "    \n",
        "Mathematically this is expressed as $W_i \\perp W_j \\mid Y=y$ for $i \\neq j$ in $X=w_{1:l}$. \n",
        "    \n",
        "And the factorisation becomes:    \n",
        "    \n",
        "\\begin{align}\n",
        "    P_{YX}(y, w_{1:l}) &\\overset{\\text{ind.}}{=} P_Y(y) \\prod_{i=1}^l \\underbrace{P_{W|Y}(w_i|y)}_{\\text{simpler}}\n",
        "\\end{align}\n",
        "    \n",
        "*Remark I.* If you have a hard time understanding this bit, have a look at the prep-work for HC2a, you might have missed some important background there.\n",
        "    \n",
        "*Remark II.* We use $\\overset{\\text{ind.}}{=}$ when an equality only holds under some independence assumption. This is different from chain rule, for example, which always holds.  \n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yu_8k1BhgiC"
      },
      "source": [
        "So far so good, but the original problem was to assign a distribution over classes given a document. Perhaps you can already see how the joint distribution above will help us get to the conditional we wanted all along. If not, don't worry, read on :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW628uqOhgiC"
      },
      "source": [
        "### <a name=\"NBC\"> Naive Bayes Classifier\n",
        "    \n",
        "The naive Bayes classifier (NBC) is called that way because it makes a strong conditional independence assumption (that's the 'naive' part of the name) and because it uses Bayes rule to infer the conditional probability $P_{Y|X}(y|x)$ from the joint distribution $P_{YX}$.\n",
        "\n",
        "    \n",
        "NBC factorises the **joint probability** of $(y, x)$ as:\n",
        "    \n",
        "\\begin{align}\n",
        "    P_{YX}(y, w_{1:l}) &\\overset{\\text{ind.}}{=} P_Y(y) \\prod_{i=1}^l P_{W|Y}(w_i|y)\n",
        "\\end{align}\n",
        "\n",
        "The simpler factor $P_Y$ is modelled as a Categorical distribution over classes.\n",
        "The other factors, $P_{W|Y=y}$ are each a Categorical cpd devined over $V$ possible words, so $P_{W|Y}$ is a collection of $K$ Categorical cpds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oyT5FnZhgiD"
      },
      "source": [
        "Here is the complete generative story as well as the parameterisation in terms of Categorical cpds:\n",
        "    \n",
        "1. Sample a target class from a prior distribution: $Y \\sim \\mathrm{Cat}(\\phi_1, \\ldots, \\phi_K)$\n",
        "2. Condition on $y$, and sample a word for each position $i$ from the class-conditioned distribution $W|Y=y \\sim \\mathrm{Cat}(\\boldsymbol\\pi^{(y)})$ with $\\boldsymbol\\pi^{(y)} \\in \\Delta_{V-1}$ until you draw a stop symbol (i.e., end-of-sequence or EOS).\n",
        "\n",
        "\n",
        "Concretely, this is the probability value that the model assigns to a pair $(y, x)$:\n",
        "    \n",
        "\\begin{align}\n",
        "    P_{YX}(y, x) &\\overset{\\text{NBC}}{=} \\mathrm{Cat}(y|\\phi_{1:C}) \\prod_{i=1}^l \\mathrm{Cat}(w_i|\\pi^{(y)}_{1:v}) \\\\\n",
        "    &= \\phi_y \\prod_{i=1}^l \\pi^{(y)}_{w_i}\n",
        "\\end{align}\n",
        "\n",
        "<details>\n",
        "    <summary><b>Note on meaning of generative story</b></summary>\n",
        "    \n",
        "If the idea of a \"generative story\" confuses you, don't worry you are not alone. Let's try and clarify that. The story tells how we *assume* the observations came about, it does not tell us how the observations did come about.\n",
        "    \n",
        "We did not actually drew samples from this distribution and ended up with a dataset filled with perfectly valid labelled documents, rather, real people labelled documents and we just downloaded a digital version of their hardwork.\n",
        "    \n",
        "But, now that we have a story that could have generated the data, in the sense that the data are in the support of the joint probability distribution we describe, we can assess the probability with which we would have generated the observed data. This quantity has practical uses, for example, in maximum likelihood estimation we use this quantity to try and find the parameters $\\boldsymbol\\phi$ and $\\boldsymbol\\pi$ that maximise the probability of this model ever generating the observed data.\n",
        "    \n",
        "</details>    \n",
        "\n",
        "\n",
        "<details>\n",
        "    <summary><b>What's an end-of-sequence symbol?</b></summary>\n",
        "    \n",
        "As we shall see in the course, models based on generative stories often need a criterion to \"stop\" their generation process. This is relevant after training, when we sometimes use the model to generate data; and it is also relevant for training, in that this criterion may help the model learn something useful (for example, the length of a document or something like that). \n",
        "    \n",
        "The end-of-sequence symbol is a special symbol that's added to the vocabulary and used to mark the end of a sentence. Common choices are `</s>` and `-EOS-` for these sequences are unlikely to belong to the vocabulary of any language, but you can choose anything you like (as long as it is not a word in your dataset).\n",
        "\n",
        "Before we estimate the parameters of our model, we make sure to add this EOS symbol to the end of our sequences. In the future we may need other symbols, such as BOS (begin-of-sequence), etc.\n",
        "    \n",
        "</details>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjwl_YcChgiD"
      },
      "source": [
        "**Exercise with solution** Express the probability that the NBC assigns to (`no way`, +1) in terms of the parameters of the model? How about (`no , no way`, -1)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPu50cV7hgiD"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "\n",
        "*Advice.* Read the question carefully. Oftentimes people start trying to compute the MLE solution, and they get confused because we gave them no data. The question is not about parameter estimation. Rather, the question asks you to express the probability of those data points in terms of the parameters of the model (the numerical values of those parameters and where they come from is not important for this). \n",
        "    \n",
        "First, we pad the document with EOS symbol, i.e., we treat the document as `no way EOS`, then we plug into the formula for NBC getting \n",
        "    \n",
        "\\begin{equation}\n",
        "    \\phi_{+1} \\times \\pi^{(+1)}_{\\text{no}} \\times \\pi^{(+1)}_{\\text{way}} \\times \\pi^{(+1)}_{\\text{EOS}}\n",
        "\\end{equation}\n",
        "    \n",
        "It's similar for the other case,     \n",
        "\\begin{equation}\n",
        "    \\phi_{-1} \\times \\left(\\pi^{(-1)}_{\\text{no}} \\right)^2 \\times \\pi^{(-1)}_{\\text{,}} \\times \\pi^{(-1)}_{\\text{way}} \\times \\pi^{(-1)}_{\\text{EOS}}\n",
        "\\end{equation}    \n",
        "but note that we use a different cpd, this time conditioning on -1 rather than +1, we also have the probability of the comma (given -1), and the probability of `no` (given -1) factors in twice.     \n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhf2eG7MhgiD"
      },
      "source": [
        "**Exercise with solution** Can we use the NBC to assign probability to a document $x$ for which we do not know the class? If so, explain how and compute the probability of `no way`, make sure to express it in terms of the parameters of the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vhWEflZhgiE"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "The NBC is a joint distribution $P_{YX}$, if we are given only one outcome $X=x$, and not the other, we can always use *marginalisation* to compute $P_{X}(x)$:\n",
        "    \n",
        "\\begin{align}\n",
        "    P_X(x) &= \\sum_{y \\in \\mathcal Y} P_{YX}(y, x) \\\\\n",
        "    &=  \\sum_{y \\in \\mathcal Y} P_{Y}(y) \\prod_{i=1}^l P_{W|Y}(w_i|y) \\\\\n",
        "    &=  \\sum_{y \\in \\mathcal Y} \\phi_{y} \\prod_{i=1}^l \\pi^{(y)}_{w_i}\n",
        "\\end{align}\n",
        "    \n",
        "Then the probability of `no way` is\n",
        "\\begin{align}\n",
        "    P_X(\\langle \\text{no}, \\text{way}, \\text{EOS} \\rangle) \n",
        "    &= \\phi_{-1} \\times \\pi^{(-1)}_{\\text{no}} \\times \\pi^{(-1)}_{\\text{way}} \\times \\pi^{(-1)}_{\\text{EOS}} \\\\\n",
        "    &+ \\phi_{0} \\times \\pi^{(0)}_{\\text{no}} \\times \\pi^{(0)}_{\\text{way}} \\times \\pi^{(0)}_{\\text{EOS}} \\\\\n",
        "    &+\\phi_{+1} \\times \\pi^{(+1)}_{\\text{no}} \\times \\pi^{(+1)}_{\\text{way}} \\times \\pi^{(+1)}_{\\text{EOS}}\n",
        "\\end{align}\n",
        "    \n",
        "\n",
        "**Warning** If this exercise was challenging, check out the prep-work for HC2a, in particular, the introduction to probabilistic graphical models.\n",
        "   \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvsxAMR4hgiE"
      },
      "source": [
        "**Exercise with solution** How do we obtain a cpd $P_{Y|X=x}$ given some input text? Give the expression in terms of parameters of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGVIqyEXhgiE"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "Via *Bayes rule*!\n",
        "    \n",
        "\\begin{align}    \n",
        "    P_{Y|X}(y|x) &= \\frac{P_{YX}(y, x)}{P_{X}(x)} \\\\\n",
        "    &= \\frac{P_Y(y)\\prod_{i=1}^l P_{W|Y}(w_i|y)}{P_{X}(x)} \\\\\n",
        "\\end{align}    \n",
        "\n",
        "Note that we can compute all terms in this formula: $P_Y(y)$ is $\\phi_y$, $P_{W|Y}(w_i|y)$ is $\\pi^{(y)}_{w_i}$, and we've just solved $P_X(x)$ in the previous exercise.\n",
        "   \n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEpYaZtthgiE"
      },
      "source": [
        "### <a name=\"classification\"> Classification\n",
        "\n",
        "How do we actually classify using the NBC? That's a fair question, and if you solved the previous quiz, you sort of already found the answer.\n",
        "\n",
        "The algorithm that maps from a probability distribution $P_{Y|X=x}$ to a single outcome is called a **decision rule**, literally because it is a made up rule that makes decisions for us. The most popular decision rule in text classification is the *most probable class* rule, which classifies a document $x$ with the label that receives highest probability under the model. \n",
        "\n",
        "Essentially, when we are given a new document $x_*$, we can solve the following search problem:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "y^* &= \\mathrm{argmax}_{k\\in \\mathcal Y} ~ P_{Y|X}(k|x_*)\n",
        "\\end{align}\n",
        "\n",
        "That is, we are looking for the class $k$ in $\\mathcal Y$ that receives maximum *posterior probability* $P_{Y|X}(k|x_*)$, this is also called the *mode* of the distribution.\n",
        "    \n",
        "In fact, we can solve an even simpler problem if we apply the definition of conditional probabilities:\n",
        "\\begin{align}\n",
        "y^* &= \\mathrm{argmax}_{k\\in \\mathcal Y} ~ \\frac{P_{YX}(k, x_*)}{P_X(x_*)}\n",
        "\\end{align}\n",
        "as we are searching for the value of $k$ while keeping the document fixed, the denominator does not change the result and can be ignored:\n",
        "\\begin{align}\n",
        "y^* &= \\mathrm{argmax}_{k\\in \\mathcal Y} ~ P_{YX}(k, x_*)\n",
        "\\end{align}\n",
        "    \n",
        "This says, the most probable class a-posteriori (that is, after observing the document) is the class for which the joint probability $P_{YX}(k, x_*)$ is highest.\n",
        "\n",
        "We can then substitute in our model definition for the joint probability    \n",
        "\\begin{align}\n",
        "y^* &= \\mathrm{argmax}_{k\\in \\mathcal Y} ~ P_{YX}(k, x_*) \\\\\n",
        "    &= \\mathrm{argmax}_{k\\in \\mathcal Y} ~ P_{Y}(k) \\prod_{i=1}^{l_*} P_{W|Y}(w_i|k) \\\\\n",
        "\\end{align}    \n",
        "and finally, substitute in our model parameters:\n",
        "\\begin{align}    \n",
        "y^*    &= \\mathrm{argmax}_{k\\in \\mathcal Y} ~  \\phi_k \\prod_{i=1}^{l_*} \\pi_{w_i}^{(k)} \\\\\n",
        "\\end{align}\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSB-mCwDhgiF"
      },
      "source": [
        "**Exercise with solution** If $\\phi_{-1}=\\phi_{+1} = \\frac{2}{5}$, $\\phi_0 = \\frac{1}{5}$ and some of the parameters $\\pi^{(y)}$ are given in the rows of the following table\n",
        "\n",
        "| $Y$ | all | the | same | EOS | ... |\n",
        "|-----|-----|-----|------|-----|-----|\n",
        "| -1  | a   | c   | b    | d   | ... |\n",
        "| 0   | a   | c   | 2.5b   | d   | ... |\n",
        "| 1   | a   | c   | b    | d   | ... |\n",
        "\n",
        "What is the most probable label of the document `no freaking way EOS`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wadfd4JOhgiF"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "Let's start by expressing the joint probabilities $P_{YX}(y, \\text{all the same EOS})$ for each possible value of $y$:\n",
        "* $y=-1$: $\\frac{2}{5} \\times a \\times c\\times b \\times d = \\frac{2}{5}abcd$\n",
        "* $y=0$: $\\frac{1}{5} \\times a \\times c \\times 2.5b  \\times d = \\frac{2.5}{5}abcd$\n",
        "* $y=+1$: $\\frac{2}{5} \\times a \\times c \\times b \\times d = \\frac{2}{5}abcd$\n",
        "\n",
        "We should now pick the label that is assigned maximum probability when paired with this document, that would be $y=0$.\n",
        "\n",
        "See that even though we had a prior preference in favour of labels other than neutral, given the evidence in the document, the posterior prefers the neutral label.\n",
        "    \n",
        "</details>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70pzIxukhgiF"
      },
      "source": [
        "### <a name=\"MLE\"> Parameter Estimation\n",
        "\n",
        "We know how NBC factorises, i.e., \n",
        "\n",
        "\\begin{equation}\n",
        "P_{YX}(y, w_{1:l}) = P_Y(y)\\prod_{i=1}^l P_{W|Y}(w_i|y)\n",
        "\\end{equation}\n",
        "\n",
        "we have chosen a parameterisation of our cpds, i.e., \n",
        "\n",
        "\\begin{align}\n",
        "P_{YX}(y, w_{1:l}) &= \\mathrm{Cat}(y|\\phi_{1:K})\\prod_{i=1}^l \\mathrm{Cat}(w_i|\\pi^{(y)}_{1:V}) \\\\\n",
        "&= \\phi_{y} \\prod_{i=1}^l \\pi^{(y)}_{w_i}\n",
        "\\end{align}\n",
        "\n",
        "now we need only estimate the numerical values of the parameters $\\boldsymbol \\phi$ and $\\boldsymbol \\pi$.\n",
        "\n",
        "If we are given a dataset of $N$ labelled observations $\\mathcal D = \\{ (x^{(n)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)}) \\}$, where each document $x^{(n)}$ has size $l_n$, we can do so via maximum likelihood estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6xzh6cohgiF"
      },
      "source": [
        "**Exercise with solution** What is the MLE for $\\phi_{y}$ for some $y$ in $\\mathcal Y$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdScX2ROhgiF"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "    \n",
        "\\begin{align}\n",
        "\\phi_y = \\frac{\\mathrm{count}_{Y}(y)}{\\sum_{k \\in \\mathcal Y}\\mathrm{count}_Y(k)} = \\frac{\\mathrm{count}_{Y}(y)}{N}\n",
        "\\end{align}\n",
        "    \n",
        "where we use $\\mathcal D$ to determine the counts as shown below\n",
        "    \n",
        "\\begin{equation}\n",
        "    \\mathrm{count}_Y(k) = \\sum_{n=1}^N [y^{(n)} = k]\n",
        "\\end{equation}\n",
        "\n",
        "For a *balanced* dataset, that is, a dataset with equal number of labelled instances for each class, $Y \\sim \\mathrm{Cat}(\\phi_{1:K})$ becomes the uniform distribution, that is, $\\phi_y = 1/K$ for all $y \\in \\mathcal Y$.\n",
        "\n",
        "---    \n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10W1YzpfhgiG"
      },
      "source": [
        "**Quiz** What is the MLE for $\\pi^{(y)}_w$ for some $y$ in $\\mathcal Y$ and $w$ in $\\mathcal W$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ufXkcJhgiG"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "    \n",
        "\\begin{align}\n",
        "\\pi^{(y)}_{w} = \\frac{\\mathrm{count}_{YX}(y, w)}{\\sum_{v \\in \\mathcal W}\\mathrm{count}_{YX}(y, v)}\n",
        "\\end{align}\n",
        "    \n",
        "where we use $\\mathcal D$ to determine the counts as shown below\n",
        "    \n",
        "\\begin{equation}\n",
        "    \\mathrm{count}_{YX}(k, v) = \\sum_{n=1}^N \\sum_{i=1}^{l_n} [y^{(n)} = k] \\times [w^{(n)}_{i} = v]\n",
        "\\end{equation}\n",
        "    \n",
        "    \n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyW2efiNhgiG"
      },
      "source": [
        "# <a name=\"code\"> Implementation\n",
        "\n",
        "    \n",
        "We will design a python class that implements NBC, but before we get to code, let's check if you are getting comfortable with the formal presentation of the model (which is in principle enough to answer the following quizzes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wDLjc6ehgiG"
      },
      "source": [
        "**Quiz** What is the time complexity of assessing joint probability $P_{YX}(y, w_{1:l})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlTQAkMthgiG"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "It takes $\\mathcal O(l)$, since we have to assess $P_Y(y)$ once and $P_{W|Y=y}$ once for every word in $w_{1:l}$.\n",
        "\n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCbeu0N-hgiH"
      },
      "source": [
        "**Quiz** What is the time complexity of assessing marginal probability $P_X(w_{1:l})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr2SAquShgiH"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "It takes $\\mathcal O(K \\times l)$, since we need to assess the joint $P_{YX}(y, w_{1:l})$ once for every label in the sample space $\\{1, \\ldots, K\\}$. This is necessary so we can marginalise $Y$ out.\n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2PhngChgiH"
      },
      "source": [
        "**Quiz** What is the storage requirements of the NBC model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-8-x-CihgiH"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "It requires space dominated by $\\mathcal O(K \\times V)$ since we have to store a $K$-dimensional vector $\\phi_{1:K}$ and $K$ class-conditioned cpds, each defined over $V$ words.\n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vlQDkRghgiH"
      },
      "source": [
        "**Quiz** What is the time complexity to classify $w_{1:l}$ with a trained model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuhOPgmhgiH"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "    \n",
        "To classify, we need to assess the joint probability $P_{YX}(y, x_{1:l})$ once for each label in the sample space $\\{1, \\ldots, K\\}$, and then pick the one which is maximum. This takes time $\\mathcal O(K \\times l)$ since each assessment of joint probability is linear in $l$ and finding the maximum in a list is not worse than linear.\n",
        "    \n",
        "---\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koy2L_kihgiI"
      },
      "outputs": [],
      "source": [
        "def validate_categorical_cpd(params: dict):\n",
        "    \"\"\"\n",
        "    Just to remind you a valid Categorical cpd is a parameterised by a vector of positive \n",
        "    numbers that sum to 1.\n",
        "    \n",
        "    We normally think of the parameters as a sequence or a vector, \n",
        "    but in a programming language, it can be useful to use a dict, \n",
        "    with a dict the indexing does not need to be numerical (we could use the label 'name' \n",
        "    or the word itself to index positions in the dictionary).\n",
        "    \n",
        "    In some cases a dict may be considered slow compared to a list or array, \n",
        "    but for the scale of our experiments in the tutorial, it's fine and \n",
        "    conceptually much easier.\n",
        "    \n",
        "    params: map outcome to probability mass\n",
        "    \"\"\"\n",
        "    return all(0 <= p <= 1 for p in params.values()) and np.isclose(sum(p for p in params.values()), 1., 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unKdU2GBhgiI"
      },
      "outputs": [],
      "source": [
        "assert validate_categorical_cpd({'pos': 0.2, 'neg': 0.8})  # fine\n",
        "assert validate_categorical_cpd({'pos': 0.3, 'neg': 0.7})  # fine\n",
        "assert validate_categorical_cpd({'pos': 0.2, 'neu': 0.1, 'neg': 0.7})  # we can have more than 2, that's fine \n",
        "assert validate_categorical_cpd({'pos': 0.2, 'neu': 0.0, 'neg': 0.8})  # we can have 0 probs inside\n",
        "assert not validate_categorical_cpd({'pos': 0.2, 'neg': 0.7})  # not good\n",
        "assert not validate_categorical_cpd({'pos': 0.2})  # not good\n",
        "assert not validate_categorical_cpd({'pos': -0.2, 'neu': 0.2, 'neg': 0.8})  # we cannot have 'negative probs' inside"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46wdJpE7hgiI"
      },
      "source": [
        "We will now begin implementing a little piece at a time all functionalities necessary to get NBC off the floor.\n",
        "\n",
        "As we will be manipulating some tables, the first two functions are already implemented for you to show you how you can use python dictionaries to store tables.\n",
        "\n",
        "\n",
        "**Exercise with solution** study and understand the design of `get_prior_parameter` and `get_cond_parameter` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpIQJ7tdhgiI"
      },
      "outputs": [],
      "source": [
        "def get_prior_parameter(y: str, phi: dict):\n",
        "    \"\"\"\n",
        "    Return P(Y=y) under the model Categorical(phi).\n",
        "    \n",
        "    y: the label\n",
        "    phi: a dictionary that maps from the label to its probability mass\n",
        "        even though mathematically we think of phi as a vector, \n",
        "        in code it can be convenient to treat it like a dictionary some times, \n",
        "        for example, as a dictionary we can use labels that are strings (rather than 0-based indices)\n",
        "        \n",
        "        For this implementation assume that phi has already been validated as a Categorical parameter.\n",
        "        \n",
        "    Return: Categorical(y|\\phi)\n",
        "    \"\"\"\n",
        "    # **SOLUTION**\n",
        "    return phi.get(y, 0)  # if an outcome is outside the support, it gets 0 mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii0IbstlhgiI"
      },
      "outputs": [],
      "source": [
        "assert get_prior_parameter('pos', {'pos': 0.2, 'neg': 0.8}) == 0.2\n",
        "assert get_prior_parameter('neg', {'pos': 0.3, 'neg': 0.7}) == 0.7\n",
        "assert get_prior_parameter('neu', {'pos': 0.3, 'neg': 0.7}) == 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfK4RLBvhgiJ"
      },
      "outputs": [],
      "source": [
        "def get_cond_parameter(y: str, w: str, pi: dict, UNK=None):\n",
        "    \"\"\"\n",
        "    Return P(W=w|Y=y) under the model Categorical(\\pi^{(y)}).\n",
        "    \n",
        "    y: the label\n",
        "    w: word\n",
        "    pi: a dictionary of dictionaries \n",
        "        first we can index it using a label to obtain a dict as return, \n",
        "        the latter dict are the parameters for a distribution over the vocabulary given the label\n",
        "        and it can be indexed using a word to obtain a probability mass\n",
        "\n",
        "        even though mathematically we think of pi as a table/matrix\n",
        "        in code it can be convenient to treat it like a dictionary some times, \n",
        "        for example, as a dictionary we can use labels that are strings (rather than 0-based indices)\n",
        "        and words that are strings (rather than 0-based indices to a vocabulary).        \n",
        "        \n",
        "        For this implementation assume that every dict inside of pi\n",
        "         has already been validated as a Categorical parameter.\n",
        "    UNK: reserved for future use\n",
        "        \n",
        "    Return: Categorical(w|\\phi^{(y)})\n",
        "    \"\"\"\n",
        "    # **SOLUTION**\n",
        "    cpd = pi.get(y, None)\n",
        "    if cpd is None:  # the label is not in the support of the model\n",
        "        return 0. \n",
        "    return cpd.get(w, 0.)  # if a word is outside the support, it gets 0 mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4qkhmcchgiJ"
      },
      "outputs": [],
      "source": [
        "# A test case for us, the cpds in it are valid as we can see\n",
        "# (do not change this test case, it will be used in other cells)\n",
        "test_phi_1 = {'pos': 0.6, 'neg': 0.4}\n",
        "test_pi_1 = {\n",
        "    'pos': {'not': 0.05, 'so': 0.1, 'good': 0.5, 'bad': 0.15, 'okay': 0.2 }, \n",
        "    'neg': {'not': 0.05, 'so': 0.1, 'good': 0.1, 'bad': 0.5, 'okay': 0.25 }\n",
        "}\n",
        "assert validate_categorical_cpd(test_phi_1)\n",
        "assert all(validate_categorical_cpd(cpd) for y, cpd in test_pi_1.items())\n",
        "\n",
        "# Fine\n",
        "assert get_cond_parameter('pos', 'good', test_pi_1) == 0.5\n",
        "assert get_cond_parameter('pos', 'not', test_pi_1) == 0.05\n",
        "assert get_cond_parameter('pos', 'so', test_pi_1) == 0.1\n",
        "assert get_cond_parameter('pos', 'bad', test_pi_1) == 0.15\n",
        "assert get_cond_parameter('pos', 'okay', test_pi_1) == 0.2\n",
        "\n",
        "# Fine\n",
        "assert get_cond_parameter('neg', 'good', test_pi_1) == 0.1\n",
        "assert get_cond_parameter('neg', 'not', test_pi_1) == 0.05\n",
        "assert get_cond_parameter('neg', 'so', test_pi_1) == 0.1\n",
        "assert get_cond_parameter('neg', 'bad', test_pi_1) == 0.5\n",
        "assert get_cond_parameter('neg', 'okay', test_pi_1) == 0.25\n",
        "\n",
        "# Not in the support\n",
        "assert get_cond_parameter('neu', 'okay', test_pi_1) == 0.\n",
        "assert get_cond_parameter('pos', 'strange', test_pi_1) == 0.\n",
        "assert get_cond_parameter('neg', 'strange', test_pi_1) == 0.\n",
        "assert get_cond_parameter('neu', 'strange', test_pi_1) == 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inh4L3qhgiK"
      },
      "source": [
        "**Exercise with solution** Impement a function to return $\\log P_Y(y)$ for a label $y \\in \\mathcal Y$ under this model. Your function should use the functionality `get_prior_parameter` and/or `get_cond_parameter` from earlier. Assume the model parameters are already available. \n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5HP0Yj3hgiK"
      },
      "outputs": [],
      "source": [
        "def log_prior_prob(y: str, phi: dict):\n",
        "    \"\"\"Return log P(Y=y) under the model\"\"\"\n",
        "    raise NotImplemented(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biKjsaZFhgiK"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "def log_prior_prob(y: str, phi: dict):\n",
        "    \"\"\"Return log P(Y=y) under the model\"\"\"\n",
        "    # **SOLUTION**\n",
        "    return np.log(get_prior_parameter(y, phi))    \n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8avNn8mhgiK"
      },
      "outputs": [],
      "source": [
        "# Your code should pass these tests\n",
        "assert log_prior_prob('pos', {'pos': 0.2, 'neg': 0.8}) == np.log(0.2)\n",
        "assert log_prior_prob('neg', {'pos': 0.3, 'neg': 0.7}) == np.log(0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EHZYw6NhgiK"
      },
      "source": [
        "**Exercise with solution** Impement a function to return $\\log P_{X|Y}(x|y)$ for a label $y \\in \\mathcal Y$  and document $x \\in \\mathcal X$ under this model. Your function should use the functionality `get_prior_parameter` and/or `get_cond_parameter` from earlier. Assume the model parameters are already available. \n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1c4kPCLhgiL"
      },
      "outputs": [],
      "source": [
        "def log_conditional_prob(x: list, y: str, pi: dict):\n",
        "    \"\"\"Return log P(X=x|Y=y)\"\"\"\n",
        "    raise NotImplemented(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xcYEeXRhgiL"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "def log_conditional_prob(x: list, y: str, pi: dict):\n",
        "    \"\"\"Return log P(X=x|Y=y) under the model\"\"\"\n",
        "    # **SOLUTION**\n",
        "    # named arguments help us not make mistakes (for example, flipping y and w accidentaly)\n",
        "    return sum(np.log(get_cond_parameter(y=y, w=w, pi=pi)) for w in x)      \n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJNBo56nhgiL"
      },
      "outputs": [],
      "source": [
        "# Your code should pass these tests\n",
        "\n",
        "# Fine\n",
        "assert np.isclose(log_conditional_prob(\"not bad\".split(), 'pos', test_pi_1), np.log(0.05 * 0.15), 1e-3)\n",
        "assert np.isclose(log_conditional_prob(\"not good\".split(), 'pos', test_pi_1), np.log(0.05 * 0.5), 1e-3)\n",
        "assert np.isclose(log_conditional_prob(\"good\".split(), 'pos', test_pi_1), np.log(0.5), 1e-3)\n",
        "\n",
        "assert np.isclose(log_conditional_prob(\"not bad\".split(), 'neg', test_pi_1), np.log(0.05 * 0.5), 1e-3)\n",
        "assert np.isclose(log_conditional_prob(\"not good\".split(), 'neg', test_pi_1), np.log(0.05 * 0.1), 1e-3)\n",
        "assert np.isclose(log_conditional_prob(\"good\".split(), 'neg', test_pi_1), np.log(0.1), 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYd8sV_ThgiL"
      },
      "source": [
        "A good code should also deal with cases that *are not* in the support of the joint distribution. Those should have 0 probability for now (later we discuss a technique to avoid 0 probs). The log of 0 is -inf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZfP5eyMhgiL"
      },
      "outputs": [],
      "source": [
        "# Not in the support (should produce warnings, but pass the test)\n",
        "assert log_prior_prob('neu', {'pos': 0.3, 'neg': 0.7}) == -np.inf\n",
        "assert log_conditional_prob(\"not that bad\".split(), 'pos', test_pi_1) == -np.inf\n",
        "assert log_conditional_prob(\"not that bad\".split(), 'neg', test_pi_1) == -np.inf\n",
        "assert log_conditional_prob(\"not okay\".split(), 'neu', test_pi_1) == -np.inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvpun40rhgiM"
      },
      "source": [
        "**Exercise with solution** Impement a function to return $\\log P_{YX}(y, x)$ for a label $y \\in \\mathcal Y$  and document $x \\in \\mathcal X$ under this model. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` from earlier. Assume the model parameters are already available. \n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ_9K0DnhgiM"
      },
      "outputs": [],
      "source": [
        "def log_joint_prob(y: str, x: list, phi: dict, pi: dict):\n",
        "    \"\"\"Return log P(Y=y, X=x) under the model\"\"\"\n",
        "    raise NotImplemented(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQIuPxwBhgiM"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "def log_joint_prob(y: str, x: list, phi: dict, pi: dict):\n",
        "    \"\"\"Return log P(Y=y, X=x) under the model\"\"\"\n",
        "    # **SOLUTION**\n",
        "    # named arguments help us not make mistakes (eg, flipping x and y in the conditional)\n",
        "    return log_prior_prob(y, phi) + log_conditional_prob(x=x, y=y, pi=pi)\n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bShgSO8hgiM"
      },
      "outputs": [],
      "source": [
        "# Your code should pass these tests\n",
        "\n",
        "assert np.isclose(\n",
        "    log_joint_prob(y='pos', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.6 * 0.05 * 0.15), \n",
        "    1e-3\n",
        ")\n",
        "assert np.isclose(\n",
        "    log_joint_prob(y='pos', x=\"not good\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.6 * 0.05 * 0.5), \n",
        "    1e-3\n",
        ")\n",
        "\n",
        "\n",
        "assert np.isclose(\n",
        "    log_joint_prob(y='neg', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.4 * 0.05 * 0.5), \n",
        "    1e-3\n",
        ")\n",
        "assert np.isclose(\n",
        "    log_joint_prob(y='neg', x=\"not good\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.4 * 0.05 * 0.1), \n",
        "    1e-3\n",
        ")\n",
        "\n",
        "# NBC is not sensitive to the order of words within the sentence\n",
        "\n",
        "assert np.isclose(\n",
        "    log_joint_prob(y='pos', x=\"bad not\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.6 * 0.05 * 0.15), \n",
        "    1e-3\n",
        ")\n",
        "\n",
        "assert np.isclose(\n",
        "    log_joint_prob(y='neg', x=\"good not\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.4 * 0.05 * 0.1), \n",
        "    1e-3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owmc7QchhgiN"
      },
      "source": [
        "**Exercise with solution** Impement a function to return $\\log P_{X}(x)$ for a document $x \\in \\mathcal X$ under this model. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` and/or `log_joint_prob` from earlier. Assume the model parameters are already available. \n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNxoCQTWhgiN"
      },
      "outputs": [],
      "source": [
        "def log_marginal_prob(x: list, phi: dict, pi: dict):\n",
        "    \"\"\"Return log P(X=x) under the model\"\"\"    \n",
        "    raise NotImplemented(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twW0gPadhgiN"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "def log_marginal_prob(x: list, phi: dict, pi: dict):\n",
        "    \"\"\"Return log P(X=x) under the model\"\"\"\n",
        "    # **SOLUTION**\n",
        "    # Tips:\n",
        "    # * compute the joint probability for x and each of the possible labels\n",
        "    #   marginalise (sum)\n",
        "    #   compute log\n",
        "    # * or, compute all of it in log space using log_joint_prob\n",
        "    #   and np.logaddexp.reduce\n",
        "    \n",
        "    # for the marginalisation, we need the support of the rv Y, \n",
        "    # we can find it within the dictionary phi (ie, the set of keys in it)\n",
        "    support_Y = phi.keys()\n",
        "    # the function logaddexp(a, b) returns log(exp(a) + exp(b))\n",
        "    # the method reduce applies that in a row to a whole list of values \n",
        "\n",
        "    return np.logaddexp.reduce([log_joint_prob(y=y, x=x, phi=phi, pi=pi) for y in support_Y]) \n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CBcZstshgiN"
      },
      "outputs": [],
      "source": [
        "# Your code should pass these tests\n",
        "\n",
        "assert np.isclose(\n",
        "    log_marginal_prob(x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.6 * 0.05 * 0.15 + 0.4 * 0.05 * 0.5), \n",
        "    1e-3\n",
        ")\n",
        "assert np.isclose(\n",
        "    log_marginal_prob(x=\"not good\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log(0.6 * 0.05 * 0.5 + 0.4 * 0.05 * 0.1), \n",
        "    1e-3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o28Huus7hgiN"
      },
      "source": [
        "**Exercise with solution** Impement a function to return $\\log P_{Y|X}(y|x)$ for a label $y\\in \\mathcal Y$ and document $x \\in \\mathcal X$ under this model. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` and/or `log_joint_prob` and/or `log_marginal_prob` from earlier. Assume the model parameters are already available. \n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XFbw3GAhgiO"
      },
      "outputs": [],
      "source": [
        "def log_posterior_prob(y: str, x: list, phi: dict, pi: dict):\n",
        "    \"\"\"Return log P(Y=y|X=x) under the model\"\"\"\n",
        "    raise NotImplemented(\"Implement me !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP8054MShgiO"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "def log_posterior_prob(y: str, x: list, phi: dict, pi: dict):\n",
        "    \"\"\"Return log P(Y=y|X=x) under the model\"\"\"\n",
        "    # **SOLUTION**\n",
        "    \n",
        "    \n",
        "    # The posterior probability is a conditional probability\n",
        "    #  by definition that is P(Y=y,X=x) / P(X=x)\n",
        "    #  and we have already implemented the joint probability in the numerator\n",
        "    log_joint_prob = log_joint_prob(y=y, x=x, phi=phi, pi=pi)\n",
        "    #  and the marginal probability in the denominator\n",
        "    log_marginal_prob = log_marginal_prob(x, phi=phi, pi=pi)\n",
        "    # As we are in computing it in log space, we can subtract the logs to divide the probs\n",
        "    return log_joint_prob - log_marginal_prob\n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rti572e-hgiO"
      },
      "outputs": [],
      "source": [
        "# Your code should pass these tests\n",
        "\n",
        "assert np.isclose(\n",
        "    log_posterior_prob(y='pos', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log((0.6 * 0.05 * 0.15)/(0.6 * 0.05 * 0.15 + 0.4 * 0.05 * 0.5)), \n",
        "    1e-3\n",
        ")\n",
        "\n",
        "assert np.isclose(\n",
        "    log_posterior_prob(y='neg', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
        "    np.log((0.4 * 0.05 * 0.5)/(0.6 * 0.05 * 0.15 + 0.4 * 0.05 * 0.5)), \n",
        "    1e-3\n",
        ")\n",
        "\n",
        "# The posterior is a distribution, thus it must add to one if we sum it over the possible labels:\n",
        "assert np.isclose(\n",
        "    np.exp(log_posterior_prob(y='pos', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1)) \n",
        "    + np.exp(log_posterior_prob(y='neg', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1)), \n",
        "    1, \n",
        "    1e-3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Mcr_YphgiO"
      },
      "source": [
        "**Exercise with solution** Impement a function to return the most probable class under this model for a given document $x \\in \\mathcal X$. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` and/or `log_joint_prob` and/or `log_marginal_prob` from earlier. Assume the model parameters are already available. \n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfc9Ozm3hgiO"
      },
      "outputs": [],
      "source": [
        "def classify(x: list, phi: dict, pi: dict):\n",
        "    \"\"\"\n",
        "    Classify x using the most probable label a-posteriori.\n",
        "    \n",
        "    x: a single document to be classified (a list of tokens)\n",
        "    phi: parameters of the prior\n",
        "    pi: parameters of the class-conditioned distributions over vocabulary\n",
        "    \n",
        "    Return the most probable label for the document\n",
        "    \"\"\"\n",
        "    raise NotImplemented(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rkeTFBqhgiO"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "def classify(x: list, phi: dict, pi: dict):\n",
        "    \"\"\"\n",
        "    Classify x using the most probable label a-posteriori.\n",
        "    \n",
        "    x: a single document to be classified (a list of tokens)\n",
        "    phi: parameters of the prior\n",
        "    pi: parameters of the class-conditioned distributions over vocabulary\n",
        "    \n",
        "    Return the most probable label for the document\n",
        "    \"\"\"\n",
        "    # **EXERCISE**\n",
        "    # Tips\n",
        "    # * for each y, evaluate log p(s, y), then select the one that has highest value\n",
        "    #   note that we have provided a helper method self.log_joint_prob(y, x)\n",
        "\n",
        "    support_Y = list(phi.keys())\n",
        "    best_k = np.argmax([log_joint_prob(y=y, x=x, phi=phi, pi=pi) for y in support_Y])\n",
        "    y_pred = support_Y[best_k]\n",
        "    return y_pred   \n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSUR1IqohgiP"
      },
      "outputs": [],
      "source": [
        "# Your code should pass these tests\n",
        "\n",
        "# documents with single word\n",
        "assert classify(\"good\".split(), phi=test_phi_1, pi=test_pi_1) == 'pos'\n",
        "assert classify(\"bad\".split(), phi=test_phi_1, pi=test_pi_1) == 'neg'\n",
        "\n",
        "# documents with multiple words\n",
        "assert classify(\"so good\".split(), phi=test_phi_1, pi=test_pi_1) == 'pos'\n",
        "assert classify(\"so bad\".split(), phi=test_phi_1, pi=test_pi_1) == 'neg'\n",
        "\n",
        "# NBC is not good to handle modifiers, because it sees documents as unordered collections\n",
        "assert classify(\"not good\".split(), phi=test_phi_1, pi=test_pi_1) == 'pos'\n",
        "assert classify(\"not bad\".split(), phi=test_phi_1, pi=test_pi_1) == 'neg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazzNSrRhgiP"
      },
      "source": [
        "Hopefully it is clear to you that \n",
        "* the model is a probability distribution\n",
        "* the model manipulates simpler probability factors to assign probability to larger documents\n",
        "* the model definition and its parameter estimation are two very different things (so far we have been implementing all sorts of probability queries without ever having to worry about how parameters are estimated -- in fact the parameters have been given to us through `test_phi_1` and `test_pi_1`\n",
        "\n",
        "We can now turn to parameter estimation via MLE, for which I need to give you a dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJguT1BYhgiP"
      },
      "source": [
        "**Exercise with solution** Implement a function that returns a dictionary of prior parameters (something similar to the `test_phi_1` object that we have been using. The numerical values of the parameters should be given by maximum likelihood estimation using a dataset of labelled documents.\n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcYy4-4ihgiP"
      },
      "outputs": [],
      "source": [
        "def estimate_phi(data_x, data_y):\n",
        "    \"\"\"\n",
        "    Return the dict that stores the parameters \\phi of the prior.\n",
        "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
        "    data_y: list of labels, each label is a string\n",
        "    \n",
        "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
        "        \n",
        "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
        "    \"\"\"\n",
        "    raise NotImplemented(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOK38RZxhgiP"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import chain\n",
        "\n",
        "    \n",
        "def estimate_phi(data_x, data_y):\n",
        "    \"\"\"\n",
        "    Return the dict that stores the parameters \\phi of the prior.\n",
        "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
        "    data_y: list of labels, each label is a string\n",
        "    \n",
        "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
        "        \n",
        "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
        "    \"\"\"\n",
        "    # **SOLUTION**\n",
        "    N = len(data_y)\n",
        "    phi = dict((y, float(n) / N) for y, n in Counter(data_y).items())\n",
        "    return phi \n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWCKe71ChgiQ"
      },
      "outputs": [],
      "source": [
        "# Your code should pass these tests\n",
        "\n",
        "test_data_x_1 = [\n",
        "    \"pretty good\".split(),\n",
        "    \"pretty good\".split(),\n",
        "    \"pretty good\".split(),\n",
        "    \n",
        "    \"quite good\".split(),\n",
        "    \"quite okay\".split(),\n",
        "    \n",
        "    \"so good\".split(),\n",
        "    \"so pretty good\".split(),\n",
        "    \n",
        "    \"bad\".split(),\n",
        "    \"bad bad bad\".split(),\n",
        "    \"so very bad\".split(),    \n",
        "    \"just so bad\".split(),\n",
        "    \"bad just really really bad\".split(),\n",
        "]\n",
        "test_data_y_1 = [\n",
        "    'pos', 'pos', 'pos',\n",
        "    'pos', 'pos',\n",
        "    'pos', 'pos',\n",
        "    'neg', 'neg', 'neg', 'neg', 'neg'\n",
        "]\n",
        "\n",
        "test_result_phi_1 = estimate_phi(test_data_x_1, test_data_y_1)\n",
        "\n",
        "# First of all, we get a proper cpd\n",
        "assert validate_categorical_cpd(test_result_phi_1)\n",
        "\n",
        "# And it contains the probs we expect\n",
        "assert np.isclose(test_result_phi_1['pos'], 7./(7+5), 0.01)\n",
        "assert np.isclose(test_result_phi_1['neg'], 5./(7+5), 0.01)\n",
        "\n",
        "# Let's use our helper code\n",
        "assert get_prior_parameter('neu', test_result_phi_1) == 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8KOixOIhgiQ"
      },
      "source": [
        "**Exercise with solution** Implement a function that returns a dictionary of dictionaries for the parameters of the class-conditioned distributions over vocabulary (something similar to the `test_pi_1` object that we have been using). The numerical values of the parameters should be given by maximum likelihood estimation using a dataset of labelled documents.\n",
        "\n",
        "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
        "\n",
        "Your code should support Laplace smoothing, but you can implement it first without (see you can pass the first set of tests, which does not depend on smoothing) and then try to modify it to use smoothing (and see if you can pass the second set of tests).\n",
        "\n",
        "\n",
        "We provide some helper code, for determining the vocabulary from data (the vocabulary should contain some special symbols, e.g.  end-of-sequence  symbol, and, if Laplace smoothing is on, also a placeholder for future unseen tokens).\n",
        "\n",
        "**Tip.** Students have found the Laplace smoothing procedure tricky in the past, so if you are spending too much time on it because your new to programming, it's okay to just study the solution. That said, try to get the non-smoothed version right, as that one is relatively simple.\n",
        "\n",
        "See API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLszzDgFhgiQ"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "def get_labels(data_y):\n",
        "    \"\"\"Return the set of labels in an observed dataset\"\"\"\n",
        "    return set(data_y)\n",
        "\n",
        "def get_vocabulary(data_x, alpha=0., EOS='-EOS-', UNK='-UNK-'):\n",
        "    \"\"\"\n",
        "    Return the set of known words in an observed collection of documents. \n",
        "    To those we add the EOS symbol which needs to be in the support of the distribution, otherwise\n",
        "     the Naive Bayes model does not have a 'stop criterion'.\n",
        "    And, possibly, a placeholder for future unknown words, when we are using Laplace smoothing.\n",
        "    \"\"\"\n",
        "    vocab = set(chain(*data_x))\n",
        "    vocab.add(EOS) # should always be part of the vocabulary\n",
        "    if alpha > 0.:  # we are reserving mass for future unseen words\n",
        "        vocab.add(UNK)\n",
        "    return vocab\n",
        "\n",
        "def estimate_pi(data_x, data_y, alpha=0., EOS='-EOS-', UNK='-UNK-'):\n",
        "    \"\"\"\n",
        "    Return the dict that stores the parameters of the class-conditioned distributions over vocabulary.\n",
        "    \n",
        "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
        "    data_y: list of labels, each label is a string\n",
        "    \n",
        "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
        "    alpha: the Laplace smoothing coefficient, \n",
        "     a virtual count that gets added to every outcome that has been seen\n",
        "     including all outcomes that have not been seen \n",
        "     (at this point we cannot know what new tokens will be seen in the future, \n",
        "      so we will be using a placeholder token for that, the special UNK token)\n",
        "    EOS: a special token to be used as the end-of-sentence marker\n",
        "     which you should pretend occurs at the end of every document\n",
        "    UNK: a special token to be used as a placeholder for all unseen tokens\n",
        "     this will only be used if you enable Laplace smoothing  (alpha > 0)  \n",
        "        \n",
        "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
        "    \"\"\"\n",
        "    if alpha < 0:\n",
        "        raise ValueError(\"Laplace smoothing requires a positive alpha\")\n",
        "        \n",
        "    support_Y = get_labels(data_y) # all classes in the corpus\n",
        "    vocab = get_vocabulary(data_x, alpha=alpha, EOS=EOS, UNK=UNK)\n",
        "    V = len(vocab)\n",
        "\n",
        "    raise NotImplemented(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqVdrWjThgiQ"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```python\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "def estimate_pi(data_x, data_y, alpha=0., EOS='-EOS-', UNK='-UNK-'):\n",
        "    \"\"\"\n",
        "    Return the dict that stores the parameters of the class-conditioned distributions over vocabulary.\n",
        "    \n",
        "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
        "    data_y: list of labels, each label is a string\n",
        "    \n",
        "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
        "    alpha: the Laplace smoothing coefficient, \n",
        "     a virtual count that gets added to every outcome that has been seen\n",
        "     including all outcomes that have not been seen \n",
        "     (at this point we cannot know what new tokens will be seen in the future, \n",
        "      so we will be using a placeholder token for that, the special UNK token)\n",
        "    EOS: a special token to be used as the end-of-sentence marker\n",
        "     which you should pretend occurs at the end of every document\n",
        "    UNK: a special token to be used as a placeholder for all unseen tokens\n",
        "     this will only be used if you enable Laplace smoothing  (alpha > 0)  \n",
        "        \n",
        "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
        "    \"\"\"\n",
        "    # **SOLUTION**\n",
        "    \n",
        "    if alpha < 0:\n",
        "        raise ValueError(\"Laplace smoothing requires a positive alpha\")\n",
        "        \n",
        "    support_Y = get_labels(data_y) # all classes in the corpus\n",
        "    vocab = get_vocabulary(data_x, alpha=alpha, EOS=EOS, UNK=UNK)\n",
        "    V = len(vocab)\n",
        "\n",
        "    # Counts for (X,Y)\n",
        "    joint_counts = dict((y, Counter()) for y in support_Y)\n",
        "    for x, y in zip(data_x, data_y):\n",
        "        counter_w_given_y = joint_counts[y]\n",
        "        counter_w_given_y.update(x + [EOS])  # we gotta pad our sentences with EOS        \n",
        "        \n",
        "    # Conditional parameters pi\n",
        "    pi = dict()\n",
        "    \n",
        "    for y in support_Y:\n",
        "        counts_w_given_y = joint_counts[y]\n",
        "        cpd = defaultdict(float)\n",
        "        total_counts = sum(counts_w_given_y.values())\n",
        "        for w in vocab:\n",
        "#         for w, n in counts_w_given_y.items():  # renormalise (dealing with virtual counts as well)\n",
        "            cpd[w] = (counts_w_given_y.get(w, 0) + alpha) / (total_counts + alpha * V)\n",
        "        pi[y] = cpd\n",
        "    \n",
        "    return pi\n",
        "```\n",
        "    \n",
        "---    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp-t2tHWhgiR"
      },
      "source": [
        "These are the tests for non-smoothed models ($\\alpha=0$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6zJTSdihgiR"
      },
      "outputs": [],
      "source": [
        "test_result_pi_1 = estimate_pi(test_data_x_1, test_data_y_1)\n",
        "\n",
        "# First of all we get proper cpds\n",
        "assert all(validate_categorical_cpd(cpd) for y, cpd in test_result_pi_1.items())\n",
        "\n",
        "# There are 8 occurrences of 'bad' in negative documents\n",
        "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
        "assert np.isclose(test_result_pi_1['neg']['bad'], 8/20, 0.01)\n",
        "\n",
        "# There are 2 occurrences of 'so' in negative documents\n",
        "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
        "assert np.isclose(test_result_pi_1['neg']['so'], 2/20, 0.01)\n",
        "\n",
        "# There are 0 occurrences of 'good' in negative documents\n",
        "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
        "assert np.isclose(test_result_pi_1['neg']['good'], 0/20, 0.01)\n",
        "\n",
        "# There are 2 occurrences of 'so' in positive documents\n",
        "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
        "assert np.isclose(test_result_pi_1['pos']['so'], 2/22, 0.01)\n",
        "\n",
        "# EOS artificially occurs once per document\n",
        "assert np.isclose(test_result_pi_1['neg']['-EOS-'], 5/20, 0.01)\n",
        "assert np.isclose(test_result_pi_1['pos']['-EOS-'], 7/22, 0.01)\n",
        "\n",
        "#assert np.isclose(test_result_pi_1['neg']['-EOS-'], 1/((2+4+4+4+6)/5), 0.01)\n",
        "#assert np.isclose(test_result_pi_1['pos']['-EOS-'], 1/((3+3+3+3+3+3+4)/7), 0.01)\n",
        "\n",
        "# No mass reserved for future unseen words\n",
        "assert get_cond_parameter(w=\"alright\", y=\"pos\", pi=test_result_pi_1) == 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoT-vGSBhgiR"
      },
      "source": [
        "For smoothed models (with $\\alpha>0$) we need to change our definition of `get_cond_parameter` so that it uses the probability of the UNK token when it finds an unseen token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4BteR3mhgiR"
      },
      "outputs": [],
      "source": [
        "def get_cond_parameter(y: str, w: str, pi: dict, UNK='-UNK-'):\n",
        "    \"\"\"\n",
        "    Return P(W=w|Y=y) under the model Categorical(\\pi^{(y)}).\n",
        "    \n",
        "    y: the label\n",
        "    w: word\n",
        "    pi: a dictionary of dictionaries \n",
        "        first we can index it using a label to obtain a dict as return, \n",
        "        the latter dict are the parameters for a distribution over the vocabulary given the label\n",
        "        and it can be indexed using a word to obtain a probability mass\n",
        "\n",
        "        even though mathematically we think of pi as a table/matrix\n",
        "        in code it can be convenient to treat it like a dictionary some times, \n",
        "        for example, as a dictionary we can use labels that are strings (rather than 0-based indices)\n",
        "        and words that are strings (rather than 0-based indices to a vocabulary).        \n",
        "        \n",
        "        For this implementation assume that every dict inside of pi\n",
        "         has already been validated as a Categorical parameter.\n",
        "    UNK: we return the probability P(W=UNK|Y=y) in case w is not in the support.\n",
        "        \n",
        "    Return: Categorical(w|\\phi^{(y)})\n",
        "    \"\"\"\n",
        "    cpd = pi.get(y, None)\n",
        "    if cpd is None:  # the label is not in the support of the model\n",
        "        return 0. \n",
        "    # we fetch the probability of the UNK symbol\n",
        "    # and use it as default for when we fetch the probability of the word w\n",
        "    return cpd.get(w, cpd.get(UNK, 0.))  # if a word is outside the support, it gets 0 mass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOpumMB3hgiR"
      },
      "source": [
        "Now you can test with Laplace smoothing on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcSkJlddhgiR"
      },
      "outputs": [],
      "source": [
        "test_result_pi_2 = estimate_pi(test_data_x_1, test_data_y_1, alpha=1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04BXty0OhgiS"
      },
      "outputs": [],
      "source": [
        "# First of all we get proper cpds\n",
        "assert all(validate_categorical_cpd(cpd) for y, cpd in test_result_pi_2.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfFORTPUhgiS"
      },
      "outputs": [],
      "source": [
        "# The first 7 documents are positive:\n",
        "#  number of words in positive docs \n",
        "#  + number of occurrences of EOS \n",
        "#  + one virtual count per word in vocab (which includes UNK and EOS)\n",
        "assert sum(len(x) for x in test_data_x_1[:7]) + 7 + 1 * len(get_vocabulary(test_data_x_1, alpha=1.0)) == 33\n",
        "# Seen word, but not seen with this label\n",
        "assert get_cond_parameter(w=\"really\", y=\"pos\", pi=test_result_pi_2) == 1/33\n",
        "# Unseen word\n",
        "assert get_cond_parameter(w=\"alright\", y=\"pos\", pi=test_result_pi_2) == 1/33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvO8heGVhgiS"
      },
      "outputs": [],
      "source": [
        "# The last 5 documents are negative:\n",
        "#  number of words in neagtive docs \n",
        "#  + number of occurrences of EOS \n",
        "#  + one virtual count per word in vocab (which includes UNK and EOS)\n",
        "assert sum(len(x) for x in test_data_x_1[-5:]) + 5 + 1 * len(get_vocabulary(test_data_x_1, alpha=1.0)) == 31\n",
        "# Seen word, but not seen with this label\n",
        "assert get_cond_parameter(w=\"quite\", y=\"neg\", pi=test_result_pi_2) == 1/31\n",
        "# Unseen word\n",
        "assert get_cond_parameter(w=\"alright\", y=\"neg\", pi=test_result_pi_2) == 1/31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9hlogR2hgiS"
      },
      "source": [
        "**Congratulations! You've implemented every aspect of an NB classifier :D**\n",
        "\n",
        "The rest of this section will just organise the code and put it for you in a single \"package\", so that you can use it in an experiment (which you will conduct in the next section).\n",
        "\n",
        "This section also contains a small demonstration of how you can use the newly developed NB classifier.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "When we have a number of related functionalities (e.g., get parameters, combine them into joint probabilities, marginal probabilities, use them for predictions, code for estimating the parameters themselves, etc.), it is convenient to group all these functionalities into something like a container.\n",
        "\n",
        "This is called a *class* in many programming languages. Think of classes as templates for instantiating objects that store data and code for you to reuse over and over in different situations.\n",
        "\n",
        "You don't need to be able write a class yourself, but it is useful to study the one below. \n",
        "\n",
        "In this course, programming skills *are not* assessed in exams, so studying a class definition and learning to use such a concept is a skill that will help your programming and it is a practical skill of great importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrU-YxBshgiS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import chain, takewhile\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    \n",
        "    def __init__(self, labels: list, vocab=[], alpha=0., EOS='</s>', UNK='<unk>', seed=None):\n",
        "        \"\"\"\n",
        "        This constructs a NaiveBayesClassifier built upon a joint distribution over X and Y\n",
        "        where X are documents and Y is a label.\n",
        "        \n",
        "        The set of labels is finite and known ahead of time.\n",
        "\n",
        "        Every document X is a sequence of tokens, a token W is a word in a finite vocabulary.\n",
        "        We can start with a given vocabulary if we already know it, \n",
        "         but in any case, this class offers a training procedure which will use the words in a given corpus\n",
        "         as the set of known words in the vocabulary.\n",
        "         \n",
        "        The NBC needs two special symbols for practical purposes. The end-of-sequence symbol (EOS)\n",
        "         helps the NBC stop generating, when the model is used for generation of new documents.\n",
        "         The UNK token is a placeholder for every unseen word we may encounter in the future.\n",
        "         \n",
        "        Without training, this NBC uses uniform cpds for every cpd in the model\n",
        "\n",
        "        With training, this NBC may uses Laplace smoothing (if you set alpha to something greater than 0.).\n",
        "        \n",
        "        After the class definition you will find a demonstration of how to use it.\n",
        "        \n",
        "        :param labels: a list of classes, each a string or integer\n",
        "        :param alpha: the smoothing coefficient for distributions of the kind X|Y=y\n",
        "        :param EOS: the EOS symbol\n",
        "        :param UNK: the UNK symbol for smoothing\n",
        "        :param seed: random generator seed, fix this for reproducibility\n",
        "        \"\"\"\n",
        "        self._EOS = EOS\n",
        "        self._UNK = UNK\n",
        "        self._alpha = alpha        \n",
        "        self._labels = tuple(labels)\n",
        "        self._rng = rng = np.random.RandomState(seed)  # Good for reproducibility\n",
        "        # The method fit will populate these\n",
        "        self._vocab = set(vocab)\n",
        "        self._vocab.add(EOS)\n",
        "        self._vocab.add(UNK)\n",
        "        # we initialise our distributions with uniform probabilities\n",
        "        #  but we will use a trick, if these objects are None, \n",
        "        #  the methods that read them will return a uniform probability by default\n",
        "        #  this way we do not need to do anything at this point\n",
        "        self._prior_probs = None\n",
        "        # intialise with nothing\n",
        "        #  we will use a trick, whenever we have None, the probabilities are going to be uniform\n",
        "        self._cond_probs = None    \n",
        "        \n",
        "    def get_prior_parameter(self, y: str):\n",
        "        \"\"\"\n",
        "        Return P(Y=y) = phi[y]\n",
        "        \n",
        "        The only difference compared to before is that this time we assume a uniform distribution\n",
        "        when the model is untrained.\n",
        "        \"\"\"\n",
        "        return 1/len(self._labels) if self._prior_probs is None else self._prior_probs[y]    \n",
        "    \n",
        "    def get_cond_parameter(self, y: str, w: str):\n",
        "        \"\"\"\n",
        "        Return P(W=w|Y=y) = pi[y, w]\n",
        "        \n",
        "        If the model is untrained, we assume the probaiblity is uniform over the vocabulary.\n",
        "        \"\"\"\n",
        "        if self._cond_probs is None:\n",
        "            # return uniform probability if the model isn't trained\n",
        "            return 1/len(self._vocab) \n",
        "        else:\n",
        "            # prob of unk|y\n",
        "            unk_prob = self._cond_probs[y].get(self._UNK, 0.0)\n",
        "            # return w|y if w is in the dictionary, if not return unk|y\n",
        "            return self._cond_probs[y].get(w, unk_prob)     \n",
        "        \n",
        "    def log_prior_prob(self, y: str):\n",
        "        \"\"\"Return log P(Y=y)\"\"\"\n",
        "        return np.log(self.get_prior_parameter(y))\n",
        "        \n",
        "    def log_conditional_prob(self, x: list, y: str):\n",
        "        \"\"\"\n",
        "        Return log P(X=x|Y=y)\n",
        "        \"\"\"\n",
        "        return sum(np.log(self.get_cond_parameter(y=y, w=w)) for w in x)  # named arguments help us not make mistakes\n",
        "    \n",
        "    def log_joint_prob(self, y: str, x: list):\n",
        "        \"\"\"Return log P(Y=y) + log P(X=x|Y=y)\"\"\"\n",
        "        return self.log_prior_prob(y) + self.log_conditional_prob(x=x, y=y)  # named arguments help us not make mistakes\n",
        "     \n",
        "    def log_marginal_prob(self, x: list):\n",
        "        \"\"\"Return log P(X=x) = log \\sum_y P(Y=y, X=x)\"\"\"\n",
        "        # the function logaddexp(a, b) returns log(exp(a) + exp(b))\n",
        "        # the method reduce applies that in a row to a whole list of values \n",
        "        return np.logaddexp.reduce([self.log_joint_prob(y=y, x=x) for y in self._labels])  # named arguments help us not make mistakes\n",
        "    \n",
        "    def log_posterior_prob(self, y: str, x: list):\n",
        "        \"\"\"Return log P(Y=y|X=x) under the model\"\"\"\n",
        "\n",
        "        # The posterior probability is a conditional probability\n",
        "        #  by definition that is P(Y=y,X=x) / P(X=x)\n",
        "        #  and we have already implemented the joint probability in the numerator\n",
        "        log_joint = self.log_joint_prob(y=y, x=x)\n",
        "        #  and the marginal probability in the denominator\n",
        "        log_marginal = self.log_marginal_prob(x)\n",
        "        # As we are in computing it in log space, we can subtract the logs to divide the probs\n",
        "        return log_joint - log_marginal\n",
        "    \n",
        "    def predict(self, data_x: list):\n",
        "        \"\"\"\n",
        "        :param data_x: a list of sentences to be classified, each sentence is a list of tokens (each token is a string)\n",
        "\n",
        "        :return: a list of predictions, one per sentence in data_x, \n",
        "            each prediction is the label with highest posterior probability given the input\n",
        "        \"\"\"        \n",
        "        y_pred = []\n",
        "        for x in data_x:            \n",
        "            # the label with highest joint probability with x is also \n",
        "            # the label with the highest posterior probability (since P(X=x) is fixed)\n",
        "            # also, we can do everything in log domain\n",
        "            best_k = np.argmax([self.log_joint_prob(y=y, x=x) for y in self._labels])\n",
        "            y_pred.append(self._labels[best_k])\n",
        "        return y_pred  \n",
        "    \n",
        "    def fit(self, data_x: list, data_y: list):\n",
        "        \"\"\"\n",
        "        Return the dict that stores the parameters of the class-conditioned distributions over vocabulary.\n",
        "\n",
        "        data_x: list of documents, each document is a list of tokens, each token is a string\n",
        "        data_y: list of labels, each label is a string\n",
        "\n",
        "            Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
        "        alpha: the Laplace smoothing coefficient, \n",
        "         a virtual count that gets added to every outcome that has been seen\n",
        "         including all outcomes that have not been seen \n",
        "         (at this point we cannot know what new tokens will be seen in the future, \n",
        "          so we will be using a placeholder token for that, the special UNK token)\n",
        "        EOS: a special token to be used as the end-of-sentence marker\n",
        "         which you should pretend occurs at the end of every document\n",
        "        UNK: a special token to be used as a placeholder for all unseen tokens\n",
        "         this will only be used if you enable Laplace smoothing  (alpha > 0)  \n",
        "\n",
        "        Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Make the set of classes\n",
        "        support_Y = self._labels\n",
        "        assert set(self._labels) == set(data_y) # we need observations for all classes \n",
        "        \n",
        "        # Make the vocabulary of words\n",
        "        # Vocabulary (support of W is all known words)\n",
        "        self._vocab = set(chain(*data_x))\n",
        "        # plus the special EOS token\n",
        "        self._vocab.add(self._EOS)\n",
        "        if self._alpha > 0.:\n",
        "            # and the special UNK token\n",
        "            self._vocab.add(self._UNK)\n",
        "        V = len(self._vocab)\n",
        "\n",
        "        # MLE for Y\n",
        "        N = len(data_y)\n",
        "        # count_Y(y) divided by total data points N (no need for smoothing here)\n",
        "        self._prior_probs = dict((y, float(n) / N) for y, n in Counter(data_y).items())\n",
        "        \n",
        "        # MLE for W|Y\n",
        "        # Counts for (X,Y)\n",
        "        joint_counts = dict((y, Counter()) for y in support_Y)\n",
        "        for x, y in zip(data_x, data_y):\n",
        "            counter_w_given_y = joint_counts[y]\n",
        "            counter_w_given_y.update(x + [self._EOS])  # we gotta pad our sentences with EOS        \n",
        "\n",
        "        # Conditional parameters\n",
        "        self._cond_probs = dict()\n",
        "\n",
        "        for y in support_Y:\n",
        "            counts_w_given_y = joint_counts[y]\n",
        "            cpd = defaultdict(float)\n",
        "            total_counts = sum(counts_w_given_y.values())\n",
        "            for w in self._vocab:\n",
        "                cpd[w] = (counts_w_given_y.get(w, 0) + self._alpha) / (total_counts + self._alpha * V)\n",
        "            self._cond_probs[y] = cpd\n",
        "    \n",
        "    def sample_n(self, num_samples=1, y=None, max_length=200):\n",
        "        \"\"\"\n",
        "        Sample a number of times from the joint distribution, or from the conditional\n",
        "        :param num_samples: how many samples do we want to generate\n",
        "        :param y: if given, sample from S|Y=y, otherwise from Y and then from S|Y    \n",
        "        :param max_length: maximum length for s\n",
        "        :return: a generator of samples, each a pair (y, x)\n",
        "        \"\"\"   \n",
        "        \n",
        "        # This algorithm is a bit tricky, \n",
        "        #  you do not need to study it necessarily\n",
        "        # Here's what it does conceptually\n",
        "        # for _ in range(S)\n",
        "        #  draw Y from Cat(phi)\n",
        "        #  draw W[i]|Y=x from Cat(pi[y]) until we draw an EOS symbol\n",
        "        # But, to make it a bit more efficient, we draw\n",
        "        #  S times from Cat(phi)\n",
        "        # Then for each of those draws s=1, ..., S we draw\n",
        "        #  L times from Cat(pi[y[s]]) where L is max length\n",
        "        #  we then find the position of the first EOS and discard from there onwards\n",
        "        \n",
        "        if y is None: # Draw num_samples times from P_Y\n",
        "            # np.random.choice returns an integer in [0, a) and p is the discrete probability distribution it samples from\n",
        "            # we obtain num_samples such samples\n",
        "            ids = self._rng.choice(len(self._labels), p=[self.get_prior_parameter(k) for k in self._labels], size=num_samples)\n",
        "            # these are the sampled labels\n",
        "            sampled_labels = [self._labels[k] for k in ids]\n",
        "        else:  # or repeat the given label num_samples times\n",
        "            # here the sampled_labels are whatever the user chose via argument y\n",
        "            sampled_labels = [y for _ in range(num_samples)]\n",
        "\n",
        "        # here we turn the support of the conditionals into a list for easy access\n",
        "        vocab = [w for w in self._vocab]\n",
        "        vocab_size = len(vocab)        \n",
        "        # here we turn the conditionals into np arrays so it's easier to sample from\n",
        "        probs = dict()        \n",
        "        for label in set(sampled_labels):\n",
        "            probs[label] = np.array([self.get_cond_parameter(y=label, w=w) for w in self._vocab])\n",
        "\n",
        "        # for each sampled label, we also sample a document\n",
        "        for label in sampled_labels:\n",
        "            # I know use numpy to sample max_length tokens following the distribution W|Y=y\n",
        "            # I also map numpy's choices (which are indices) to words using `vocab`\n",
        "            seq = [vocab[v] for v in self._rng.choice(vocab_size, max_length, p=probs[label])]\n",
        "            # Finally, I throw away whatever happens after the first occurrence of EOS\n",
        "            # since the sampler should have stopped at that point\n",
        "            # (this wastes a bit of computation, but it's okay for our demonstration)\n",
        "            yield (label, list(takewhile(lambda w: w != self._EOS, seq)) )   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JBBYlrOhgiT"
      },
      "source": [
        "Here is a **toy demonstration** to help you with debugging your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plyvZ6f6hgiT"
      },
      "outputs": [],
      "source": [
        "# an untrained model with \n",
        "toy_nbc = NaiveBayesClassifier(['neg', 'pos'], vocab=['pepper', 'the', 'dog'], alpha=0.01, seed=23)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q77O2PjhgiT"
      },
      "source": [
        "See that you can sample from an untrained model. That's because an untrained model *is* a probability distribution nonetheless. It's just a not very good one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnSAGIjvhgiT"
      },
      "outputs": [],
      "source": [
        "for y, x in toy_nbc.sample_n(10):\n",
        "    print('{}: {}'.format(y, ' '.join(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgnoP5HYhgiU"
      },
      "source": [
        "Here's a demonstration of how to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BurwPN9MhgiU"
      },
      "outputs": [],
      "source": [
        "toy_nbc.fit(\n",
        "    [\n",
        "        'this is bad'.split(), \n",
        "        'this is really good'.split(),\n",
        "        'this is pretty good'.split(),\n",
        "        'this is great'.split(),\n",
        "    ], ['neg', 'pos', 'pos', 'pos']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU8l6GUDhgiU"
      },
      "outputs": [],
      "source": [
        "assert toy_nbc._prior_probs == {'neg': 0.25, 'pos': 0.75},  \"Note the toy example has 1 negative and 3 positive instances, so we expect 1/4 and 3/4 as probability, not: {}\".format(toy_nbc._phi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM6lELaChgiU"
      },
      "outputs": [],
      "source": [
        "assert toy_nbc._vocab == {'</s>', '<unk>', 'bad', 'good', 'great', 'is', 'pretty', 'really', 'this'}, \"Did you perhaps forget to create a vocab or to add the UNK and EOS tokens?\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqpO4AlnhgiU"
      },
      "outputs": [],
      "source": [
        "assert toy_nbc.get_cond_parameter(y='neg', w='pretty') == toy_nbc.get_cond_parameter(y='neg', w='<unk>'), \"The word 'pretty' was never seen by\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q72AaWQNhgiV"
      },
      "source": [
        "Note that the distributions should normalise over the complete sample space (which includes EOS and UNK):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuEUyd95hgiV"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(sum(toy_nbc.get_cond_parameter(y='neg', w=w) for w in toy_nbc._vocab), 1, 1e-3), \"Did you smooth things correctly? Try again with alpha=0.0, then see if you can fix it for alpha > 0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPIVQLg7hgiW"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(sum(toy_nbc.get_cond_parameter(y='pos', w=w) for w in toy_nbc._vocab), 1, 1e-3), \"Did you smooth things correctly? Try again with alpha=0.0, then see if you can fix it for alpha > 0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDsTYGYqhgiW"
      },
      "source": [
        "Now you can use the model for making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3WSf-jghgiX"
      },
      "outputs": [],
      "source": [
        "toy_pred = toy_nbc.predict(\n",
        "    [\n",
        "        'this is really bad'.split(), \n",
        "        'this is good'.split(), \n",
        "        'this is pretty'.split()\n",
        "    ]\n",
        ")\n",
        "assert toy_pred == ['neg', 'pos', 'pos'], \"Something seems to have gone wrong with your predictions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8cALK0chgiX"
      },
      "source": [
        "and for computing probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMDZojhshgiX"
      },
      "outputs": [],
      "source": [
        "assert toy_nbc.log_conditional_prob(x='this is really bad'.split(), y='neg') > toy_nbc.log_conditional_prob(x='this is really bad'.split(), y='pos'), \"The negative LM likes this sentence more\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se2LYfh5hgiY"
      },
      "outputs": [],
      "source": [
        "assert toy_nbc.log_conditional_prob(x='this is really bad'.split(), y='neg') >  toy_nbc.log_marginal_prob(x='this is really bad'.split()) > toy_nbc.log_conditional_prob('this is really bad'.split(), y='pos'), \"The marginal probability is in between\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOM1MxTnhgiY"
      },
      "source": [
        "You can even sample from the generative story of the model. \n",
        "\n",
        "See how about 3/4 of the samples will be positive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN1m-8CVhgiY"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(sum(1 for y, s in toy_nbc.sample_n(1000) if y == 'pos')/1000, 0.75, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA0ybwRUhgiY"
      },
      "outputs": [],
      "source": [
        "# Random generation:\n",
        "for y, x in toy_nbc.sample_n(10):\n",
        "    print('{}: {}'.format(y, ' '.join(x)))\n",
        "    \n",
        "print('\\nOut of 100 samples we have {} positive.'.format(sum(1 for y, s in toy_nbc.sample_n(100) if y == 'pos')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP1VbZLuhgiY"
      },
      "source": [
        "We can also choose to sample conditionally, in this case, we ask for samples from the *positive* class only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTbZadrvhgiZ"
      },
      "outputs": [],
      "source": [
        "for y, s in toy_nbc.sample_n(10, y='pos'):\n",
        "    print('{}: {}'.format(y, ' '.join(s)))\n",
        "\n",
        "print('\\nAverage length in 100 positive samples is {:.2f}'.format(sum(len(s) for y, s in toy_nbc.sample_n(100, y='pos'))/100.))    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeVshdrRhgiZ"
      },
      "source": [
        "Same can be done for the negative class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unV4PI2AhgiZ"
      },
      "outputs": [],
      "source": [
        "for y, s in toy_nbc.sample_n(10, y='neg'):\n",
        "    print('{}: {}'.format(y, ' '.join(s)))\n",
        "\n",
        "print('\\nAverage length in 100 positive samples is {:.2f}'.format(sum(len(s) for y, s in toy_nbc.sample_n(100, y='neg'))/100.))    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8q4ms54hgiZ"
      },
      "source": [
        "And of course, we can check posterior probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjkinbM7hgiZ"
      },
      "outputs": [],
      "source": [
        "np.exp(toy_nbc.log_posterior_prob(x=\"too bad\".split(), y=\"neg\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkGU6YFUhgia"
      },
      "outputs": [],
      "source": [
        "np.exp(toy_nbc.log_posterior_prob(x=\"too bad\".split(), y=\"pos\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xupwf5wvhgia"
      },
      "source": [
        "The NB classifier *supports* actual documents, that is, it assigns non-zero probability to actual documents. But it also supports plenty of sequences that are not at all plausible, even after training. This may seem strange at first, but it is actually to be expected, given that it makes too strong assumptions (it is a \"naive\" model after all).\n",
        "\n",
        "This does not mean the model is bad. Generating documents is not its primary *application*, even though being able to generate a document is part of its fundamental design. We call it a \"document\", but for the model it is just a collection of words that are considered related to classes, theses words are not in order, and the model has no clue what these words mean for a human. \n",
        "\n",
        "The primary application of NBC is to represent our uncertainty about the categorisation of a document, so its primary application are inferences we make based on the posterior distribution (or the joint distribution, depending on the purpose).  In the next section we will use the model for classification. \n",
        "\n",
        "We will also play with it as a sampler, just to see what we get, but don't hope for much yet :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKl90gsUhgia"
      },
      "source": [
        "## <a name=\"exp\">  Experiment\n",
        "\n",
        "\n",
        "We will now try our NBC on one of NLTK's datasets.\n",
        "\n",
        "As usual in testing a machine learning model, we will need training data, development data, and test data. Check [Section 4.8](https://web.stanford.edu/~jurafsky/slp3/4.pdf) for a recap of how to train/test a machine learning classifier. There's no need to implement cross-validation (a training/development/test split is sufficient, and we provide helper code for splitting the data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnsUjYw4hgia"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import chain\n",
        "\n",
        "\n",
        "def split_corpus(sentences, ratio=0.9):\n",
        "    \"\"\"\n",
        "    Randomly split a list of sentences into two sets according to the given ratio.\n",
        "    \n",
        "    :param sentences: already tokenized sentences (list of strings)\n",
        "    \"\"\"\n",
        "    # This will guarantee that the permutation is the same every time (which is important for reproducibility)\n",
        "    rng = np.random.RandomState(42)    \n",
        "    indices = rng.permutation(len(sentences))\n",
        "    n = int(indices.size * ratio)\n",
        "    return [sentences[i] for i in indices[:n]], [sentences[i] for i in indices[n:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWOX_HfAhgia"
      },
      "source": [
        "Let's attempt to predict sentence polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf3WRmlrhgia"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import sentence_polarity\n",
        "\n",
        "pos_sents = sentence_polarity.sents(categories='pos')\n",
        "neg_sents = sentence_polarity.sents(categories='neg')\n",
        "print(len(pos_sents), 'positive sentences such as:\\n', ' '.join(pos_sents[0]))\n",
        "print(len(neg_sents), 'negative sentences such as:\\n', ' '.join(neg_sents[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rmnOq43hgia"
      },
      "source": [
        "And here is a reasonable split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axqTbDvjhgia"
      },
      "outputs": [],
      "source": [
        "training_pos, dev_pos = split_corpus(pos_sents, 0.8)\n",
        "dev_pos, test_pos = split_corpus(dev_pos, 0.5)\n",
        "training_neg, dev_neg = split_corpus(neg_sents, 0.8)\n",
        "dev_neg, test_neg = split_corpus(dev_neg, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz-HwpbVhgia"
      },
      "outputs": [],
      "source": [
        "rows = [\n",
        "    ['pos', len(training_pos), len(dev_pos), len(test_pos)],\n",
        "    ['neg', len(training_neg), len(dev_neg), len(test_neg)]\n",
        "]\n",
        "print(tabulate(rows, headers=['Label', 'Training', 'Development', 'Test']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNODcGWWhgia"
      },
      "source": [
        "<a name=\"ex-NBC\"> **Graded Exercise - NBC for sentence polarity** \n",
        "    \n",
        "Fit NBC, evaluate it on dev set. To recap how you evaluate a classifier, check [Section 4.7](https://web.stanford.edu/~jurafsky/slp3/4.pdf). \n",
        "\n",
        "For this use $\\alpha=1.0$. For evaluation you can use `sklearn.metrics import classification_report`, check its documentation. You can install scikit-learn via `!pip install sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTRIBUTE YOUR SOLUTION"
      ],
      "metadata": {
        "id": "ch__6tn0kNAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5g8JEFQhgib"
      },
      "source": [
        "<a name=\"ex-grid\"> **Graded Exercise - Grid Search** \n",
        "    \n",
        "Grid-search for a good value of $\\alpha$, use the grid shown below. Use performance on *development* set to pick the best value of the hyperparameter. If your data are not balanced (e.g., brown) you should choose the precise metric carefully (not every metric in scikit's evaluation report is equally good with class imbalance). For the best value of $\\alpha$ then test your NBC on the test-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPMgNPPshgib"
      },
      "outputs": [],
      "source": [
        "grid_alpha = 0.1 + np.arange(0., 1.5, 0.1)\n",
        "grid_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBXLtAFihgib"
      },
      "outputs": [],
      "source": [
        "# CONTRIBUTE YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJicuwnvhgib"
      },
      "source": [
        "## Error analysis \n",
        "\n",
        "This is a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), it displays a summary of our predictions in terms of true positives (TP), false positive (FP), false negative (FN), and true negatives (TN). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFFrnjolhgib"
      },
      "source": [
        "<a name=\"ex-analysis\"> **Graded Exercise - Error analysis** \n",
        "    \n",
        "Print the confusion matrix of your classifier using the dev set, use $\\alpha=0.1$ for training. \n",
        "\n",
        "You can build the matrix yourself, or you can use sklearn's `from sklearn.metrics import confusion_matrix`. \n",
        "\n",
        "Then, manually inspect some of the mistakes (FP or FN) and speculate about directions for improvement in the future, in particular, find (and list) examples of **two or more** patterns that the model fails to recognise **because** of its assumptions. Explain how you think the model assumptions are responsible for the failure."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTRIBUTE YOUR SOLUTION"
      ],
      "metadata": {
        "id": "PDpeP022kR_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WABx9HnMhgib"
      },
      "source": [
        "# <a name=\"further\"> Further analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HaPBnxBhgib"
      },
      "source": [
        "\n",
        "\n",
        "**Ungraded exercise without solution** Draw samples from this NBC model (use as many samples as you have data points in total).\n",
        "\n",
        "Investigate the following:\n",
        "\n",
        "* label frequency\n",
        "* sequence length\n",
        "* sequence length per class\n",
        "* most frequent words per class\n",
        "\n",
        "\n",
        "What aspects of the observed data do you think model samples reflect well?\n",
        "\n",
        "What aspects do they not?\n",
        "\n",
        "This exercise is here just to help you visualise some consequences of our modelling assumptions so far. It is not part of the tutorial per se. It connects T1 and T2, so in that sense we recommend you look into this, for example, after you are done with your graded assignments, there will be more discussion on this topic in class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCjG7IYnhgib"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "T2_student.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hEpYaZtthgiE",
        "70pzIxukhgiF"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}